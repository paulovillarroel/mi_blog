[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Sobre m√≠",
    "section": "",
    "text": "Soy Enfermero egresado de la Universidad de Chile con 18 a√±os de experiencia profesional, con vasta experiencia en √°mbitos de gesti√≥n, innovaci√≥n y evaluaci√≥n de proyectos. Durante m√°s de 1 d√©cada he liderado distintos proyectos de mejora de procesos, inteligencia de negocios y excelencia operacional (LEAN y Seis Sigma) en distintas instituciones de salud. Actualmente, junto a mi rol cl√≠nico, desarrollo iniciativas de Ciencia de Datos aplicada, tanto a nivel profesional como docente.\nEmprendedor y fundador de la √∫nica comunidad de innovaci√≥n abierta en salud p√∫blica de Chile (OpenSalud LAB). Adem√°s, soy creador y coordinador del curso de Data Science en salud en espa√±ol m√°s grande del mundo."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Blog",
    "section": "",
    "text": "Mejorando la gesti√≥n de pacientes GES\n\n\n15 min\n\n\n\nges\n\n\ntutorial\n\n\nexcel\n\n\n\nTe muestro una propuesta de modelo de gesti√≥n de garant√≠as GES, para simplificar la gesti√≥n y el monitoreo, basado en tiempos de espera y prioridad cl√≠nica.\n\n\n\nFeb 15, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDescargu√© casi 3000 im√°genes de Fortnite!\n\n\n11 min\n\n\n\ntutorial\n\n\napi\n\n\nr\n\n\n\nTe ensa√±ar√© paso a paso como usar una API, usar funciones iterativas y automatizar diversas tareas para pasar de d√≠as a solo minutos.\n\n\n\nDec 30, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUsa la inteligencia artificial en tu hoja de c√°lculo\n\n\n13 min\n\n\n\ntutorial\n\n\nmachine learning\n\n\n\nLas hojas de c√°lculo las usamos en casi todo y para todo. En este art√≠culo te explico c√≥mo usar IA utilizando algoritmos de Google para predicci√≥n.\n\n\n\nDec 24, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nContando letras\n\n\n10 min\n\n\n\nr\n\n\npython\n\n\ntutorial\n\n\nkatas\n\n\n\nResolvemos un reto de l√≥gica de programaci√≥n, aprovechamos de hacerlos de varias formas y nos lanzamos con unos test unitarios adem√°s!\n\n\n\nNov 23, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAPI de cero a experto (parte 1)\n\n\n8 min\n\n\n\npython\n\n\ntutorial\n\n\napi\n\n\n\nEste es el comienzo de una serie de art√≠culos en donde revisaremos c√≥mo trabajar usando API¬¥s de forma profesional. Revisamos los fundamentos para que comiences este viaje.\n\n\n\nNov 19, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n¬øC√≥mo descargar datos desde GitHub m√°s r√°pido?\n\n\n6 min\n\n\n\ntutorial\n\n\ngithub\n\n\npython\n\n\nr\n\n\n\nVemos paso a paso c√≥mo descargar archivos desde GitHub usando Python y R. Adem√°s, tomamos el tiempo de cu√°nto demoran!\n\n\n\nOct 27, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n¬øA qui√©n le compra m√°s el Estado?\n\n\n7 min\n\n\n\ntutorial\n\n\nr\n\n\ndatos abiertos\n\n\n\nUso datos abiertos de compras p√∫blicas para averiguar los proveedores top en tratos directos.\n\n\n\nOct 15, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n¬øQui√©n sobrevivi√≥ al accidente del Titanic?\n\n\n17 min\n\n\n\nr\n\n\ntutorial\n\n\nmachine learning\n\n\n\nEntreno 20 modelos de Machine Learning para averiguarlo.\n\n\n\nOct 8, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFactores en R\n\n\n12 min\n\n\n\nr\n\n\ntutorial\n\n\n\nBreve tutorial sobre las funciones m√°s interesantes para trabajar con factores en R.\n\n\n\nOct 4, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nContribuye al c√≥digo abierto\n\n\n2 min\n\n\n\nopensource\n\n\ntutorial\n\n\ngit\n\n\ngithub\n\n\n\nRevisemos c√≥mo contribuir a que crezca el conocimiento en el mundo!\n\n\n\nOct 3, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMi ruta de aprendizaje en Python\n\n\n5 min\n\n\n\npython\n\n\ncurso\n\n\n\nAprende desde cero el lenguaje de programaci√≥n m√°s usado y demandado del mundo.\n\n\n\nOct 3, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPersonalizando el blog\n\n\n6 min\n\n\n\nquarto\n\n\ntutorial\n\n\n\nAhora que ya tenemos nuestro blog arriba, toca personalizarlo y dejarlo m√°s lindo.\n\n\n\nSep 19, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCreando mi primer blog con Quarto\n\n\n8 min\n\n\n\nquarto\n\n\ntutorial\n\n\n\nTe ense√±o a crear tu primer blog con Quarto, publicar nuevos art√≠culos, subirlo a GitHub y desplegarlo en la web.\n\n\n\nSep 17, 2022\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/api-cero-a-experto-p1/index.html",
    "href": "posts/api-cero-a-experto-p1/index.html",
    "title": "API de cero a experto (parte 1)",
    "section": "",
    "text": "Hola!!!\nGracias por estar por estos lados nuevamente.\nCon este art√≠culo pretendo iniciar una serie de tutoriales para ense√±ar a consumir datos desde una API. No tengo claro cu√°ntos ser√°n en total, pero como siempre, deseo hacerlo muy de a poco y simples, de modo que puedas sentirte c√≥modo/a al final de todo.\nPara estos tutoriales usar√© mayormente Python (versi√≥n 3.10), pero recuerda que m√°s all√° del lenguaje, lo importante es comprender la l√≥gica que est√° detr√°s y los fundamentos de programaci√≥n. El lenguaje no es m√°s que una herramienta y si bien la sintaxis puede variar (la forma en que se escribe el c√≥digo), si puedes entender c√≥mo funciona, te ser√° muy simple adaptarlo a tu lenguaje favorito.\nOk!!\nEntonces partamos‚Ä¶\n\n¬øPor qu√© aprender a usar una API?\nSi est√°s partiendo en la programaci√≥n y en an√°lisis de datos, de seguro que te has trabajado con el t√≠pico dataset iris o el de los autos jajaja. Con unos cl√°sicos.\nTambi√©n es posible que est√©s usando archivos Excel para analizarlos o algunos CSV.\nLa cosa es que todos esos datos est√°n ordenados y en formatos m√°s o menos f√°ciles de trabajar. Pero en la vida real, la historia es muy distinta. Te encontrar√°s con ese tipo de archivos, pero actualmente la forma m√°s com√∫n de interactuar con datos es de otra forma y lo es usando las API.\nEs un mundo que debes estudiar, pues tarde o temprano te vas a ver en la necesidad de trabajar usando una API. No es tan complejo, pero requiere de ciertos conocimientos y de pr√°ctica.\nSi me preguntas, a d√≠a de hoy, el saber de API es una cosa b√°sica, al menos, el saber hacer solicitudes y manipular los datos de respuesta.\nTranquilo/a!!!\nVeremos todo es de a poco.\n\n\n¬øQu√© es una API?\nDe forma simple, podemos entender a un API como una forma en que 2 sistemas inform√°ticos se comunican entre ellos. Para poder hacerlo, es necesario que estos sistemas ‚Äúhablen el mismo lenguaje‚Äù para que se puedan entender. Con lenguaje no me refiero a lenguaje de programaci√≥n, sino a un conjunto de reglas y protocolos que son conocidos y acordados por ambas partes. Es como hablar con alguien. Si vas a una tienda y quieres comprar una bebida, te acercas al vendedor y le dices: ‚ÄúHola!! Me da una bebida en lata, por favor‚Äù. Si todo anda bien, el vendedor te va a preguntar cu√°l bebida quieres y te indicar√° las que tiene disponibles. Tu eliges la que gustes y la compras. Esto que parece b√°sico y trivial, para las computadoras no es tan simple, ni trivial. Deben existir una serie de acuerdos y formas de comunicaci√≥n para que las partes se logren entender. Imagina que vuelves al mismo local para comprar otra bebida, pero esta vez se lo pides de esta forma: ‚ÄúWitam, czy mogƒô prosiƒá o nap√≥j z puszki!‚Äù. Seguramente, el vendedor te va a mirar con una cara media rara, con cara de WTF! ü•¥ Y es que se lo pediste en polaco!! Claro, le pediste una bebida en lata, igual que antes, pero esta vez en un idioma que √©l no entiende. Eso hace que no logres obtener lo que quieres. Con las API¬¥s pasa lo mismo. Ambas partes deben entenderse para poder comunicarse y lograr una comunicaci√≥n entre ellas. Para eso est√°n los protocolos y definiciones que te mencion√© hace un rato.\nAPI? API significa ‚Äúinterfaz de programaci√≥n de aplicaciones‚Äù (del ingl√©s ‚ÄúApplication Programming Interface‚Äù), para que lo tengas presente.\nLa comunicaci√≥n entre las partes funciona como una pregunta (solicitud o query) y una respuesta. Como el ejemplo de las bebidas.\nAhora, las partes tienden a llamarse servidor y cliente, donde el cliente es quien hace la solicitud y el servidor es el que responde. Piensa en el servidor como la base de datos que contiene (y almacena) los datos y en los clientes como las p√°ginas web, aplicaciones de tel√©fonos o programas que consumen los datos de esos servidores. Habitualmente las personas acceden a los datos desde los clientes, pero tambi√©n se puede hacer directamente a los servidores. En eso trabajan los ingenieros de datos o backend. Aunque eso es otro tema.\nLas API pueden funcionar de cuatro maneras diferentes, seg√∫n el momento y el motivo de su creaci√≥n:\n\nSOAP¬†\nRPC\nWebSocket\nREST\n\nPara efectos de esta serie de tutoriales, usar√© la API REST, que tiende a ser una de las m√°s comunes y con la que te vas a encontrar frecuentemente en muchas de las aplicaciones, en especial, en las web.\nEn las API REST, el cliente env√≠a las solicitudes al servidor como datos. El servidor utiliza esta entrada del cliente para iniciar funciones internas y devuelve los datos de salida al cliente. Una API REST es una API que cumple los principios de dise√±o del estilo de arquitectura REST o transferencia de estado representacional. Tambi√©n se llaman a las API REST como API RESTful.\nEste dato es importante‚Ä¶\nLas API REST no son un protocolo de comunicaci√≥n en si mismo, sino una arquitectura de dise√±o. Esta arquitectura est√° plasmada en la tesis de Roy Fielding, ‚ÄúArchitectural Styles and the Design of Network-based Software Architectures‚Äù, en donde se se√±ala que las API son RESTful siempre que cumplan con las 6 limitaciones principales de un sistema RESTful:\n\nArquitectura cliente-servidor:¬†la arquitectura REST est√° compuesta por clientes, servidores y recursos; y administra las solicitudes con HTTP.\nSistema sin estado:¬†el contenido de los clientes no se almacena en el servidor entre las solicitudes. En su lugar, la informaci√≥n sobre el estado de la sesi√≥n est√° en posesi√≥n del cliente.\nCapacidad de almacenamiento en cach√©:¬†el almacenamiento en cach√© elimina la necesidad de algunas interacciones cliente-servidor.\nSistema en capas:¬†las interacciones cliente-servidor pueden estar mediadas por capas adicionales. Estas capas pueden ofrecer funcionalidades adicionales, como equilibrio de carga, cach√©s compartidos o seguridad.\nDisponibilidad del c√≥digo seg√∫n se solicite (opcional):¬†los servidores pueden ampliar las funciones de un cliente transfiriendo c√≥digo ejecutable.\nInterfaz uniforme:¬†esta limitaci√≥n es fundamental para el dise√±o de las API de RESTful.\n\nFuente: https://www.redhat.com/es/topics/api/what-are-application-programming-interfaces\n\n\n\nEsquema b√°sico de comunicaci√≥n con API\n\n\nPara no dejar de mencionarlo, aunque realmente no lo veremos ac√°, es que este tema de la comunicaci√≥n entre servidores y clientes es tan relevante (pero MUY relevante) que hay muchas personas y grupos trabajando en nuevos protocolos y formas de hacer la comuncaci√≥n m√°s r√°pida y segura. Un ejemplo de ello es GraphQL. Facebook desarroll√≥ GraphQL y comenz√≥ a utilizarlo en aplicaciones m√≥viles en el 2012. En 2015, se habilit√≥ la especificaci√≥n de GraphQL como open source.\n\n\nFunciones CRUD\nLas API REST funcionan por medio de solicitudes HTTP y pueden ejecutar funciones t√≠picas de bases de datos. Si est√°s familiarizado/a con SQL, de seguro √©sto lo conoces. Si no tienes idea qu√© es SQL, deber√≠as preocuparte si lo que haces es an√°lisis de datos. Pero ese tema se escapa de este tutorial, pero lo veremos mas adelante.\nSi te interesa aprender SQL (que deber√≠as!!!!!!), en internet est√° repleto de cursos gratuitos, libros y videos de YouTube. Pero te recomiendo que veas un peque√±o trozo del siguiente video (hasta el minuto 10, el link te lo dej√© marcado desde donde empezar). Ese video no es de API propiamente tal, pero los conceptos de las funciones de bases de datos aplican perfectamente. As√≠ que dale una mirada a esos minutos y sigue con el art√≠culo.\nSigamos‚Ä¶\nREST define un conjunto de funciones como GET, PUT, DELETE, etc. que los clientes pueden utilizar para acceder a los datos del servidor. Sin embargo, las GET (obtener) te las encontrar√°s por todas partes y son las que debes manejar s√≠ o s√≠ en una API. Es lo b√°sico.\nPor eso, en esta serie de art√≠culos me dedicar√© principalmente a este tipo de m√©todos.\n\n\nTipos de API\nOtra forma de clasificar a las API es por las funciones que desarrollan, pudiendo ser privadas o p√∫blicas. Las privadas se usan al interior de las instituciones para conectar aplicaciones dentro de ella. Las p√∫blicas est√°n, como sun nombre lo indica, abiertas a personas externas. En este √∫ltimo tipo, se pueden requerir claves o credenciales de acceso, o bien, pueden ser de libre acceso.\nEstas credenciales de acceso pueden ser pricipalmente de 2 tipos:\n\nTokens de autenticaci√≥n\n\nSe utilizan para autorizar a los usuarios a hacer la llamada a la API. Los tokens de autenticaci√≥n comprueban que los usuarios son quienes dicen ser y que tienen los derechos de acceso para esa llamada concreta a la API. Por ejemplo, cuando inicia sesi√≥n en el servidor de correo electr√≥nico, el cliente de correo electr√≥nico utiliza tokens de autenticaci√≥n para un acceso seguro.\nClaves de API\n\nLas claves de API verifican el programa o la aplicaci√≥n que hace la llamada a la API. Identifican la aplicaci√≥n y se aseguran de que tiene los derechos de acceso necesarios para hacer la llamada a la API en cuesti√≥n. Las claves de API no son tan seguras como los tokens, pero permiten supervisar la API para recopilar datos sobre su uso. Es posible que haya notado una larga cadena de caracteres y n√∫meros en la URL de su navegador cuando visita diferentes sitios web. Esta cadena es una clave de la API que el sitio web utiliza para hacer llamadas internas a la API.\n\nFuente: https://aws.amazon.com/es/what-is/api/#seo-faq-pairs#how-to-secure-a-rest-api\nEsto que acabamos de ver es super importante. Es necesario que aprendas a autenticarte en la consulta de la API, pues es muy frecuente que √©stas est√©n protegidas por temas de seguridad, ya sea por un token o una clave.\n\n\nEn resumen\nVimos bastantes conceptos y fundamentos sobre las API, y que son necesario que tengas en tu mente. Es importante que entiendas c√≥mo funcionan ‚Äúpor detr√°s‚Äù, al menos, desde lo b√°sico.\n\nRevisamos la importancia de conocer sobre las API.\nVimos qu√© son las API y c√≥mo permiten que los sistemas se comuniquen.\nExplicamos qu√© son los servidores y los clientes.\nMencionamos las funciones m√°s clasicas de las API.\n\n\n\n¬øQu√© se viene?\nEn el pr√≥ximo art√≠culo ya empezaremos a ver c√≥digo de Python y veremos los primeros pasos para consumir una API.\nSaludos!!! ü§ó\nRecuerda leer el resto de mis art√≠culos."
  },
  {
    "objectID": "posts/api-fortnite/index.html",
    "href": "posts/api-fortnite/index.html",
    "title": "Descargu√© casi 3000 im√°genes de Fortnite!",
    "section": "",
    "text": "Hola nuevamente!!!\nSi es primera vez que est√°s por ac√°, espero que este contenido te sea interesante y logre desquearte alguna idea. Si ya me conoces, te agradezco estar por ac√° otra vez.\nEl post de hoy es medio off-topic de las tem√°ticas que habitualmente hablo en el blog, pero me parece que es puede llegar a ser de inter√©s para muchas personas. Habitualmente mezclo cosas de distintas tem√°ticas, algo como la medicina traslacional jajaja üòÖ\nTe cuento‚Ä¶\nMi hijo mayor es un entusiasta del dise√±o gr√°fico y a los videojuegos, como buen pre-adolescente. La cosa es la semana pasada me dijo que habia encontrado una web en donde hay muchas skins y gr√°ficas de Fortnite. Se supone que ah√≠ estan todas las im√°genes de los personajes, las armas, gliders y esas cosas del juego.\nComo le gusta dise√±ar con Photoshop, se descarga las im√°genes que le gustan, las edita y haces alguno dise√±os con ellas. El tema es que para descargarlas, hay que navegar por una serie de men√∫s y hacer varios clics para tener la imagen en una buena resoluci√≥n, para luego descargarla.\nLo que me pregunt√≥ fue que si hab√≠a una forma de descargar todas las im√°genes del sitio de forma masiva, pero las de buena calidad, ya que hacerlo de a una era muy lento.\nOk‚Ä¶ pues tenemos nuestro problema a resolver!!!\nAh!! De ahora en adelante, nombrar√© a mi hijo como mi cliente."
  },
  {
    "objectID": "posts/api-fortnite/index.html#el-problema",
    "href": "posts/api-fortnite/index.html#el-problema",
    "title": "Descargu√© casi 3000 im√°genes de Fortnite!",
    "section": "El problema",
    "text": "El problema\nEn programaci√≥n partimos desde un problema que es necesario resolver. Bueno, hay un paso clave ac√° que es el de comprender profundamente el problema de modo de buscar las mejores alternativas de soluci√≥n, que incluso, pueden no ser de programaci√≥n como tal. Ahora, si este fuera un caso de Ciencia de Datos, igual hay un paso previo que es el de an√°lisis de factibilidad, pero de eso no te voy a hablar ahora. Ya lo veremos en una pr√≥xima oportunidad.\nPues bien, veamos el problema un poco m√°s en detalle‚Ä¶\nLa idea es poder disponer de todas las im√°genes de Fortnite en buena calidad para poder realizar dise√±os. El tema, es que descargarlas manualmente desde la web es una tarea muy tediosa y lenta. Adem√°s, que la cantidad de im√°genes son bastantes."
  },
  {
    "objectID": "posts/api-fortnite/index.html#posibles-soluciones",
    "href": "posts/api-fortnite/index.html#posibles-soluciones",
    "title": "Descargu√© casi 3000 im√°genes de Fortnite!",
    "section": "Posibles soluciones",
    "text": "Posibles soluciones\nComo me hab√≠a mostrado la web, lo primero que se me pas√≥ por la mente fue hacer un scraper para descargar las im√°genes. Estuve indagando esta opci√≥n, pero la desech√© al poco tiempo. Los motivos fueron que algunas im√°genes no er√°n de buena calidad (la gran mayor√≠a si lo era, en todo caso). Adem√°s, las rutas en algunos casos eran extra√±as y algunos botones para acceder a las secciones ten√≠an bastante variabilidad. Todas esas cosas hac√≠a que codificar el scraper sea m√°s complejo. Posiblemente lo habr√≠a resuelto con algunos d√≠as de trabajo, pero ten√≠a un requerimiento del cliente expl√≠cito: necesitaba las im√°genes antes de las 14hrs. ¬øPor qu√©? No tengo idea jajaja\nPero bueno, fue as√≠. Ten√≠a un par de horas, entonces.\nComo pens√© que en el scraper me tomar√≠a m√°s tiempo del disponible, indagu√© otras opciones.\nFortnite es un juego ultra conocido. Mi pensamiento era buscar otra web similar e intentar el scraper, pero mi cliente no deseaba otra web.\nEn la b√∫squeda de webs me surgi√≥ la duda‚Ä¶ Bueno, y esta gente, ¬øDe d√≥nde saca las im√°genes? Porque de alg√∫n lado las sacan, no? No creo que se scrapeen mutuamente. Adem√°s, que son cientos o miles de im√°genes y que constantemente se est√°n actualizando. Esa labor no se hace de forma manual.\nEntonces, pens√© que deb√≠a haber un repositorio central con todas las im√°genes y que los desarrolladores sacaban desde ah√≠ sus datos. Lo cl√°sico en estos casos es usar una API (Application Programming Interface, o Interfaz de Programaci√≥n de Aplicaciones en espa√±ol).\nTe comento que estoy desarrollando una serie de art√≠culos para ense√±ar qu√© son las API y c√≥mo consumirlas. Puedes leer este otro art√≠culo en donde explico desde cero qu√© son las API. A modo de ultra resumen, una API podemos entenderlas como la forma en que 2 sistemas inform√°ticos se comunican para lo cual usan una serie de protocolos y estandarizaciones de modo que se puedan entender. Entonces, una parte hace una solicitud y la otra (el servidor) responde con algunos determinados datos. Si no sabes mucho del tema, igaual te aconsejo que te leas ese art√≠culo primero y lueg vuleve a √©ste, para que te quede todo m√°s claro.\nEn este caso, me puse a buscar alguna API de Fortnite que estuviera disponible.\nHay una web que me gusta bastante y que suelo usar, pues tiene cientos de API y es bastante ordenada. Se llama Rapid API.\n\n\n\n\n\nBueno, investigando la web de Rapid API encontr√© varias de Fortnite.\nMe llam√≥ la atenci√≥n una de ellas, pues es bastante completa, tiene baja latencia, buena disponibilidad y es gratis.\nEstuve viendo la API junto a mi cliente y le pareci√≥ una buena idea.\nPues bien! Ahora vamos a programar un poco para implementar el consumo de la API."
  },
  {
    "objectID": "posts/api-fortnite/index.html#la-soluci√≥n",
    "href": "posts/api-fortnite/index.html#la-soluci√≥n",
    "title": "Descargu√© casi 3000 im√°genes de Fortnite!",
    "section": "La soluci√≥n",
    "text": "La soluci√≥n\nEl lenguaje que usar√© es R, pues es el que m√°s controlo.\nPara poder usar la API hay que suscribirse a ella. Esto te genera un token para que puedas interectuar y autentizarte en ella. Bueno, la API solo permite aplicar m√©todos GET. En todo caso, eso me da igual, pues es justamente lo que necesitamos. As√≠ que todo bien.\n\n\n\n\n\nLa API tiene varios endpoints. Los endpoint de un API son puntos finales o direcciones en un servidor que pueden ser accedidos a trav√©s de la red (como Internet) para recibir o enviar datos. Un API puede tener m√∫ltiples endpoint, y cada uno de ellos puede realizar una funci√≥n espec√≠fica, como obtener datos de una base de datos, enviar un correo electr√≥nico, o realizar una acci√≥n en un sistema externo.\nEn resumen, los endpoint de un API son puntos de acceso a los servicios y funcionalidades que ofrece una aplicaci√≥n o sistema a trav√©s de la red. Para nuestro caso, la API nos ofrece varias opciones. Entre ellas hay una que nos pareci√≥ interesante y es que nos permite acceder a un listado con todos los skins y elementos de los personajes. A primera vista parece ser lo que necesitamos. Pero es necesario darle una vista un poco m√°s en detalle.\nEntonces, veamos c√≥mo pedir esos datos y ver qu√© nos da‚Ä¶\nPara implementar el m√©todo GET en R, podemos usar este c√≥digo:\n\nlibrary(httr)\n\nurl <- \"https://fortnite1.p.rapidapi.com/items/list\"\n\nresponse <- VERB(\"GET\", url, add_headers(\n    \"Authorization\" = \"string\", \n    \"X-RapidAPI-Key\" = \"xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\", \n    \"X-RapidAPI-Host\" = \"fortnite1.p.rapidapi.com\"), \n    content_type(\"application/octet-stream\"))\n\ncontent(response, \"text\")\n\nEn Python, podemos hacerlo de la siguiente forma:\n\nimport requests\n\nurl = \"https://fortnite1.p.rapidapi.com/items/list\"\n\nheaders = {\n    \"Authorization\": \"string\",\n    \"X-RapidAPI-Key\": \"xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\",\n    \"X-RapidAPI-Host\": \"fortnite1.p.rapidapi.com\"\n}\n\nresponse = requests.request(\"GET\", url, headers=headers)\n\nprint(response.text)\n\nHe reemplazo la API-key por XXXXXXX l√≥gicamente por temas de seguridad. Ese dato es √∫nico para cada persona que se suscriba a a API, asi que reempl√°zalo por el que te corresponda.\nLa respuesta del servidor es un JSON.\nEl JSON (JavaScript Object Notation) es un formato de texto utilizado para representar datos estructurados. Es muy com√∫n en el desarrollo de aplicaciones y se utiliza para intercambiar datos a trav√©s de la red (como Internet).\nUn archivo o texto JSON se compone de pares de clave-valor, que se encierran entre llaves {}. Cada par de clave-valor se separa con una coma. Las claves son cadenas de texto (encerradas entre comillas) y los valores pueden ser de diferentes tipos, como n√∫meros, cadenas de texto, o incluso otros objetos JSON anidados.\nAqu√≠ hay un ejemplo de c√≥mo se ver√≠a un objeto JSON:\n\n{\n  \"nombre\": \"Juan\",\n  \"edad\": 30,\n  \"direcci√≥n\": {\n    \"calle\": \"Calle Falsa\",\n    \"ciudad\": \"Ciudad Falsa\",\n    \"pa√≠s\": \"Pa√≠s Falso\"\n  },\n  \"intereses\": [\"lectura\", \"deportes\", \"viajes\"]\n}\n\nEn este ejemplo, tenemos un objeto JSON con cuatro pares de clave-valor: ‚Äúnombre‚Äù, ‚Äúedad‚Äù, ‚Äúdirecci√≥n‚Äù y ‚Äúintereses‚Äù. El valor de ‚Äúnombre‚Äù es una cadena de texto, el valor de ‚Äúedad‚Äù es un n√∫mero, el valor de ‚Äúdirecci√≥n‚Äù es otro objeto JSON anidado y el valor de ‚Äúintereses‚Äù es una lista de cadenas de texto.\nJSON es muy utilizado en el desarrollo de aplicaciones porque es un formato sencillo y f√°cil de leer tanto para personas como para m√°quinas. Adem√°s, es ampliamente compatible con diferentes lenguajes de programaci√≥n y plataformas.\nOk‚Ä¶ ya con eso explicado a grandes rasgos, veamos que nos da nuestra API de Fortnite:\n\n> content(response, \"text\")\n\n[1] \"{\\\"lastUpdate\\\":0,\\\"lanuage\\\":\\\"en\\\",\\\"data\\\":[{\\\"itemId\\\":\\\"7fe1e64e-fc28-4a2c-bb1e-d5711a04fb16\\\",\\\"lastUpdate\\\":1668185110,\\\"item\\\":{\\\"name\\\":\\\"Gods of Thunder Pack\\\",\\\"description\\\":\\\"\\\",\\\"type\\\":\\\"bundle\\\",\\\"rarity\\\":\\\"epic\\\",\\\"series\\\":\\\"marvel\\\",\\\"cost\\\":2500,\\\"upcoming\\\":false,\\\"images\\\":{\\\"icon\\\":\\\"https://dropin-bucket.mativecdn.com/cosmetics/br/7fe1e64e-fc28-4a2c-bb1e-d5711a04fb16_l5bp7gje/icon.png\\\",\\\"featured\\\":null,\\\"background\\\":\\\"https://dropin-bucket.mativecdn.com/cosmetics/br/7fe1e64e-fc28-4a2c-bb1e-d5711a04fb16_l5bp7gje/icon.png\\\",\\\"information\\\":\\\"https://dropin-bucket.mativecdn.com/cosmetics/br/7fe1e64e-fc28-4a2c-bb1e-d5711a04fb16_l5bp7gje/icon.png\\\"},\\\"backpack\\\":{},\\\"obtained\\\":\\\"2500\\\",\\\"obtained_type\\\":\\\"vbucks\\\",\\\"ratings\\\":{\\\"avgStars\\\":3.68,\\\"totalPoints\\\":2228,\\\"numberVotes\\\":606},\\\"costmeticId\\\":\\\"7fe1e64e-fc28-4a2c-bb1e-d5711a04fb16_l5bp7gje\\\".....\n\nQu√© es eso!!! üòñ\nRecort√© la respuesta, porque era muy larga. En todo caso, verlo as√≠ es medio complejo y se ve horrible. Se ve que tiene varios datos, pero tratemos de darle una forma m√°s simple de leer y ver si realmente nos sirve para resolver el problema de mi cliente.\nPara el caso de R, podemos usar la librer√≠a jsonlite que nos facilita leer este tipo de respuestas y transformarla a un objeto con el cual podamos interactuar de mejor forma:\n\nitems <- fromJSON(rawToChar(response$content))\n\nCon eso obtenemos una lista ordenada con todos los datos de la API.\n\ndf <- unnest(items$data$item, cols = c(images, backpack, ratings))\n\nLuego, pasamos esta lista a un dato de tipo dataframe, que nos permite estructurar los datos como una tabla y que facilita ver que hay dentro.\nNos da como resultado algo as√≠. Ojo, el archivo tiene m√°s de 8300 filas y 19 variables (columnas).\n\n\n\n\n\nAl ver que hay, vemos que contiene varios links a im√°genes .png\nCreo que nos gusta!! üéâ\nEstuvimos mirando, en detalle, los datos y los links. Encontramos que hay algunos variables que contienen links a las im√°genes que est√°bamos buscando. Si!!! Efectivamente. Ten√≠amos un listado muy extenso con los detalles de los personajes de Fortnite con link a sus skins, armas y una serie de elementos.\nGenial!!!\n\n\n\n\n\nMuy bien. Ya ten√≠amos varias cosas. Ese listado con los personajes y los links a sus im√°genes, con buena calidad. Incluso, varias de las que en la web tenian poca resoluci√≥n, ac√° estaban bien. Mejor a√∫n.\nAhora toca seguir con la soluci√≥n, porque hasta ahora solo tenemos un listado gigante de links. Que est√° bien, pero no tiene sentido copiar y pegar cada link en el navegador, para luego descargar las im√°genes. Al final, es casi lo mismo de antes. Y son miles de registros. No nos sirve esa l√≥gica. Debemos automatizar la lectura de esos links y las descargas.\nEntonces, vamos con la segunda parte‚Ä¶ Automatizar las descargas."
  },
  {
    "objectID": "posts/api-fortnite/index.html#automatizando",
    "href": "posts/api-fortnite/index.html#automatizando",
    "title": "Descargu√© casi 3000 im√°genes de Fortnite!",
    "section": "Automatizando",
    "text": "Automatizando\nTe voy a dejar el c√≥digo que hicimos y luego te lo explico‚Ä¶\n\nlibrary(tidyverse)\n\nfeatured <- df |> \n  filter(featured != is.na(featured)) |> \n  mutate(name = str_replace_all(name, \" \", \"-\")) |> \n  select(name, featured)\n\nurl <- \"https://dropin-bucket.mativecdn.com/cosmetics/br/character_darkazalea/featured.png\"\n\ndestfile <- \"C:/your/path/fortnite/img/image.png\"\n\ndownload.file(url, destfile, mode = \"wb\")\n\nPrimero, hago alguna manipulaci√≥n del listado de antes, pues no todos las variables nos sirven y ajustamos un poco los nombres de las variables.\nPara probar si los pasos para descargar las i√°genes est√°n bien, hago un prueba con 1 link (y no con los miles de registros). En R tenemos la funci√≥n download.file() que nos permite descargar archivos desde internet. A esta funci√≥n le tenemos que pasar 2 par√°metros: la url (donde est√° el archivo en la web) y la ruta de destino del archivo. Finalmente se agrega el modo = \"wb\" para que no hayan problemas de codificaci√≥n del archivo de tipo imagen (en Windows eso suele pasar en estos casos).\nLuego de la prueba, logramos descargar la imagen sin problemas.\nLo que toca ahora es aplicarlo a todas las im√°genes y links. Para ello, construimos el siguiente for:\n\nfor (i in (1:nrow(featured))){\n  \n  names <- featured$name[i]\n  url <- featured$featured[i]\n  destfile <- paste0(\"C:/your/path/fortnite/img/\", names ,\".png\")\n  \n  download.file(url, destfile, mode = \"wb\")\n  \n}\n\nEste c√≥digo no es el mejor, tiene varias cosas que se podr√≠an optimizar, pero recuerda que mi cliente me estableci√≥ un l√≠mite horario impostergable jajaja\nLo importante‚Ä¶\nEl c√≥digo funcion√≥!! Se tom√≥ su tiempo, varios minutos. Despu√©s de todo, descargu√© m√°s de 3000 im√°genes.\n¬øSe puede mejorar? Si, claro. Pero es posible que no vuelva a usar el c√≥digo en mucho tiempo y ya ten√≠a las im√°genes. Asi que ponerme a optimizarlo, en esos momentos, no ten√≠a mucho sentido.\nCuando le mostr√© el resultado a mi cliente qued√≥ sorprendido ü§Ø\nNo pod√≠a creer que ten√≠a m√°s de 3000 im√°genes de alta calidad de sus personajes de Fortnite en una sola carpeta. Y antes del plazo que me hab√≠a definido. Boom! Misi√≥n cumplida.\nBuen√≠simo!!! üéâ\nOk. Ahora que estoy escribiendo este art√≠culo, se me ocurri√≥ hacerle algunas mejoras al c√≥digo:\n\nsapply(seq_len(nrow(featured)), function(i) {\n  \n  names <- featured$name[i]\n  url <- featured$featured[i]\n  destfile <- paste0(\"C:/your/path/fortnite/img/\", names ,\".png\")\n  \n  download.file(url, destfile, mode = \"wb\")\n  \n})\n\nUs√© la funci√≥n seq_len() en lugar de 1:nrow(featured) para iterar sobre los √≠ndices de las filas de featured. Esto es m√°s r√°pido y evita tener que evaluar nrow(featured) en cada iteraci√≥n del bucle.\nAdem√°s, us√© la funci√≥n sapply() en lugar de un bucle for para descargar los archivos. Esto es m√°s r√°pido y puede ser m√°s f√°cil de leer y mantener."
  },
  {
    "objectID": "posts/api-fortnite/index.html#conclusiones",
    "href": "posts/api-fortnite/index.html#conclusiones",
    "title": "Descargu√© casi 3000 im√°genes de Fortnite!",
    "section": "Conclusiones",
    "text": "Conclusiones\nMuchas veces me contactan personas que est√°n empezando a programar y me dicen que no tienen proyectos para hacer, que no saben de donde sacar datos o cosas as√≠. A veces, no podemos acceder a los datos que nos gustar√≠a en especial en salud que hay hartos temas de privacidad. Pero podemos usar otros tipos de datos que de todas formas nos permiten ejercitar nuestras habilidades y l√≥gica. Tienes que ser curiso/a e indagar varias alternativas. No te quedes con lo primero que se te ocurra.\nAdem√°s, lo relevante es lograr identificar bien el problema, las limitaciones y los requisitos que nos permiten darnos cuenta de cu√°ndo lo resolver√≠amos. Es com√∫n no detenerse en esos puntos y lanzarse de una al codigo.\nSepara el problema en peque√±as partes, m√°s abordables y manejables.\nValida r√°pido con el cliente los avances y si las salidas cumplen sus expectativas. Todos esas partes (en que dividiste el problema) val√≠dalas y avanza en a medida que sea necesario.\nLa tecnolog√≠a sirve para mucho, pero una de las cosas que ayuda mucho es para acceder a grandes vol√∫menes datos y automatizar actividades, reduciendo las tareas manuales y los tiempos de ejecuci√≥n.\nY, finalmente, si tu soluci√≥n resuelve el problema, d√©jalo as√≠. En especial si es un proyecto puntual. Ya habr√° momentos de optimizar el c√≥digo y de mejorar su desempe√±o y esas cosas, pero es un trabajo posterior y solo para casos en donde tengas que reutilzar el c√≥digo o que haya que hacerle mantenimiento. Primero enf√≥cate en resolver el problema. Da igual si queda feo. A tu cliente no le interesa el c√≥digo, le interesa el resultado. La clave es resolver el problema!\nEspero que este art√≠culo te haya sido de utilidad.\nSaludos!"
  },
  {
    "objectID": "posts/compras-publicas/index.html",
    "href": "posts/compras-publicas/index.html",
    "title": "¬øA qui√©n le compra m√°s el Estado?",
    "section": "",
    "text": "Hola!!\n¬øTe gustar√≠a aprender a usar datos abiertos de compras p√∫blicas (Chile) y realizar este lindo gr√°fico?\nPues bueno, ac√° te ense√±o paso a paso.\nAh! No voy a realizar juicios de valor sobre los resultados. El fin de este art√≠culo es mostrar una forma de usar datos abiertos y utilizando programaci√≥n, realizar un breve an√°lisis de consolidaci√≥n de los datos y realizar el gr√°fico.\nDicho eso, pero sin dejar de mencionar que llama la atenci√≥n los 3 primeros proveedores a los cuales se le ha comprado m√°s jajaja üòÇ, vamos con el tema."
  },
  {
    "objectID": "posts/compras-publicas/index.html#los-datos",
    "href": "posts/compras-publicas/index.html#los-datos",
    "title": "¬øA qui√©n le compra m√°s el Estado?",
    "section": "Los datos",
    "text": "Los datos\nEl objetivo es investigar qu√© proveedores son los m√°s top en adjudicarse √≥rdenes de compra en salud.\nPara ello, usaremos los datos abiertos disponibles en la plataforma de ChileCompra.\nEsta web es interesante, pues contiene los datos de todas las compras p√∫blicas desde hace muchos a√±os hasta la fecha, que se han realizado por medio de Mercado P√∫blico. Por fines de investigaci√≥n, siempre es una buena idea indagar en qu√© tipo de datos est√°s disponibles y cuales no. Adem√°s, de entender las distintas variables que contiene. Para eso, puedes revisar la documentaci√≥n que el mismo sitio posee.\nOk. Partamos! üòé\nCargamos las librerias de R que usaremos:\n\nlibrary(tidyverse)\nlibrary(here)\nlibrary(archive)\nlibrary(data.table)\nlibrary(lubridate)\nlibrary(hrbrthemes)\n\nVamos a utilizar la descripci√≥n que sale en la documentaci√≥n sobre el uso de URL¬¥s para la descarga de los archivos.\nExiste la posibilidad de usar una API, pero requiere de un registro previo que no tengo, por lo cual no usar√© esa opci√≥n. Sin embargo, esa ser√≠a la opci√≥n ideal para nuestro caso.\n\n\n\n\n\nBasado en el ejemplo, como deseo descargar los datos de las √≥rdenes de compra de salud habr√≠a que tener construir un link similar a https://chc-oc-files.s3.amazonaws.com/sector/2020/Sem1/Salud.7z\nConstruyamos un peque√±o loop para generar las URL¬¥s necesarias para la descarga de los datos.\nEn la plataforma hay registros para los a√±os 2007 al 2022, separados por semestres en cada caso. Pero para efectos de este art√≠culo solo descargar√© desde el a√±o 2017 en adelante.\n\ntotal_url <- list()\n\nyear <- as.character(2017:2022)\n\nfor (i in year) {\n  url_sem1 <- paste0(\"https://chc-oc-files.s3.amazonaws.com/sector/\", i, \"/Sem1/Salud.7z\")\n  url_sem2 <- paste0(\"https://chc-oc-files.s3.amazonaws.com/sector/\", i, \"/Sem2/Salud.7z\")\n\n  total_url[[i]] <- c(url_sem1, url_sem2)\n}\n\nVeamos las URL¬¥s:\n\nurls <- unlist(total_url, use.names = FALSE)\nurls\n\n [1] \"https://chc-oc-files.s3.amazonaws.com/sector/2017/Sem1/Salud.7z\"\n [2] \"https://chc-oc-files.s3.amazonaws.com/sector/2017/Sem2/Salud.7z\"\n [3] \"https://chc-oc-files.s3.amazonaws.com/sector/2018/Sem1/Salud.7z\"\n [4] \"https://chc-oc-files.s3.amazonaws.com/sector/2018/Sem2/Salud.7z\"\n [5] \"https://chc-oc-files.s3.amazonaws.com/sector/2019/Sem1/Salud.7z\"\n [6] \"https://chc-oc-files.s3.amazonaws.com/sector/2019/Sem2/Salud.7z\"\n [7] \"https://chc-oc-files.s3.amazonaws.com/sector/2020/Sem1/Salud.7z\"\n [8] \"https://chc-oc-files.s3.amazonaws.com/sector/2020/Sem2/Salud.7z\"\n [9] \"https://chc-oc-files.s3.amazonaws.com/sector/2021/Sem1/Salud.7z\"\n[10] \"https://chc-oc-files.s3.amazonaws.com/sector/2021/Sem2/Salud.7z\"\n[11] \"https://chc-oc-files.s3.amazonaws.com/sector/2022/Sem1/Salud.7z\"\n[12] \"https://chc-oc-files.s3.amazonaws.com/sector/2022/Sem2/Salud.7z\"\n\n\nMuy bien!! Nos qued√≥ genial!\nAhora nos toca descargar los archivos desde esas direcciones web.\n\nfor (i in 1:length(urls)) {\n  destfile <- paste0(here::here(), \"/data/\", i, \".7z\")\n  url <- urls[i]\n  download.file(url, destfile, mode = \"wb\")\n}\n\nLos archivos tambi√©n pueden descargarse desde la misma plataforma sin necesidad de usar estas URL¬¥s, sino que por medio de la propia interfaz de Mercado P√∫blico. Sin embargo, me parece mejor idea el usar c√≥digo para automatizar esas tareas para as√≠ evitar errores manuales y facilitar la descarga de nuevos archivos en un futuro.\nF√≠jate que los archivos vienen comprimidos en un formato 7z.\nSi abrimos un archivo, nos encontraremos con otros 3 √≥ 4 en su interior:\n\nConvenio marco\nLicitaciones\nTrato directo\nCompra √°gil\n\nEn mi caso, me interesa indagar en los tratos directos.\n¬øEl motivo?\nMe parecen m√°s interesantes esos datos, puesto que hay gran variedad de compras y es en esta modalidad de compra en donde, muchas veces, se presta para fraude o problemas de legitimidad. Aunque con este art√≠culo no pretendo realizar un an√°lisis de ese estilo, podr√≠a ser de inter√©s para alguien m√°s y ocuparlo como base para fines de investigaci√≥n.\nA continuaci√≥n realizar√© algunos loops para automatizar la extracci√≥n de los datos.\nCabe mencionar que se podr√≠a hacer en menos lineas de c√≥digo, pero prefiero dejarlo as√≠ para que te sea m√°s simple de seguir el paso a paso.\nPara la funci√≥n de extracci√≥n (descomprimir) de m√°s abajo, necesitamos ponerle nombres de los archivos 7z para pas√°rselo a la funci√≥n archive_extract().\n\nfiles_zipped <- list()\n\nfor (i in 1:length(urls)) {\n  file_name_zip <- paste0(\"data/\", i, \".7z\")\n  files_zipped[[i]] <- file_name_zip\n}\n\nfiles_zipped <- unlist(files_zipped, use.names = FALSE)\n\nfiles_zipped\n\n [1] \"data/1.7z\"  \"data/2.7z\"  \"data/3.7z\"  \"data/4.7z\"  \"data/5.7z\" \n [6] \"data/6.7z\"  \"data/7.7z\"  \"data/8.7z\"  \"data/9.7z\"  \"data/10.7z\"\n[11] \"data/11.7z\" \"data/12.7z\"\n\n\nLos archivos de los ultimos 2 a√±os son diferentes a los anteriores y el nombre del archivo comprimido es distinto. Para resolver eso, implement√© un if(), seg√∫n qu√© archivo sea.\n\nhere::i_am(\"index.qmd\")\n\noc <- list()\n\nfor (i in files_zipped) {\n  archive_extract(archive = i, dir = \"data\")\n\n  if (file.exists(\"data/17TratoDirecto.csv\")) {\n    oc[[i]] <- fread(\"data/17TratoDirecto.csv\", encoding = \"Latin-1\")\n  } else {\n    oc[[i]] <- fread(\"data/17OCTratoDirecto.csv\", encoding = \"Latin-1\")\n  }\n}\n\nAhora pasamos el objeto que nos hab√≠amos creado a un data.frame y lo ‚Äúaplanamos‚Äù para no tener problemas despu√©s con los an√°lisis. Para lo √∫ltimo, usamos unnest().\n\nall_data <- rbindlist(oc) |> \n  unnest(cols = c())\n\nVeamos todas las variables que contiene el archivo.\n\ndim(all_data) #Filas y Columnas\n\n[1] 1429423      44\n\n\n\nnames(all_data) #Nombre de variables\n\n [1] \"codigoOC\"               \"FechaEnvioOC\"           \"NombreOC\"              \n [4] \"DescripcionOC\"          \"EstadoOC\"               \"ProcedenciaOC\"         \n [7] \"MonedaOC\"               \"MontoNetoOC\"            \"DescuentosOC\"          \n[10] \"CargosOC\"               \"ImpuestosOC\"            \"MontoTotalOC\"          \n[13] \"ImpuestosOC_CLP\"        \"MontoNetoOC_CLP\"        \"MetodoPago\"            \n[16] \"TipoDespacho\"           \"Financiamiento\"         \"UnidadCompra\"          \n[19] \"UnidadCompraRUT\"        \"RegionUnidadCompra\"     \"entCode\"               \n[22] \"Institucion\"            \"Sector\"                 \"Proveedor\"             \n[25] \"ProveedorRUT\"           \"ActividadProveedor\"     \"TamanoProveedor\"       \n[28] \"RegionProveedor\"        \"RubroN1\"                \"RubroN2\"               \n[31] \"RubroN3\"                \"CodigoProductoONU\"      \"ONUProducto\"           \n[34] \"NombreItem\"             \"DescripcionItem\"        \"CantidadItem\"          \n[37] \"UnidadMedida\"           \"MonedaItem\"             \"MontoNetoItem\"         \n[40] \"DescuentoItem\"          \"CargosItem\"             \"ImpuestoEspecificoItem\"\n[43] \"MontoTotalItem\"         \"MontoNetoItemCLP\"      \n\n\nImportante\nLos archivos descargados son grandes y cuando se descrompimen, los son mucho m√°s! Algunos llegan a pesar m√°s de 200 o 300 Mb. De hecho, el objeto con todos los datos combinados sobrepasa 1 GB y tiene casi 1 mill√≥n y medio de filas. Con un Excel no podr√°s abrirlo, pues supera su l√≠mite. Por eso es importante usar programaci√≥n. Otra instancia ser√≠a levantar un servidor SQL y cargar los datos, pero me parece que no vale la pena paa estos efectos.\nComo los archivos descargados son bastante grandes, es mejor borrarlos, pues ya tenemos lo que necesitamos.\n\nunlink(here::here(\"data/*\"), recursive = TRUE, force = TRUE)"
  },
  {
    "objectID": "posts/compras-publicas/index.html#an√°lisis",
    "href": "posts/compras-publicas/index.html#an√°lisis",
    "title": "¬øA qui√©n le compra m√°s el Estado?",
    "section": "An√°lisis",
    "text": "An√°lisis\nVoy a considerar s√≥lo las √≥rdenes de compra (OC) en estados de recepci√≥n conforme y aceptadas. Esta es una decisi√≥n que tom√© en base a lo que se menciona en el portal, en especial, a los est√°ndares OCDS.\nEstos est√°ndares permiten tener un lenguaje com√∫n sobre el estado de las OC y un formato estructurado que facilite la revisi√≥n, comprensi√≥n y consumo de los datos.\nLa decisi√≥n de usar solo esas etapas es m√°s que nada para sacar del an√°lisis todas aquellas OC que fueron rechazadas, tienen observaciones o sufrieron modificaciones en el camino. La idea es dejar aquellas efectivamente ejecutadas o que si bien a√∫n no ocurren, ya fueron aceptadas por las partes.\n\n\n\n\n\nAlgo que me parece interesante de observar, es el gasto asociado a esas OC durante los a√±os analizados.\n\nsuppliers <- all_data |>\n  filter(EstadoOC %in% c(\"Recepcion Conforme\", \"Aceptada\")) |>\n  separate(FechaEnvioOC, into = c(\"Fecha\", \"Hora\"), sep = \" \", remove = TRUE) |>\n  mutate(Fecha = dmy(Fecha)) |>\n  group_by(Proveedor, year = year(Fecha)) |>\n  summarise(mount = sum(as.numeric(MontoTotalItem), na.rm = TRUE)) |>\n  ungroup()\n\n\nsuppliers |>\n  group_by(year) |>\n  summarise(mount = sum(mount, na.rm = TRUE)) |>\n  ggplot(aes(year, mount)) +\n  geom_col() +\n  scale_y_continuous(labels = scales::comma) +\n  scale_x_continuous(breaks = c(2017, 2018, 2019, 2020, 2021, 2022)) +\n  theme_bw() +\n  labs(\n    x = \"A√±o\",\n    y = \"Monto Total OC ($)\"\n  )\n\n\n\n\nVemos que en los a√±os 2022 y 2021 los montos asociados a este tipo de OC aumentario de forma considerable respecto de los a√±os anteriores. Posiblemente, eso tenga mucha relaci√≥n con la pandemia y la necesidad de contar r√°pidamente con insumos, medicamentos, infraestructura, personal, ex√°menes y muchas otras cosas.\nSi vemos los proveedores, podemos hace un r√°nking de √©stos y revisar cu√°les fueron los 5 de cada a√±o que tuvieron mayores cantidades de transacciones por OC:\n\ntop5_by_year <- suppliers |>\n  mutate(Proveedor = str_to_upper(Proveedor)) |>\n  arrange(desc(mount)) |>\n  group_by(year) |>\n  do(head(., 5)) |>\n  ungroup()\n\ntop5_by_year |>\n  DT::datatable()\n\n\n\n\n\n\nSi tienes inter√©s en profundizar en el tema, ya tienes todo lo necesario. Tienes los datos y el c√≥digo para replicarlo. Puedes seguir investigando los registros, de seguro hay cosas entretenidas. Quiz√°s quieras dirigir el an√°lisis a un proveedor en espec√≠fico o a una instituci√≥n p√∫blica en especial. La verdad, no hay l√≠mites. Solo tu curiosidad e imaginaci√≥n."
  },
  {
    "objectID": "posts/compras-publicas/index.html#gr√°fico",
    "href": "posts/compras-publicas/index.html#gr√°fico",
    "title": "¬øA qui√©n le compra m√°s el Estado?",
    "section": "Gr√°fico",
    "text": "Gr√°fico\nPodemos graficarlo para ver mejor esos datos y analizar el comportamiento a lo largo de los a√±os de los proveedores con m√°s montos en OC.\nPara eso usaremos la famosa libreria ggplot2 con algunos agregados para hacer el gr√°fico m√°s atractivo.\n\nplot <- top5_by_year |>\n  mutate(Proveedor = fct_reorder(Proveedor, mount, .fun = \"sum\"),\n         mount_mill = mount/1e6) |> \n  ggplot(aes(Proveedor, mount_mill, fill = factor(year))) +\n  geom_col(position = \"stack\") +\n  coord_flip() +\n  scale_y_continuous(labels = scales::comma) +\n  expand_limits( y = c(0, 250000)) +\n  labs(title = \"Compras p√∫blicas por trato directo en el sector salud\",\n       subtitle = \"Se consideran las OC con estados de recepci√≥n conforme y aceptadas\",\n       x = \"\",\n       y = \"Millones de pesos ($)\",\n       fill = \"A√±o\",\n       caption = \"Fuente: ChileCompra (https://datos-abiertos.chilecompra.cl)\\nElaborado por Paulo Villarroel\") +\n  theme_ipsum_rc(grid = \"XY\") +\n  theme(axis.text.x = element_text(hjust = c(0, 0.5, 0.5, 0.5, 1)))\n\n\n\n\n\n\nSi quieres ver la imagen m√°s grande, dale al boton derecho y abrir en nueva ventena.\nSi, Si. Es el mismo gr√°fico del inicio ü•∞\nListo!!"
  },
  {
    "objectID": "posts/compras-publicas/index.html#finalmente",
    "href": "posts/compras-publicas/index.html#finalmente",
    "title": "¬øA qui√©n le compra m√°s el Estado?",
    "section": "Finalmente",
    "text": "Finalmente\nEspero que te haya gustado este art√≠culo. Trat√© de ser lo m√°s explicativo posible. Adem√°s, de dejarte el paso a paso para que tu sigas adelante.\nLos datos abiertos son una muy buena oportunidad para que la ciudadan√≠a pueda realizar fiscalizaci√≥n y auditoria al funcionamiento de los organismos p√∫blicos. Eso si, se requiere de ciertos conocimientos para saber d√≥nde encontrar esos datos y para poder analizarlos. Es mi deseo que este art√≠culo te sea de ayuda, despierte tu curiosidad y seamos agentes de divulgaci√≥n cient√≠fica de cara a la gente.\nNos vemos!!"
  },
  {
    "objectID": "posts/contando-letras/index.html",
    "href": "posts/contando-letras/index.html",
    "title": "Contando letras",
    "section": "",
    "text": "Hola!!\nQue bueno que est√©s por estos lados nuevamente! Porque ya has le√≠do mis otros art√≠culos, verdad??\nVerdad!!??\nOjito‚Ä¶\njajajaja ü§£\nYa, ok‚Ä¶ En serio. Lo que vamos a ver en este art√≠culo es algo interesante. O es lo que pienso. Lo que har√© es resolver un reto de l√≥gica de programaci√≥n (o kata le dicen algunos). Pero quiero darle una peque√±a vuelta de tuerca al tema. No quiero que solo sea poner el c√≥digo y punto. Sino que explicar√© varias formas de resolverlo, algunas m√°s verbosas (es decir, con m√°s l√≠neas de c√≥digo) para que sea m√°s simple de seguir la l√≥gica y el paso a paso. La idea es evitar la ‚Äúmagia‚Äù o c√≥digos que hacen todo por debajo y uno ni se entera. Los explicar√© de forma progresiva. Tambi√©n usar√© otras formas de resolverlo con menos c√≥digo. Mi intenci√≥n es que veas que, generalmente, hay varias formas de resolver las cosas en programaci√≥n. No hay mejores o peores perse, pero hay algunos que son m√°s eficientes que otros o que son mas legibles y comprensibles.\nPara resolverlo usar√©, principalmente, el lenguaje de progamaci√≥n R y tambi√©n pondr√© algo en Python por ah√≠. Me interesa que entiendas la l√≥gica de la soluci√≥n, el lenguaje que uses es lo de menos.\nPor esos mismos motivos (para la l√≥gica), es que evitar√© usar librer√≠as o dependencias externas a las del propio lenguaje y que tienden a estar presentes en muchos lenguajes, quiz√°s con una sintaxis (la forma en c√≥mo se escribe) algo distinta, pero que hacen lo mismo. Y como te mencion√©, no busco generar el c√≥digo m√°s corto, eficiente o lindo, sino que sea f√°cil de entender.\nEstamos?\n\nEl problema\nEl problema no es nuevo ni una creaci√≥n m√≠a, sino que es de una web cl√°sica en donde puedes encontrar muchos desaf√≠os o retos de c√≥digo, llamada CodeWars.\nEl desaf√≠o que veremos ahora dice as√≠:\n\n\n\n\n\nB√°sicamente, la idea es crear una funci√≥n que permita contar las vocales que tiene un determinado texto.\nSe entiende?\nEs simple. A priori, al menos.\n\n\nSoluciones\nYa. Tenemos nuestro desaf√≠o, ahora veamos c√≥mo podemos resolverlo.\nMi primer approach ser√° con R. Crear√© una funci√≥n que cuente las vocales de un texto que le pasemos a esa funci√≥n. Recuerda que para crear o definir una funci√≥n se usa function. A ella le agregamos un par√°metro text, que ser√° el texto que queremos analizar y contar las vocales.\nAntes de partir, vamos a definir unos test unitarios. Los test unitarios son pruebas que consisten en aislar una parte del c√≥digo y comprobar que funciona a lo esperado. Son peque√±os tests que validan el comportamiento de un objeto y la l√≥gica, fundamentalmente. En R, para realizar este tipo de test usaremos la librer√≠a testthat. Se escapa de los alcances de este art√≠culo, pero te recomiendo que aprendas a realizar testing, pues es una forma de asegurarte que las cosas salen como deber√≠an salir, lo cual es especialmente √∫til cuando tienes desarrollos un poco m√°s extensos, hay varias personas metiendo mano o haces cambios en el c√≥digo y no quieres roper nada (roper es un t√©rmino usado en programaci√≥n para aludir a que ya no funciona bien algo). Si corremos el c√≥digo que te dejo abajo ver√°s que arroja puros errores!!!!! No!!!!!!!\nBueno, qu√© esperas? Si no hemos definido ninguna funci√≥n ni nada jajajaj ü§™\n\nlibrary(testthat) # librer√≠a para test unitarios en R\n\n\ntest_count_vowels <- test_that(\"Example Tests\", {\n  expect_equal(count_vowels(\"Esta es otra frase\"), \"Number of vowels: 7\")\n  expect_equal(count_vowels(\"Ma√±ana salimos al parque\"), \"Number of vowels: 10\")\n  expect_equal(count_vowels(\"\"), \"Number of vowels: 0\")\n})\n\n# ‚îÄ‚îÄ Error (Line 2): Example Tests ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n# Error in `count_vowels(\"Esta es otra frase\")`: could not find function \"count_vowels\"\n# Backtrace:\n#  1. testthat::expect_equal(count_vowels(\"Esta es otra frase\"), \"Number of vowels: 7\")\n#  2. testthat::quasi_label(enquo(object), label, arg = \"object\")\n#  3. rlang::eval_bare(expr, quo_get_env(quo))\n\nOk. Ya tenemos nuetro test que falla por todos lados. Ahora debemos arreglarlo.\nPartiremos definiendo nuestra funci√≥n count_vowels, que como te mencion√© antes, le pasaremos como argumento un texto en el par√°metro text.\nA ver‚Ä¶ mi l√≥gica de pensamiento es la siguiente‚Ä¶ como necesito contar las vocales que tiene un texto, primero le tengo que decir al computador cu√°les son las letras que deseo contar (las vocales). Una vez que tenga eso claro, voy a buscar esas vocales en el texto, contar cu√°ntas hay y sumarlas.\nTener en mente la l√≥gica es lo b√°sico. Antes de ponder a escribir c√≥digo. Primero dise√±a la soluci√≥n (o algo parecido) en tu mente. Muchas veces yo uso un l√°piz y papel para tirar dibujos que me permitan entender los pasos y partes.\nVamos con el c√≥digo. Dentro de la funci√≥n especificamos:\n\nCreamos el objeto vowels que es un vector de strings con las vocales.\nPara poder buscar las vocales en el texto, necesito separar el texto en todas sus letras, porque mi b√∫squeda y conteo es por letra, no por un conjunto de ellas.\nCreamos el objeto split_text que ser√° un vector con todas las letras del texto ya separadas. Para evitar complicaciones, pondremos todo a min√∫sculas. Recuerda que C no es igual a c, por ejemplo. Este tema no lo hab√≠a considerado al inicio, pero me di cuenta que no sumaba bien con algunas palabras y era por el asunto de las may√∫sculas.\nComo deseo contar las vocales dentro del texto (ya separado por letras), usar√© un for para ello. Dejar√© un objeto char_count = 0 en donde ir√© acumulando las sumas de vocales.\nImplemento el for. F√≠jate que en la medida que recorro el vector del texto separado y encuentro una vocal, sumo 1 y lo voy acumulando. Para realizar esab b√∫squeda, us√© char %in% vowels.\nTe recomiendo que veas la documentaci√≥n del operador %in% para m√°s detalles, pero lo que hace es lo que te mencion√©, recorre un vector por cada uno de sus elementos.\nFinalmente, imprimo por consola ese n√∫mero acumulado de letras. Le puse un mensaje para que sea m√°s entendible la salida del c√≥digo.\n\nAs√≠ quedar√≠a esa implementaci√≥n:\n\ncount_vowels <- function(text) {\n  vowels <- c(\"a\", \"e\", \"i\", \"o\", \"u\")\n  split_text <- tolower(unlist(strsplit(text, \"\")))\n  char_count <- 0\n\n  for (char in split_text) {\n    if (char %in% vowels) {\n      char_count <- char_count + 1\n    }\n  }\n  print(paste0(\"Number of vowels: \", char_count))\n}\n\nPodemos probarla con algunos ejemplos:\n\ncount_vowels(\"hllp\")\n\n[1] \"Number of vowels: 0\"\n\ncount_vowels(\"paulo\")\n\n[1] \"Number of vowels: 3\"\n\ncount_vowels(\"Ma√±ana salimos al parque\")\n\n[1] \"Number of vowels: 10\"\n\n\nOk‚Ä¶ al parecer funciona‚Ä¶ Veamos c√≥mo le va a nuestra funci√≥n con el test unitario:\n\ntest_count_vowels <- test_that(\"Example Tests\", {\n  expect_equal(count_vowels(\"Esta es otra frase\"), \"Number of vowels: 7\")\n  expect_equal(count_vowels(\"Ma√±ana salimos al parque\"), \"Number of vowels: 10\")\n  expect_equal(count_vowels(\"\"), \"Number of vowels: 0\")\n})\n\n[1] \"Number of vowels: 7\"\n[1] \"Number of vowels: 10\"\n[1] \"Number of vowels: 0\"\nTest passed üò∏\n\n\nGenial!!!!\nPasamos!!!! üéâ\nVeamos una implementaci√≥n similar, pero esta vez en Python:\n\ndef getCount(text):\n    text = text.lower()\n    vowels = ['a','e','i','o','u']\n    numVowels = 0\n    \n    for char in text:\n        if (char in vowels):\n            numVowels += 1\n    \n    print(f\"Number of vowels: {numVowels}\")\n\nRevisemos unos ejemplos:\n\ngetCount(\"paulo\")\n\nNumber of vowels: 3\n\ngetCount(\"Ma√±ana salimos al parque\")\n\nNumber of vowels: 10\n\n\nOk‚Ä¶ tenemos nuestras primeras formas de resolver este desaf√≠o de c√≥digo.\nHagamos una peque√±a variaci√≥n al desaf√≠o. Anteriomente est√°bamos contando todas las vocales que ten√≠a un texto. ¬øPero qu√© pasa si ahora lo que queremos saber es cu√°l es la vocal que m√°s repite?\nHagamos nuestros test primero, que como ya sabes, van a fallar porque no tenemos nada a√∫n.\n\ntest_count_vowels <- test_that(\"Example Tests\", {\n  expect_equal(max_vowels(\"Esto es un texto\"), \"Vowel: e / Freq: 3\")\n  expect_equal(max_vowels(\"Hoy hizo calor\"), \"Vowel: o / Freq: 3\")\n  expect_equal(max_vowels(\"\"), \"No vowels in text\")\n})\n\n# ‚îÄ‚îÄ Error (Line 2): Example Tests ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n# Error in `max_vowels(\"Esto es un texto\")`: could not find function \"max_vowels\"\n# Backtrace:\n#  1. testthat::expect_equal(max_vowels(\"Esto es un texto\"), \"Vowel: e / Freq: 3\")\n#  2. testthat::quasi_label(enquo(object), label, arg = \"object\")\n#  3. rlang::eval_bare(expr, quo_get_env(quo))\n\nVamos a usar parte de lo que tenemos realizado ya, y su l√≥gica por cierto. Finalmente, igual necesitamos identificar las vocales que hay en el texto, pero en esta ocasi√≥n, en vez de sumarlas, lo que haremos es contar cada vocal cu√°ntas apariciones tiene y seleccionaremos la que tiene mayor frecuencia.\nLa diferencia m√°s relevante con el reto anterior es que ahora debemos contar individualmente cada vocal. Antes solo acumul√°bamos la suma de las aparaciones. Para eso, me parece que m√°s l√≥gico armar una tabla con las vocales que tiene el texto y la frecuencia de cada una.\n\ntext <- \"Esta es una prueba del texto\"\n\nvowels <- c(\"a\", \"e\", \"i\", \"o\", \"u\")\nsplit_text <- tolower(unlist(strsplit(text, \"\")))\ntable_freq <- data.frame(table(split_text[split_text %in% vowels]))\n\nEl objeto table_freq contiene la tabla que te mencionaba. F√≠jate que lo que hice fue transformar una tabla media rara a un dataframe, que es m√°s √∫til para trabajar y calcular el m√°ximo de frecuencias.\n\ntable(split_text[split_text %in% vowels])\n\n\na e o u \n3 5 1 2 \n\n\n\ndata.frame(table(split_text[split_text %in% vowels]))\n\n  Var1 Freq\n1    a    3\n2    e    5\n3    o    1\n4    u    2\n\n\nCon esa tabla ya hecha, lo que debemos hacer es buscar la frecuencia m√°s alta (max) e indicar la vocal que corresponde a ese valor. Para eso, analiza esta l√≠nea de c√≥digo table_freq$Var1[table_freq$Freq == max]\n\nmax_vowels <- function(text) {\n  vowels <- c(\"a\", \"e\", \"i\", \"o\", \"u\")\n  split_text <- tolower(unlist(strsplit(text, \"\")))\n  table_freq <- data.frame(table(split_text[split_text %in% vowels]))\n  max <- max(table_freq$Freq)\n  max_vowels_text <- table_freq$Var1[table_freq$Freq == max]\n\n  print(paste0(\"Vowel: \", max_vowels_text, \" / Freq: \", max))\n}\n\n\nmax_vowels(\"La salud necesita mejores personas\")\n\n[1] \"Vowel: e / Freq: 5\"\n\n\nVolvamos a probar nuestros test‚Ä¶\n\ntest_count_vowels <- test_that(\"Example Tests\", {\n  expect_equal(max_vowels(\"Esto es un texto\"), \"Vowel: e / Freq: 3\")\n  expect_equal(max_vowels(\"Hoy hizo calor\"), \"Vowel: o / Freq: 3\")\n  expect_equal(max_vowels(\"\"), \"No vowels in text\")\n})\n\n# [1] \"Vowel: e / Freq: 3\"\n# [1] \"Vowel: o / Freq: 3\"\n# [1] \"Vowel:  / Freq: -Inf\"\n# ‚îÄ‚îÄ Failure (Line 2): Example Tests ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n# max_vowels(\"Esto es un texto\") not equal to \"Vowel: 3 / Freq: 3\".\n# 1/1 mismatches\n# x[1]: \"Vowel: e / Freq: 3\"\n# y[1]: \"Vowel: 3 / Freq: 3\"\n# \n# ‚îÄ‚îÄ Warning (Line 4): Example Tests ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n# no non-missing arguments to max; returning -Inf\n# Backtrace:\n#  1. testthat::expect_equal(max_vowels(\"\"), \"No vowels in text\")\n#  4. global max_vowels(\"\")\n# \n# ‚îÄ‚îÄ Failure (Line 4): Example Tests ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n# max_vowels(\"\") not equal to \"No vowels in text\".\n# 1/1 mismatches\n# x[1]: \"Vowel:  / Freq: -Inf\"\n# y[1]: \"No vowels in text\"\n\nMmmmmmm. A ver. Los 2 primeros test pasaron, pero el tercero fall√≥, el que tiene un argumento vac√≠o.\nEn el ejemplo anterior, cuando le pasamos un argumento vacio no arroj√≥ error, pero ac√° si. Posiblemente el tema pasa por la tabla.\n\ntext <- \"\"\n\nvowels <- c(\"a\", \"e\", \"i\", \"o\", \"u\")\nsplit_text <- tolower(unlist(strsplit(text, \"\")))\ntable_freq <- data.frame(table(split_text[split_text %in% vowels]))\n\ntable_freq\n\n[1] Freq\n<0 rows> (or 0-length row.names)\n\n\nSi le pasamos un texto vac√≠o, no se genera la tabla, nos da como resultado 0 rows. Bueno, es l√≥gico. Si no hay nada, la tabla no existe b√°sicamente. Para resolver este problema, podr√≠amos agregar un condicional a la funci√≥n. Lo que deber√≠a pasar es que cuando se detecte que el texto es vac√≠o, nos de un mensaje de error o algo similar, y si no es as√≠, que haga el tema de la tabla y todo lo dem√°s.\n\nmax_vowels <- function(text) {\n  vowels <- c(\"a\", \"e\", \"i\", \"o\", \"u\")\n  split_text <- tolower(unlist(strsplit(text, \"\")))\n\n  if (text == \"\") {\n    \n    print(\"No vowels in text\")\n    \n  } else {\n    \n    table_freq <- data.frame(table(split_text[split_text %in% vowels]))\n    max <- max(table_freq$Freq)\n    max_vowels_text <- table_freq$Var1[table_freq$Freq == max]\n\n    print(paste0(\"Vowel: \", max_vowels_text, \" / Freq: \", max))\n    \n  }\n}\n\nCorramos nuevamente nuestros lindos test unitarios:\n\ntest_count_vowels <- test_that(\"Example Tests\", {\n  expect_equal(max_vowels(\"Esto es un texto\"), \"Vowel: e / Freq: 3\")\n  expect_equal(max_vowels(\"Hoy hizo calor\"), \"Vowel: o / Freq: 3\")\n  expect_equal(max_vowels(\"\"), \"No vowels in text\")\n})\n\n[1] \"Vowel: e / Freq: 3\"\n[1] \"Vowel: o / Freq: 3\"\n[1] \"No vowels in text\"\nTest passed üéâ\n\n\nYuju!!!! üòç\nTe dejo tarea para la casa. Mira el resultado siguiente:\n\nmax_vowels(\"eeeoooaqui\")\n\n[1] \"Vowel: e / Freq: 3\" \"Vowel: o / Freq: 3\"\n\n\nEpa! Ac√° las vocales e y o tienen ambas una frecuencia de 3 y son las m√°s altas. Nuestro c√≥digo no resuelve bien eso. Muestra correctamente las vocales, pero el mensaje es extra√±o, sale repetido y no se entiende bien. ¬øSe podr√≠a hacer mejor?\nEs tu turno!! Int√©ntalo.\n\nAhora te voy a mostrar unas cuantas soluciones, pero con menos c√≥digo. Es m√°s complejo de entender si no sabes bien lo que hace cada funci√≥n, pero me parece interesante mostrarte otras opciones de resolver el problema original (el de contar vocales).\nDale unas vueltas e investiga cada una.\nOpci√≥n 1:\n\nget_count_vowels <- function(text){\n  vowels <- nchar(gsub(\"[^aeiou]\", \"\", text, ignore.case = TRUE))\n  print(paste0(\"Number of vowels: \", vowels))\n}\n\nget_count_vowels(\"Esta es otra frase\")\n\n[1] \"Number of vowels: 7\"\n\n\nOpci√≥n 2:\n\nlibrary(stringr) # Usando esta librer√≠a\n\ncount_vowels_lib <- function(text) {\n  str_count(text, \"[AEIOUaeiou]\")\n}\n\ncount_vowels_lib(\"Esta es una prueba de texto\")\n\n[1] 11\n\n\nOpci√≥n 3:\n\ngetCount = lambda s: sum(s.count(i) for i in 'aeiou') # Opci√≥n un poco cr√≠ptica\n\ngetCount(\"Frase linda\")\n\n4\n\n\n\n\nFinalmente\nUf! Teminamos este art√≠culo. Espero que te haya gustado y, principalmente, haya sido de utilidad.\nEspero haber sido claro en la l√≥gica de pensamiento y la forma de resolver un problema. Adem√°s, de mostrarte muchas formas de abordarlo en distintos idiomas. Y algo que quiz√°s pas√≥ medio desapercibido, pero es muy relevante, son los test unitarios y la arquitectura de desarrollo TDD (Test Driven Development). Esto ayuda mucho a tener un c√≥digo sostenible y si le haces cambios y falla, te vas a dar cuenta.\nRecuerda dejar tus comentarios e inscribirte en el blog (en la secci√≥n de m√°s abajo).\nNos vemos!! ü§ó"
  },
  {
    "objectID": "posts/contribuye-al-codigo-abierto/index.html",
    "href": "posts/contribuye-al-codigo-abierto/index.html",
    "title": "Contribuye al c√≥digo abierto",
    "section": "",
    "text": "Hola (otra vez!!)\nHoy te quiero hablar de un tema, que considero, muy relevante: el c√≥digo abierto.\nEn ingl√©s al c√≥digo abierto se le denomina open source. Y es que m√°s que una forma de desarrollar c√≥digo de forma colaborativa, es una filosof√≠a o un movimiento. Es una l√≥gica que supera solo a la producci√≥n del software mismo, sino que abarca valores del dise√±o descentralizado y soluci√≥n de problemas de forma colectiva y abierta. Si deseas saber un poco m√°s del open source, puedes revisar este link.\nEn lo personal, me encanta esta filosof√≠a del open source. Y es que esta forma de trabajo puede abarcar no solo el desarrollo de c√≥digo, sino que de otras industrias y √°reas. La innovaci√≥n tiene mucho de √©sto, de desarrollar soluciones de forma colaborativa y abierta.\nHoy quiero que tu puedas contribuir al c√≥digo abierto. Si!! Que ayudes a generar conocimiento colectivo. Entonces debes conocer un par de cosas. Una de √©stas es el control de versiones GIT y la otra es alguna plataforma de repositorios remotos , como GitHub, GitLab o Bitbucket.\nPero cuando me puse a pensar en c√≥mo explicarte a contribuir al c√≥digo abierto, me parece relevante explicarte un paso antes. Y es que para que puedas hacer estas cosas, debes conocer algo de GIT."
  },
  {
    "objectID": "posts/contribuye-al-codigo-abierto/index.html#qu√©-es-git",
    "href": "posts/contribuye-al-codigo-abierto/index.html#qu√©-es-git",
    "title": "Contribuye al c√≥digo abierto",
    "section": "¬øQu√© es GIT?",
    "text": "¬øQu√© es GIT?\nEn la web de Microsoft indican:\nGit es un sistema de control de versiones distribuido, lo que significa que un clon local del proyecto es un repositorio de control de versiones completo. Estos repositorios locales plenamente funcionales permiten trabajar sin conexi√≥n o de forma remota con facilidad. Los desarrolladores confirman su trabajo localmente y, a continuaci√≥n, sincronizan su copia del repositorio con la copia en el servidor. Este paradigma es distinto del control de versiones centralizado, donde los clientes deben sincronizar el c√≥digo con un servidor antes de crear nuevas versiones.\nYa te contar√© m√°s sobre GIT en otros art√≠culos. En esta oportunidad ser√© muy concreto. Te explicar√© c√≥mo instalar GIT en tu computador para que puedas empezar a contribuir a proyectos de c√≥digo abierto lo antes posible."
  },
  {
    "objectID": "posts/contribuye-al-codigo-abierto/index.html#pasos-para-instalar-git",
    "href": "posts/contribuye-al-codigo-abierto/index.html#pasos-para-instalar-git",
    "title": "Contribuye al c√≥digo abierto",
    "section": "Pasos para instalar GIT",
    "text": "Pasos para instalar GIT\nInstalar GIT no es nada complejo. Es como cualquier programa que instalas. B√°sicamente es descargar el instalador y darle clic a ‚Äúsiguiente‚Äù muchas veces.\nDe todas formas, hace unos meses me toc√≥ instalar GIT y tom√© capturas de pantalla de cada paso. Te las dejo para que te sirvan de referencia.\n\n\n\nVersi√≥n de GIT Puede que la versi√≥n que instales no sea la misma que muestro ac√°, pero la instalaci√≥n no deber√≠a ser muy distinta."
  },
  {
    "objectID": "posts/contribuye-al-codigo-abierto/index.html#finalmente",
    "href": "posts/contribuye-al-codigo-abierto/index.html#finalmente",
    "title": "Contribuye al c√≥digo abierto",
    "section": "Finalmente‚Ä¶",
    "text": "Finalmente‚Ä¶\nSi sigues los pasos, no deber√≠as tener mayor dificultades para tener instalado GIT.\nEso es lo primero para empezar a contribuir al open source.\nEn el pr√≥ximo art√≠culo te explicar√© de forma muy precisa, c√≥mo hacer tu primera pull request, que es la forma por excelencia de contribuir a proyectos abiertos. Pero eso, lo veremos en detalle en mi pr√≥ximo art√≠culo.\nNos vemos!!! üòÅ"
  },
  {
    "objectID": "posts/creando-mi-primer-blog-con-quarto/index.html",
    "href": "posts/creando-mi-primer-blog-con-quarto/index.html",
    "title": "Creando mi primer blog con Quarto",
    "section": "",
    "text": "Hola!! ü§ò\nEste es el primer post de mi nuevo blog. Ac√° ir√© escribiendo sobre distintas tem√°ticas relacionadas al mundo de la programaci√≥n y la ciencia de datos.\nBienvenido/a nuevamente.\nEn esta oportunidad te voy a ense√±ar a crear tu propio blog, como el que est√°s viendo ahora. Si! Como este mismo. Te parece?\nPues vamos a ello!!\nLo primero que debes saber es que este blog est√° constru√≠do sobre la base Quarto. Quarto es una nueva plataforma open-source, lanzada hace poco tiempo, y que busca facilitar la publicaci√≥n de art√≠culos cient√≠ficos. La gracia que tiene es que soporta distintos lenguajes de programaci√≥n (R, Python, Julia) en una sola aplicaci√≥n.\nBueno, dentro de las cosas que permite hacer Quarto es crear un blog. Que es lo que haremos ahora."
  },
  {
    "objectID": "posts/creando-mi-primer-blog-con-quarto/index.html#paso-1-descargar-quarto",
    "href": "posts/creando-mi-primer-blog-con-quarto/index.html#paso-1-descargar-quarto",
    "title": "Creando mi primer blog con Quarto",
    "section": "Paso 1: Descargar Quarto",
    "text": "Paso 1: Descargar Quarto\nLo primero que tienes que hacer es descargar Quarto desde su web oficial. Elige la versi√≥n que corresponda a tu sistema operativo. La instalaci√≥n no tiene ninguna cosa extra√±a, es como cualquier programa nada m√°s.\n\n\n\n\n\nEn lo personal, estoy usando RStudio como editor de texto, pero se pueden usar otros como Visual Studio Code o Jupyter Notebooks. En este caso, explicar√© los pasos para RStudio."
  },
  {
    "objectID": "posts/creando-mi-primer-blog-con-quarto/index.html#paso-2-crea-un-proyecto",
    "href": "posts/creando-mi-primer-blog-con-quarto/index.html#paso-2-crea-un-proyecto",
    "title": "Creando mi primer blog con Quarto",
    "section": "Paso 2: Crea un proyecto",
    "text": "Paso 2: Crea un proyecto\nSi est√°s familiarizado con RStudio, sabr√°s lo que es un proyecto. Si no, pues un proyecto es un conjunto de archivos que se mantienen relacionados entre s√≠, de modo que sea m√°s simple el desarrollo y vinculaci√≥n. Adem√°s, de facilitar el uso de rutas relativas lo cual es genial a la hora de compartir c√≥digo o de proyectos m√°s grandes.\nEn RStudio debes ir al men√∫ superior y seleccionar New Project‚Ä¶\n\n\n\n\n\nEso te abrir√° una nueva pesta√±a, en donde debes seleccionar la opci√≥n New Directory.\n\n\n\n\n\nLuego, debes eligir el tipo de proyecto. En este caso, selecciona Quarto Blog.\n\n\n\n\n\nSe abrir√° una nueva ventana. Ac√° debes poner el nombre de la carpera que vas a crear (en Directory name). Verifica la carpeta en donde vas a crear este directorio. Luego dale a Create Project.\n\n\n\n\n\nCon eso, Quarto te crear√° una serie de archivos y la estructura del blog de forma autom√°tica!!!\nVer√°s que ahora tienes muchos archivos nuevos en tu visor."
  },
  {
    "objectID": "posts/creando-mi-primer-blog-con-quarto/index.html#paso-3-preview-de-tu-blog",
    "href": "posts/creando-mi-primer-blog-con-quarto/index.html#paso-3-preview-de-tu-blog",
    "title": "Creando mi primer blog con Quarto",
    "section": "Paso 3: Preview de tu blog",
    "text": "Paso 3: Preview de tu blog\nCon los pasos anteriores, Quarto te cre√≥ un blog completamente funcional. Eso si, con art√≠culos de muestra que despu√©s hay que eliminar o modificar, obviamente. Pero ya tienes la base.\nPara revisar c√≥mo se ve tu blog, debes darle clic al bot√≥n Render de la parte superior.\n\n\n\n\n\nVer√°s una serie de cosas que van a sair en la consola de RStudio, pero es parte del proceso de renderizado. O sea, para armar el blog y pasar los archivos a una p√°gina web.\nSi todo sale bien, se deber√≠a abrir tu blog en una pesta√±a de tu navegador.\nY listo!!!\nYa tienes tu primer blog üéâ\nEs posible que tu blog se muestra en el mismo RStudio en una ventana lateral. Esa opci√≥n me parece que viene por defecto (no recuerdo bien), pero yo la tengo configurada para que me aparezca el preview en una ventana del navegador. Me gusta m√°s esa opci√≥n, pues se ve mejor el contenido. Para poner esa opci√≥n, debes darle clic a la tuerca que est√° al lado derecho del bot√≥n Render y seleccionar Preview in Window.\n\nF√≠jate que la web est√° en tu computador y se est√° mosrando desde una direcci√≥n que dice localhost. Esto es por ese mismo motivo. A pesar de que se ve como una web a trav√©s de tu navegador, el blog solo est√° en tu computador. No est√° subido a internet ni alojado en ning√∫n servicio que permita que otros lo vean. Para eso, hay que hacer algunas cosas, que las veremos un poco m√°s adelante."
  },
  {
    "objectID": "posts/creando-mi-primer-blog-con-quarto/index.html#paso-4-crear-un-post",
    "href": "posts/creando-mi-primer-blog-con-quarto/index.html#paso-4-crear-un-post",
    "title": "Creando mi primer blog con Quarto",
    "section": "Paso 4: Crear un post",
    "text": "Paso 4: Crear un post\nCuando creas tu proyecto de blog con Quarto, la aplicaci√≥n crea la estructura y pone 2 post de ejemplo.\nSi observas los archivos, ver√°s una carpeta llamada posts. En esta carpeta es donde se deben ir guardando los archivos para los nuevos posts.\nPor defecto te crea 2 carpetas: welcome y post-with-code\nEsos son post de muestra, que los puedes eliminar. Pero antes que lo hagas, quiero que te fijes en c√≥mo estan estructurados. Esa ser√° la forma en que deber√°s crear tu pr√≥ximas entradas.\nCada art√≠culo (o post) consiste en una carpeta. Dentro de ella hay un archivo index.qmd y algunas im√°genes.\nEsta es la estructura de archivos que se te crear√° de forma autom√°tica:\n\nCreated _quarto.yml\nCreated index.qmd}\nCreated posts/welcome/index.qmd\nCreated posts/post-with-code/index.qmd\nCreated about.qmd\nCreated styles.css\nCreated posts/_metadata.yml\n\nEn la carpeta posts crea un nuevo archivo con el nombre de tu nueva entrada. Dentro de esa carpeta, crea un archivo .qmd y ll√°malo index. Es decir, te deber√≠a quedar un archivo de nombre index.qmd\nEsto es fundamental. Siempre usa index para nombrar a los nuevos posts. De lo contrario, Quarto no encontrar√° el archivo al cual hacer referencia para renderizar adecuadamente la web.\nSi ya est√°s familiarizado/a con R Markdown, notar√°s que trabajar con Quarto es muy similar. De hecho, los archivos .qmd tienen la misma estructura. Un encabezado (YAML) con algunas cinfiguraciones generales y luego el espacio para escribir texto.\nPara este post, us√© este YAML:\n\n\n\n\n\nEn art√≠culos posteriores veremos c√≥mo configurar el YAML, pues tiene muchas opciones.\nAhora que ya tienes creada una nueva carpeta dentro de posts y agregaste el index.qmd, b√°sicamente ya tienes un nuevo art√≠culo listo para el blog.\nGuarda todos los cambios y dale a Render nuevamente. Si ya ten√≠as abierto el blog en tu navegador, deber√≠an verse reflejados los cambios realizados. Si no pasa eso, actualiza la p√°gina para ver los cambios.\nAs√≠ se ve mi blog, solo con 1 art√≠culo publicado (que es este)."
  },
  {
    "objectID": "posts/creando-mi-primer-blog-con-quarto/index.html#paso-5-subiendo-el-blog-a-github",
    "href": "posts/creando-mi-primer-blog-con-quarto/index.html#paso-5-subiendo-el-blog-a-github",
    "title": "Creando mi primer blog con Quarto",
    "section": "Paso 5: Subiendo el blog a GitHub",
    "text": "Paso 5: Subiendo el blog a GitHub\nMe encanta el open-source y el compartir c√≥digo. Y una de las mejores formas de dejarlo publicado en GitHub.\nPara hacerlo, desde RStudio es bastante simple. Ah! Debes tener una cuenta ya creada en GitHub y tener GIT instalado.\nTeniendo lo anterior ya realizado, podemos usar la librer√≠a {usethis} para simplificar el proceso. Ac√° puedes ver la documentaci√≥n oficial.\nPrimero, instalamos la librer√≠a:\ninstall.packages(\"usethis\")\nConfiguramos nuestras credenciales (los datos que usamos para la cuenta de GitHub):\nusethis::use_git_config(\n   # your name\n   user.name = \"Mi nombre\",\n   # your email used in your GitHub account\n   user.email = \"micorreo@gmail.com\"\n )\nCreamos nuestro GitHub PAT Token con usethis::create_github_token() (es como una contrase√±a, pero m√°s segura). Cuando ejecutas este comando, se te abrir√° una pesta√±a de tu navegador. Tendr√°s que verificar algunos permisos a GitHub. Luego, en la configuraci√≥n del token, cambia el nombre, ajustael tiempo de expiraci√≥n y el resto d√©jalo como est√°. Crea el token.\nSe te mostrar√° una nueva p√°gina con tu claves.\nPara guardar tu nuevo token, usa gitcreds::gitcreds_set(). En la consola se te mostrar√° un men√∫. Selecciona la opci√≥n 2 Replace these credentials. Luego copia el token de GitHub, actualiza y guarda tus credenciales.\nReinicia la sesi√≥n de RStudio para que se hagan efectivos los cambios. Usualmente puedes usar CTRL + SHIFT + F10 o del men√∫ de arriba selecciona la pesta√±a de Session y luego Restart R.\nUsa usethis::git_sitrep() para verificar si tus credenciales como nombre, email y PAT est√°n correctamente configuradas.\nF√≠jate que salga este texto Personal access token for 'https://github.com': '<discovered>'\nCon eso ya deber√≠amos estar listos con la configuraci√≥n de GIT y GitHub.\nAhora configuremos algunas cosas m√°s‚Ä¶\nUsaremos use_git() para iniciar el control de versiones de GIT en nuestro proyecto.\nusethis::use_git()\n‚úî Setting active project to '/Users/Desktop/name-of-your-blog/'\n‚úî Initialising Git repo\n‚úî Adding '.Rproj.user', '.Rhistory', '.Rdata', '.httr-oauth', '.DS_Store' to '.gitignore'\nThere are 8 uncommitted files:\n* '_quarto.yml'\n* '.gitignore'\n* 'about.qmd'\n* 'example-quarto-blog.Rproj'\n* 'index.qmd'\n* 'posts/'\n* 'profile.jpg'\n* 'styles.css'\n\n\nIs it ok to commit them?\n\n1: Not now\n2: Yup\n3: Negative\n\nSelection: 2\n\n\n‚úî Adding files\n‚úî Making a commit with message 'Initial commit'\n‚Ä¢ A restart of RStudio is required to activate the Git pane\nRestart now?\n\n1: No way\n2: Definitely\n3: No\n\nSelection: 2\nAhora, usaremos use_github() para crear un repositorio en GitHub y subir el proyecto.\nusethis::use_github()\nSi todo ha salido bien, se deber√≠a abrir tu navegador con el nuevo repositorio de GitHub.\nAc√° puedes ver el repositorio de este proyecto."
  },
  {
    "objectID": "posts/creando-mi-primer-blog-con-quarto/index.html#paso-6-desplegar-con-netlify",
    "href": "posts/creando-mi-primer-blog-con-quarto/index.html#paso-6-desplegar-con-netlify",
    "title": "Creando mi primer blog con Quarto",
    "section": "Paso 6: Desplegar con Netlify",
    "text": "Paso 6: Desplegar con Netlify\nAhora vamos a publicar nuestro blog en internet para que otros lo puedan ver. Recuerda que hasta ahora, sigue estando solo en tu computador.\nPara hacer el deploy (despliegue) usaremos Netlify.\nEste es un servicio que har√° que el blog est√© disponible en una direcci√≥n web. Adem√°s, entre otras funciones super interesantes, tiene la opci√≥n de conectarlo a un repositorio de GitHub, que es lo que haremos en este caso.\nUsaremos esta opci√≥n, ya que nos ayudar√° a ser m√°s transparentes en todo el flujo de trabajo y podremos desplegar los cambios que vayamos realizando en nuestro proyecto y repo de GitHub.\nPara eso, primero deber crearte una cuenta en Netlify. Mi recomendaci√≥n es que lo hagas usando tu cuenta de GitHub.\nUna vez iniciada sesi√≥n, se te mostrar√° algo as√≠:\n\n\n\n\n\nEn la secci√≥n Sites, dale clic a Import from Git.\nSelecciona GitHub.\n\n\n\n\n\nSe comenzar√° a sincronzar con tu perfil de GitHub y te mostrar√° todos los respositorio disponibles.\nElige el que acabamos de crear, donde est√° nuestro blog.\nSe mostrar√° algunos detalles. Ac√° debes ingresar en Base directory _site, como se muestra en la imagen.\nEste paso es importante, de lo contrario no funcionar√° tu web.\n\n\n\n\n\nDale al bot√≥n Deploy site.\nTe va a salir algo como esto‚Ä¶\n\n\n\n\n\nSi no cambia el estado y sigue saliendo el mensaje Site deply in progress, puedes actualizar la p√°gina de tu navegador. Habitualmente este proceso es r√°pido y solo toma unos segundos.\n\n\n\n\n\nSi todo anda bien, deber√≠a estar ya desplegado nuestro blog.\nPara acceder a √©l, anda al link que te ponen (xxxx.netlify.app).\nEsta es una direcci√≥n web con un nombre aleatorio que te da Netlify. La podemos cambiar en la configuraci√≥n por algo que nos haga m√°s sentido. E incluso, podemos usar nustro propio dominio si es que lo tenemos.\nSe abrir√° una pesta√±a en tu navegador, mostrando tu blog!!! üòç\n\n\n\n\n\nEn Netlify, en la secci√≥n de Site settings podemos encontrar la opci√≥n para cambiar la direcci√≥n web.\nPara ello, debemos dar clic en Change site name.\n\n\n\n\n\nSe te abrir√° una ventana. Ingresa el nombre que desees y listo.\nCambiado el nombre. Nota que este cambio de nombre cambia la direcci√≥n web, por lo que si has compartido el link previo, este ya no funcionar√°. Ten cuidado con cambiar el nombre a cada rato.\nEspero que este tutorial te haya servido y que puedas tener tu propio blog.\nEn pr√≥ximas entradas, estar√© revisando c√≥mo hacer ajustes al blog, personalizar algunas cosas est√©ticas y agregarle algunas funcionaidades.\nNos vemos! üòú\n\n\nüëâ Revisa la segunda parte de este tutorial en este enlace."
  },
  {
    "objectID": "posts/descargar-datos-github/index.html",
    "href": "posts/descargar-datos-github/index.html",
    "title": "¬øC√≥mo descargar datos desde GitHub m√°s r√°pido?",
    "section": "",
    "text": "Hola!!!\nEn esta ocasi√≥n veremos un tema que me preguntan mucho siempre. Habitualmente cuando hago clases o alg√∫n taller de programaci√≥n, sale esta duda‚Ä¶\n¬øC√≥mo descargar los archivos desde GitHub?\nMe refiero a descargar los archivos csv alojados en GitHub para poder usarlos.\nOk.\nRevisemos paso a paso c√≥mo hacerlo. Y para ello, usar√© 2 lenguajes de programaci√≥n que me gustan mucho: Python y R. Adem√°s, que si est√°s interesado/a en la ciencia de datos, los m√°s seguro es que uses alguno de ellos.\nAh! Ac√° no voy a revisar c√≥mo descargar todo el repositorio remoto desde GitHub (clonarlo), sino a solo un archivo que est√© ah√≠ disponible. Tampoco har√© un an√°lisis de los datos. Ya habr√° momento para eso.\nPero haremos algo entretenido, que es medir cu√°nto demoran cada uno!! üòé"
  },
  {
    "objectID": "posts/descargar-datos-github/index.html#accediendo-a-github",
    "href": "posts/descargar-datos-github/index.html#accediendo-a-github",
    "title": "¬øC√≥mo descargar datos desde GitHub m√°s r√°pido?",
    "section": "Accediendo a GitHub",
    "text": "Accediendo a GitHub\nPara este tutorial, usar√© el repositorio del Ministerio de Ciencias y Tecnolog√≠a de Chile, que contiene muchos datos de los casos COVID-19.\nEn particular, usaremos uno de los data products, que es sobre las camas cr√≠ticas disponibles a nivel nacional. Para revisar los datos, debes entrar a esta direcci√≥n.\nPara esta parte, no necesitas tener cuenta creada en GitHub.\nCuando entres a esa secci√≥n, te encontrar√°s con algo similar a √©sto:\n\n\n\n\n\nAdem√°s del README con los detalles de los datos ac√° disponibles, ver√°s 3 archivos .csv (comma separate values), que son b√°sicamente los dataset con los datos. Usaremos el que tiene el sufijo _std, pues tiene un formato m√°s f√°cil de comprender para nuestros efectos.\nPara poder ver esos datos, hacemos clic sobre el enlace NumeroVentiladores_std.csv.\nSe te mostrar√° la siguiente informaci√≥n:\n\n\n\n\n\nSale una previsualizaci√≥n de los datos.\nMe ha tocado ver a muchos que lo que hacen ac√° es copiar y pegar, sin m√°s. Es decir, seleccionan con el rat√≥n los datos y los copian en un Excel o algo as√≠.\nPor favor‚Ä¶ No hagas eso!!!! ü§¢\nJustamente, este art√≠culo es para ello y que no cometas ese error.\nVamos‚Ä¶\nF√≠jate que arriba a la derecha hay unos botones. Uno de ellos dice Raw. Dale clic.\n\n\n\n\n\nSe mostrar√°n los datos de una forma extra√±a, como todos juntos. No te asustes!!! Es normal, pues es el formato de los archivos csv.\n\n\n\n\n\nLo relevante ac√° es la direcci√≥n web que sale en tu navegador. Esa direcci√≥n es la que usaremos para nuestro c√≥digo y descargar los datos, pues desde las pantallas anteriores no podr√°s hacerlo de forma adecuada.\nAhora veamos c√≥mo descargar esos datos a nuestro computador para poder usarlos en los an√°lisis. Como te mencion√©, pondr√© ejemplos tanto de Python como de R. Adem√°s, har√© un breve benchmark para revisar cu√°nto se demoran los scripts en ejecutar las funciones."
  },
  {
    "objectID": "posts/descargar-datos-github/index.html#usando-python",
    "href": "posts/descargar-datos-github/index.html#usando-python",
    "title": "¬øC√≥mo descargar datos desde GitHub m√°s r√°pido?",
    "section": "Usando Python",
    "text": "Usando Python\nPara descargar los datos, usaremos Pandas y su funci√≥n read_csv() a la cual le debemos pasar la direcci√≥n web (url) donde se encuentran los datos que vimos anteriormente en formato raw.\nF√≠jate que la url se la pasamos como un dato de tipo string (cadena de texto), por lo cual la tenemos que envolver con comillas.\nRevisemos el c√≥digo:\n\nimport pandas as pd\n\nurl = 'https://raw.githubusercontent.com/MinCiencia/Datos-COVID19/master/output/producto20/NumeroVentiladores_std.csv'\nbeds_py = pd.read_csv(url)\n\n\nprint(beds_py.head())\n\n  Ventiladores       fecha  numero\n0        total  2020-04-14    1550\n1  disponibles  2020-04-14     564\n2     ocupados  2020-04-14     986\n3        total  2020-04-15    1563\n4  disponibles  2020-04-15     577\n\n\nY eso es todo!!\nYa creamos un objeto (dataframe) con los datos desde GitHub."
  },
  {
    "objectID": "posts/descargar-datos-github/index.html#usando-r",
    "href": "posts/descargar-datos-github/index.html#usando-r",
    "title": "¬øC√≥mo descargar datos desde GitHub m√°s r√°pido?",
    "section": "Usando R",
    "text": "Usando R\nAhora vemos c√≥mo descargar los datos con R. La sintaxis es muy similar.\nUsaremos la librer√≠a reader y su funci√≥n read_csv(), a la cual le pasamos la url de los datos en raw. Igual que antes, se coloca la direcci√≥n como un string (entre comillas)\n\nlibrary(readr)\n\nurl <- \"https://raw.githubusercontent.com/MinCiencia/Datos-COVID19/master/output/producto20/NumeroVentiladores_std.csv\"\nbeds_r <- read_csv(url) \n\n\nhead(beds_r)\n\n# A tibble: 6 √ó 3\n  Ventiladores fecha      numero\n  <chr>        <date>      <dbl>\n1 total        2020-04-14   1550\n2 disponibles  2020-04-14    564\n3 ocupados     2020-04-14    986\n4 total        2020-04-15   1563\n5 disponibles  2020-04-15    577\n6 ocupados     2020-04-15    986\n\n\n\nclass(beds_r)\n\n[1] \"spec_tbl_df\" \"tbl_df\"      \"tbl\"         \"data.frame\" \n\n\nListo!!! Ya tenemos los datos.\nAl igual que antes, tenemos un objeto de tipo dataframe, aunque con unos peque√±os detalles, pues la esta funci√≥n nos devuelve un tibble. Si quieres indagar m√°s sobre las diferencias entre un tibble y un dataframe cl√°sico, puedes revisar este enlace. Pero para nuestros objetivos, da igual.\nComo puedes ver, descargar archivos desde GitHub es bastante simple y no requiere gran esfuerzo ni complicaciones. Pero quiz√°s te est√°s preguntando y para qu√© quiero hacer eso? Si finalmente podr√≠a tener los datos en el computador y bastar√≠a.\nLa gracia de tener esta ‚Äúconexi√≥n‚Äù con el repositorio remoto de GitHub es que en la medida que los datos sean modificados, cada vez que ejecute el c√≥digo, voy a descargar la √∫ltima versi√≥n del archivo. Esto es muy √∫til para temas de automatizaciones y esas cosas. Pero como te estar√°s imaginando, no lo ver√© ahora y ya habr√° momento para revisar ese tema."
  },
  {
    "objectID": "posts/descargar-datos-github/index.html#desempe√±o",
    "href": "posts/descargar-datos-github/index.html#desempe√±o",
    "title": "¬øC√≥mo descargar datos desde GitHub m√°s r√°pido?",
    "section": "Desempe√±o",
    "text": "Desempe√±o\nVimos c√≥mo descargar los datos usando Python y R usando las funciones que m√°s comunmente se suelen usar para ello. Los datos que descargamos son peque√±os y no representan gran trabajo. La cosa se podr√≠a poner m√°s compleja si la cantidad de datos es mayor. Esto es importante a la hora de desplegar un script, por lo cual es interesante evaluar cu√°nto demora cada funci√≥n en realizar esta tarea.\nComo ya estamos usando estos datos, veamos c√≥mo hacerlo en cada lenguaje.\n\nPython\nForma 1: usando time\n\nimport time\n\ntic = time.perf_counter()\nurl = 'https://raw.githubusercontent.com/MinCiencia/Datos-COVID19/master/output/producto20/NumeroVentiladores_std.csv'\nbeds_py_bench1 = pd.read_csv(url)\ntoc = time.perf_counter()\n\nprint(\"Segundos: \", toc - tic) # Resulstado en milisegundos\n\nSegundos:  0.07435119999900053\n\n\nForma 2: usando ttictoc\n\nfrom ttictoc import tic,toc\n\ntic()\nurl = 'https://raw.githubusercontent.com/MinCiencia/Datos-COVID19/master/output/producto20/NumeroVentiladores_std.csv'\nbeds_py_bench2 = pd.read_csv(url)\nelapsed = toc()\n\nprint(\"Segundos: \", elapsed)\n\nSegundos:  0.06629780000002938\n\n\nLos resultados pueden variar entre una ejecuci√≥n y otra y se ven afectado por la calidad de la conexi√≥n a internet, pero vemos que entre ambos ejemplos no hay muchas diferencias en tiempo.\n\n\nR\n\nlibrary(tictoc)\n\ntic()\nurl <- \"https://raw.githubusercontent.com/MinCiencia/Datos-COVID19/master/output/producto20/NumeroVentiladores_std.csv\"\nbeds_r_bench <- read_csv(url) \ntoc()\n\n0.15 sec elapsed\n\n\nAl igual que lo mencionado antes, los resultados pueden variar entre una ejecuci√≥n y otra del c√≥dico. Sin embargo, al parecer esta forma es m√°s lenta que en Python. Bueno, el tidyverse a pesar de que me encanta, no es la cosa m√°s r√°pida en general. Para ello, hay otras librer√≠as con mejor desempe√±o como vroom (que es parte de tidyverse de igual forma) y data.table.\nUsando vroom:\n\ntic()\nurl <- \"https://raw.githubusercontent.com/MinCiencia/Datos-COVID19/master/output/producto20/NumeroVentiladores_std.csv\"\nbeds_r_bench1 <- vroom::vroom(url) \ntoc()\n\n0.16 sec elapsed\n\n\nUsando data.table:\n\nlibrary(data.table)\n\ntic()\nurl <- \"https://raw.githubusercontent.com/MinCiencia/Datos-COVID19/master/output/producto20/NumeroVentiladores_std.csv\"\nbeds_r_bench2 <- data.table::fread(url) \ntoc()\n\n0.07 sec elapsed\n\n\nAmbos tiempos est√°n cercanos, a veces gana data.table otras vroom, pero van cambiando constantemente en la medida que ejecutamos nuevas pruebas.\n¬øPodemos solucionar eso?\nHagamos algo loco‚Ä¶ porque hasta ahora solo han sido unas cuantas ejecuciones de cada c√≥digo y puede no ser muy representativo.\nQu√© tal si corremos el c√≥digo 1000 veces?\nSi, mil.\nPara suerte de nosotros, no tenemos que estar apretando el bot√≥n todas esas veces, sino que podemos usar alguna librer√≠a de bencharking para eso.\n\nlibrary(microbenchmark)\n\nbench_fread <- microbenchmark(\n  url <- \"https://raw.githubusercontent.com/MinCiencia/Datos-COVID19/master/output/producto20/NumeroVentiladores_std.csv\",\n  beds_fread_bench <- data.table::fread(url),\n  times = 1000\n)\n\nbench_fread\n\nUnit: nanoseconds\n                                                                                                                    expr\n url <- \"https://raw.githubusercontent.com/MinCiencia/Datos-COVID19/master/output/producto20/NumeroVentiladores_std.csv\"\n                                                                              beds_fread_bench <- data.table::fread(url)\n      min       lq        mean   median       uq       max neval cld\n        0        1     1361.59     1300     2001     19901  1000  a \n 11468101 15399052 21544789.11 19157751 22793351 280058401  1000   b\n\n\n\nbench_vroom <- microbenchmark(\n  url <- \"https://raw.githubusercontent.com/MinCiencia/Datos-COVID19/master/output/producto20/NumeroVentiladores_std.csv\",\n  beds_vroom_bench <- vroom::vroom(url),\n  times = 1000\n)\n\nbench_vroom\n\nUnit: nanoseconds\n                                                                                                                    expr\n url <- \"https://raw.githubusercontent.com/MinCiencia/Datos-COVID19/master/output/producto20/NumeroVentiladores_std.csv\"\n                                                                                   beds_vroom_bench <- vroom::vroom(url)\n      min        lq         mean      median        uq       max neval cld\n        0       101      1107.31       850.5      1801     11101  1000  a \n 91071301 125492802 153726785.42 141691301.0 163883802 691454301  1000   b\n\n\nComo he mencionado, con cada ejecuci√≥n pueden variar los resultados. El output de este microbench est√° en nanosegundos. Para efectos de comparaci√≥n y mejor entendimiento, usar√© la mediana (ojo con la media en todo caso) y lo pasar√© a segundos. Dejo un resultado que obtuve hace unos minutos, a modo de ejemplo.\nNota: los datos mostrados en las tablas del benchmark pueden diferir.\n\n# data.table\ndatatable_1000 <- 16549550 / 1.0000E+9\nprint(paste(\"Segundos:\", datatable_1000))\n\n[1] \"Segundos: 0.01654955\"\n\n\n\n# vroom\nvroom_1000 <- 89001800 / 1.0000E+9\nprint(paste(\"Segundos:\", vroom_1000))\n\n[1] \"Segundos: 0.0890018\"\n\n\nDiferencia porcentual relativa:\nUsaremos la siguiente f√≥rmula para el c√°lculo.\n\\(| (x2 - x1) | / ((x2 + x1)/2) * 100\\)\n\nabs(vroom_1000 - datatable_1000) / ((datatable_1000 + vroom_1000) / 2) * 100\n\n[1] 137.2834\n\n\nVemos que data.table es un 137% m√°s r√°pido. No deja de ser interesante ese dato."
  },
  {
    "objectID": "posts/descargar-datos-github/index.html#finalmente",
    "href": "posts/descargar-datos-github/index.html#finalmente",
    "title": "¬øC√≥mo descargar datos desde GitHub m√°s r√°pido?",
    "section": "Finalmente",
    "text": "Finalmente\nRevisamos c√≥mo descargar archivos csv desde GitHub y lo hicimos tanto en Python como R.\nPractica tus an√°lisis usando estas funciones. Investiga distintos respositorios que contengan datos y √∫salos.\nAdem√°s, hicimos algunas comparaciones de cu√°ndo demoran y vimos que hay diferencias importantes. Si bien para este ejemplo no es muy significativo, dado que el archivo es peque√±o, puede llegar a ser un gran problema si no te fijas bien. El an√°lisis critico y la decisi√≥n de qu√© funci√≥n usar en cada momento, es algo que requiere pr√°ctica y conocimientos t√©cnicos para optimizar el desempe√±o del c√≥digo, en especial, en tareas de mucha carga. La optimizaci√≥n del c√≥digo permite mejorar el rendimiento, uso de memoria y eficiencia y es un aspecto relevante en soluciones inform√°ticas de mayor escala.\nNos vemos pronto!! üòÅ"
  },
  {
    "objectID": "posts/espera-apalancada-ges/index.html",
    "href": "posts/espera-apalancada-ges/index.html",
    "title": "Mejorando la gesti√≥n de pacientes GES",
    "section": "",
    "text": "Hola! üëã\nHace tiempo que no escrib√≠a un art√≠culo en el blog. Gracias por estar por estos lados nuevamente.\n\nEl contexto\nEn esta oportunidad te quiero hablar sobre el GES (Garant√≠as Explicitas en Salud), que es una parte fundamental de la reforma de salud que se hizo en Chile all√° por los a√±os 2004 y 2005. Esta es una Ley que establece, entre otras cosas, 4 ejes claves (o garant√≠as): garant√≠a de acceso, garant√≠a financiera, garant√≠a de calidad y garant√≠a de oportunidad. En este articulo me voy a referir, principalmente, a la √∫ltima.\nLa garant√≠a de oportunidad establece plazos m√°ximos para que los pacientes reciban determinadas prestaciones de salud como atenciones m√©dicas, realizarse estudios de imagenolog√≠a, ser operados o recibir alg√∫n medicamente, por ejemplo. Estos plazos est√°n delimitados por tipos de patolog√≠as y la etapa asistencial en que se encuentre un paciente. Esto hace que los plazos m√°ximos dependan de cada patolog√≠a y de su etapa (sospecha, estudios, tratamiento, seguimiento o rehabilitaci√≥n).\nActualmente hay 87 grupos de patolog√≠as incluidas en el GES. El listado completo de patolog√≠as lo puedes revisar en la web oficial ministerial.\nDel mismo modo, estos plazos y las prestaciones asociades, determinan un derecho exigible por los pacientes, pudiendo reclamar por ellos cuando no son entregados de forma oportuna y en la manera indicada. Adem√°s, estas patolog√≠as tienen una valoraci√≥n mas alta que otras prestaciones similares, pero que no corresponden al GES. Esto hace que los ingresos financieros de las instituciones de salud derivados de las atenciones de pacientes GES tengan un importante peso en los presupuestos anuales. Es decir, hay un incentivo a la atenci√≥n de estos pacientes, tanto financiero como legal.\nYa. Ok‚Ä¶ Mucha palabrer√≠a jaja üòÖ\nPero era importante situarte en el contexto y explicarte eso antes. En especial si no sabes nada del GES.\nExiste una plataforma llamada SIGGES (mantenida por FONASA), que es una aplicaci√≥n web en donde se realizan los registros de los pacientes ingresados al GES. Adem√°s, esta plataforma sirve para registrar las atenciones y prestaciones entregadas a los pacientes de la cual se desprenden las transferencias financieras de las que hablaba antes y mantiene un sistema de conteo de los plazos y tiempos de espera para cada paciente, en base a su patolog√≠a y etapa. Cada instituci√≥n p√∫blica tiene un grupo de pacientes asignados para resolver, ya sea porque fue atendido en la misma instituci√≥n o fue derivado desde otro centro o atenci√≥n primaria.\nEl tema es que esta plataforma si bien podr√≠a considerarse buena para efectos de registros (estoy siendo generoso, la verdad. Es mala, compleja y requiere 100% de actividad manual para mantenerla al d√≠a), para monitoreo es peor. Ofrece algunas funcionalidades para descargar informaci√≥n de los pacientes en formatos Excel, pero de forma segmentada.\nTe explico‚Ä¶\nLos pacientes GES pueden estar en 2 grandes categor√≠as: abiertos y cerrados.\nLos casos abiertos implican que aun hay cosas que tiene pendientes los pacientes. Los cerrados es que ya cumplieron todas las etapas o bien, se cerr√≥ por alguna causal (como fallecimiento, cambios de seguro de salud, rechazo de las atenciones o indicaci√≥n m√©dica, por ejemplo).\nLos casos cerrados los voy a dejar de lado por ahora. Me voy a preocupar de los casos abiertos. Es decir, de aquellos que tiene pendiente alguna prestaci√≥n o definici√≥n diagnostica o de tratamiento.\nLos casos abiertos pueden estar en alguna de estas 3 categor√≠as: vigentes, retrasados o exceptuados.\nLos casos vigentes son pacientes que se encuentran dentro del plazo legal garantizado (para una determinada patolog√≠a y etapa asistencial).\nLos casos retrasados son pacientes que excedieron el plazo legal.\nY los exceptuados son pacientes a los cuales se les catalog√≥ como tal en alg√∫n momento. Estas excepciones corresponden a situaciones inherentes a los pacientes y que impiden o retrasan la entrega de las prestaciones establecidas en el GES. Por ejemplo, una patolog√≠a podr√≠a tener un plazo de 30 d√≠as para que un paciente sea citado a evaluaci√≥n medica de especialidad. Pero el paciente refiere que esta de viaje y que no puede asistir en la fecha ofrecida por la instituci√≥n, sino hasta varias semanas mas tarde. Esta es una situaci√≥n ajena a la organizaci√≥n e inherente al paciente, por cuanto se podr√≠a exceptuar y postergar la plazo GES. Estas excepciones est√°n normadas y descritas en la documentaci√≥n GES.\nOk.\nHasta ahora, todo muy interesante, pero nada muy problem√°tico.\nHasta ahora‚Ä¶\n\n\nEl problema\nEl problema comienza porque, por defecto, no hay un solo listado de pacientes GES. Est√°n segmentados por patolog√≠as (las 87), lo cual parece razonable. Pero tambi√©n est√°n segmentados, a su vez, entre casos abiertos y cerrados, entre vigentes, retrasados y exceptuados.\nTodos estos listados hacen que no podamos tener un solo listado √∫nico.\nY a estas alturas te estar√°s preguntando‚Ä¶ ¬øY por qu√© tanta relevancia tener un listado √∫nico?\nEl disponer de un solo listado permite tener a todos los pacientes en solo 1 repositorio de datos, lo que facilita la b√∫squeda. Adem√°s, y quiz√°s lo mas relevante, es que se pueden ordenar.\nY este orden es important√≠simo! Pues permite tener una visi√≥n general del estado de los pacientes y ayudar a priorizar casos en virtud de su orden de prelaci√≥n.\n¬øPero c√≥mo construir ese orden?\nOrdenar a los pacientes no es una tarea banal. Por el contrario, es algo muy complejo. En este art√≠culo no pretendo ser la referencia m√°xima ni ser el modelo definitivo. Existen, por lo dem√°s, diversas metodolog√≠as para ordena a pacientes y priorizarlos, como los modelos DELPHI u otros sistemas basados en la inclusi√≥n de factores de riesgo, factores sociales o de riesgo de mortalidad. Incluso modelos en donde se pueden usar algoritmos de inteligencia artificial para clasificaci√≥n no supervisada.\nEn esta ocasi√≥n quiero ser m√°s pragm√°tico, menos filos√≥fico y m√°s concreto. Y usar lo que hay, sin ponerme muy creativo ni impulsar grandes cambios culturales ni tecnol√≥gicos. No.¬†Solo deseo ser simple y extrem√°damente pr√°ctico.\nPor ello, usar√© algunos datos descargables del SIGGES y un Excel. Solo eso, de modo que sea muy simple el que lo uses en tu instituci√≥n sin grandes complicaciones y desde el d√≠a cero.\nLo que busco con ese orden es responder la pregunta hipot√©tica: Si tuviera un cupo para ma√±ana, ¬øA qui√©n cito?\n\n\nLa soluci√≥n\nPara responder esa pregunta, es necesario tener a los pacientes ordenados, desde el con mayor prioridad hasta al con menos prioridad. De modo, que si tengo un cupo, cito al primero de ese listado. Si tengo 2 cupos, cito a los 2 primeros y as√≠ sucesivamente. Tambi√©n es cierto que quiz√°s el pcte en el lugar 1 no pueda asistir o no tenga las condiciones (o estudios) para hacerlo, entonces paso al segundo. Si este tampoco tiene las condiciones, paso al tercero. El tema es dise√±ar ese orden de prelaci√≥n ex-ante y mantenerlo en vista al momento de citarlos o de programar para procedimientos o cirug√≠as.\nDel mismo modo, este listado ordenado ayuda a brindar transparencia al proceso y dar explicabilidad de porque citar a un paciente primero que otro. Estos elementos (transparencia y explicabilidad) son fundamentales en el manejo √©tico y responsable de datos.\nOtro punto relevante, es evitar sesgos.\nAc√° los sesgos est√°n m√°s enfocados a privilegiar a determinados grupos de pacientes, como suele hacerse en la gesti√≥n habitual de garant√≠as GES. Es habitual que haya mayor inter√©s en resolver y atender a pacientes retrasados que vigentes. Esto ocurre porque existen una serie de indicadores de evaluaci√≥n de desempe√±o institucional que miden preferentemente la disminuci√≥n de garant√≠as retrasadas. Por otro lado, los pacientes exceptuados dejar de ser visibles a los sistemas de monitoreo, lo que disminuyen sus posibilidades de atenci√≥n y resoluci√≥n.\nEl generar un listado √∫nico, con todos los pacientes vigentes, retrasados y exceptuados ayuda a dar visibilidad a todos los pacientes pendientes y el orden de √©stos, a brindar posibilidades de atenci√≥n en base a sus tiempos de espera y plazos m√°ximos. Es, te√≥ricamente, un modelo m√°s justo.\nPues bien‚Ä¶ ya sabemos que necesitamos ese listado ordenado, no solo para ayudar a la decisi√≥n de a quienes priorizar, sino que ser mas transparentes y evitar sesgos de selecci√≥n.\n¬øY c√≥mo ordenarlos?\nBuena pregunta!!!\n¬øComo generar ese orden?\nComo te mencion√© antes, no quiero ponerme filos√≥fico ac√°, ser√© intensamente pragm√°tico. Los pacientes GES que cuentan con garant√≠a de oportunidad tienen definidos por Ley los plazos m√°ximos de atencion o de entrega de determinadas prestaciones. Eso es algo que no podemos obviar. Es Ley.\nSin embargo, no es lo √∫nico debemos considerar.\nEste orden deber√≠a ayudar a resolver las siguientes situaciones:\n\nResolver los pacientes excedidos en sus plazos.\nEvitar que los pacientes pasen del plazo garantizado.\nNo es lo mismo estar excedido en 1 d√≠a que en 100 d√≠as.\nNo es lo mismo estar a 5 d√≠as de que se cumpla el plazo m√°ximo, que estar a 90.\nLa mezcla de pacientes debe incluir a los 3 tipos de categor√≠as.\nEvitar solo resolver retrasados.\n\nPara lo anterior, una forma de abordarlo es definir un tiempo de espera espec√≠fico para cada paciente ajustado por un factor de apalancamiento.\nEste factor de apalancamiento lo que hace es dar mayor peso a determinados grupos de pacientes. Por ejemplo, dar mayor prioridad a personas m√°s excedidas en su plazo que los que tienen menos tiempo de exceso. Del mismo modo, un d√≠a de exceso de pacientes con mucho retraso es proporcionalmente mayor que 1 d√≠a de alguien con poco retraso. Este factor tambi√©n se aplica a pacientes vigentes, de modo de generar mayor relevancia a pacientes pr√≥ximos a vencer que casos reci√©n derivados.\nEste modelo de apalancamiento no es idea original m√≠a, sino que fue descrita en una tesis de magister de la Universidad de Chile, en donde aplicaron este concepto en la gesti√≥n de pabellones en un hospital p√∫blico pedi√°trico. No tengo claro si realmente lograron implementarlo y dejarlo en producci√≥n o solo fue un piloto. Pero este concepto me gust√≥. Le hice algunos ajustes para aplicarlo a GES, pero es una idea similar.\nEl modelo implica establecer una espera apalancada E(t).\nEn el caso de que el tiempo de espera sea menor a un tiempo m√°ximo definido (T max.), el tiempo de espera es propio tiempo de espera contado en d√≠as. En el caso de que el tiempo de espera sea mayor al tiempo m√°ximo, se debe aplicar el factor de apalancamiento (Œ±).\nSi lo vemos como f√≥rmula, tenemos:\n\nPara el caso de las patolog√≠as GES, ya tenemos un plazo m√°ximo, que es el legal. Pero eso es insuficiente. Es necesario dise√±ar una tabla con plazos intermedios y de mayor rango para abordar las situaciones antes mencionadas.\nPara los efectos de este art√≠culo y a modo de prueba, tom√© la patolog√≠a de colecistectom√≠a preventiva. Esto es por varios motivos. Esta patolog√≠a tiende a ser una de las con mayor n√∫mero de pacientes derivados a los hospitales (sin contar las de oftalmolog√≠a y ayudas t√©cnicas) y para el caso de pabell√≥n, es de las m√°s comunes. Adem√°s, solo tiene 1 garant√≠a que es la de intervenci√≥n quir√∫rgica. Eso facilita el ejemplo.\nEl plazo legal GES es de 90 d√≠as (desde la derivaci√≥n hasta la intervenci√≥n quir√∫rgica).\nHe construido la siguiente tabla con tiempos m√°ximos en distintos cortes:\n\nT max corresponden a determinados cortes en el tiempo transcurrido desde la derivaci√≥n. Por tanto, los cortes de 20, 50 y 80 d√≠as son de pacientes que est√°n dentro del plazo legal (recuerda que es de 90) y los dem√°s aplican para pacientes que excedieron el plazo m√°ximo. Todos estos cortes ayudan a diferenciarlos en el listado √∫nico y generar un factor de apalancamiento para cada segmento. Ya te explico lo de la prioridad.\nEl factor Œ± es calculado. Se realiza dividiendo el T max por el valor m√≠nimo de T max en la tabla.\nAs√≠, para la prioridad 2, el factor de apalancamiento se obtiene de 160/20 = 8. Para la prioridad 5, se obtiene de 50/20 = 1.3\nAplicando esa formula a cada campo, se puede construir la tabla sin problemas.\nEste factor de apalancamiento lo que hace es dar m√°s peso a los d√≠as que pasan un determinado corte. Por ejemplo, un d√≠a de un paciente con 15 d√≠as de tiempo transcurrido equivale a 12.5 d√≠as de un paciente con m√°s de 250 d√≠as.\nEsta tabla esta pensada para la patolog√≠a GES de colecistectom√≠a. Para otras patolog√≠as es necesario ajustarla a los plazos legales y generar los cortes (T max), seg√∫n sea necesario. Esta construcci√≥n es en base a la experiencia en la gesti√≥n y a necesidades de cada organizaci√≥n. Es recomendable que sea ajustada para cada caso. Y de preferencia, hacer una tabla para cada garant√≠a, pues los plazos pueden varias entre ellos. De todas formas, no recomiendo aplicarlo a todas las patolog√≠as y a todas las garant√≠as en un principio, sino que focalizarlas en donde tenga mayor beneficio. Ahora, si cuentas con la capacidad de hacerlo y dispones de alguna plataforma que te facilite estas labores, podr√≠as hacerlo.\nEn todo caso, te dejar√© disponible el Excel para lo puedas revisar con calma, mirar las f√≥rmulas y usarlo de ejemplo.\nAhora veamos c√≥mo generar el algoritmo y aplicarlo a los pacientes.\n\n\nEl algoritmo\nEn el Excel, adem√°s, de los datos propios de los pacientes, te encontrar√°s con varios campos calculados.\nTe los explico brevemente.\n\ninicio_ajustado: este campo corresponde a la fecha de inicio del plazo GES. Para los pacientes exceptuados, esta fecha se ajusta a la fecha en que fue exceptuado.\nt_transcurrido: este es el tiempo (d√≠as) transcurrido entre la fecha de inicio y la fecha actual (en la cual estes analizando los datos).\nt_exceso: son los d√≠as de exceso por sobre el plazo legal definido. En el caso de ejemplo, es 90. Debes ajustarlo a la patolog√≠a que estes usando.\nprioridad_tiempo: este campo se completa en base a la tabla de T max antes mencionada.\nprioridad_clinica: este es un campo opcional, en donde se puede clasificar a los pacientes, seg√∫n la prioridad cl√≠nica que tenga en un determinado momento. Si no se especifica nada, se asume y calculan los plazos y el tiempo apalancado en base a los T max. Si escribes ‚ÄúP‚Äù (sin las comillas) se aplica una prioridad alta. Si escribes ‚ÄúPP‚Äù (sin comillas) se aplica una prioridad muy alta. El uso de estas prioridades afecta el lugar en donde se encuentra el paciente y aumenta el factor de apalancamiento en 1 o 2 pasos, seg√∫n corresponda. El motivo de que la tabla de T max tenga prioridades 0 y -1 es por este motivo, ya que, si un paciente se encuentra en prioridad 1 por su tiempo de espera, al clasificarlo con prioridad cl√≠nica alta pasa a prioridad 0 (se resta 1) y con prioridad cl√≠nica muy alta pasa a -1 (se resta 2).\nprioridad_final: corresponde a la prioridad final del paciente, considerando el tiempo de espera y la clasificaci√≥n cl√≠nica, si tuviera.\nt_max: es el plazo m√°ximo para cada corte. Se se√±ala la categor√≠a en que queda el paciente en base a su prioridad final.\nfactor: corresponde al factor de apalancamiento (Œ±).\nE(t): corresponde al tiempo de espera apalancado. Este concepto ya lo expliqu√© previamente.\nLugar: este es un campo calculado que muestra el lugar que ocupa un determinado paciente en el listado unificado. Este es el orden de prelaci√≥n que se aplica a los pacientes, en base al E(t).\n\nUn detalle sobre la clasificaci√≥n de prioridad cl√≠nica. Esto es un dato que lo puedes poner con el fin de aumentar la prioridad final de un determinado paciente y, por tanto, dejarlo mas arriba en el listado de prelaci√≥n. Esta clasificaci√≥n es eminentemente cl√≠nica y va a depender de tu nivel de conocimientos sobre el estado de cada paciente. Por ejemplo, como estamos viendo la colecistectom√≠a, un paciente podr√≠a estar reci√©n ingresando al listado, pues la derivaron hace poco desde atenci√≥n primaria. Esto hace que quede en los √∫ltimos puestos del listado. Pero descubres que esta en regular estado, muy sintom√°tica, con alteraci√≥n de sus pruebas hep√°ticas y con varias consultas al servicio de urgencia. Entonces, podr√≠as aumentar su prioridad final agregando una prioridad cl√≠nica muy alta. Esto hace que quede mas arriba en el listado. Y no solo eso, sino que cada d√≠a que pase, tendr√° m√°s relevancia, pues el factor de apalancamiento tambi√©n aumenta.\nYo puse 3 clasificaciones cl√≠nicas: normal, alta y muy alta. Quiz√°s tu podr√≠as poner m√°s. Para eso, debes ajustar la formula del Excel, l√≥gicamente. Pero me parece que, con esas 3 clasificaciones, se cubre la gran mayor√≠a de los casos.\nPara los campos de los datos de los pacientes, tienes que usar los listados del SIGGES de garant√≠as vigentes, garant√≠as vencidas y garant√≠as exceptuadas.\nPara el caso de los pacientes exceptuados, solo debes considerar los con causas transitorias. Esto es, exceptuados por inasistencia y por postergaci√≥n. Estos son lo que deber√≠an estar pendientes y son los que indica la norma que deben quedar en monitoreo y continuar sus atenciones. Ten en cuenta que el SIGGES tiene una opci√≥n para descargar las excepciones transitorias, pero mira bien el listado, pues la plataforma tiene un error (entre varios m√°s) en donde no reconoce bien que a los pacientes exceptuados ya se les digit√≥ la prestaci√≥n o el documento que cumple la garant√≠a y lo sigue mostrando como pendiente cuando no es tal.\nDe todas formas, el construir este listado ordenado te va a ayudar a reconocer esos casos para actualizar la plataforma SIGGES o dejar un listado anexo m√°s limpio.\nDel mismo modo, el listado te puede ayudar a actualizar otros pacientes que ya est√©n resueltos, pero que en el SIGGES no tengan todo digitado.\n‚úÖ Entonces, puedes descargar la plantilla Excel que hice para que la mires y lo apliques con tus pacientes.\n\n\n\n\n\nEl hecho de hacerlo en Excel es para facilitar el uso y la cantidad de campos calculados es para mostrar paso a paso como se construye el algoritmo de ordenamiento. Se podr√≠a hace en muchos menos campos o se puede programar una app que te haga el ajuste de forma autom√°tica, pero decid√≠ dejarlo as√≠ para mejor accesibilidad.\nYa ahora depende de ti el usarlo y mejorarlo.\n\n\nAlgo m√°s‚Ä¶\nSi bien te he hablado de usarlo para generar un orden de los pacientes para una mejor gesti√≥n. Este listado te puede servir para auditor√≠a interna.\nComo tienes un listado ordenado, lo l√≥gico es ir resolvendo los casos en ese orden. Pero no siempre es posible hacerlo. Ya te mencion√© algunos ejemplos. Pero ojo, si no se sigue el orden es por alg√∫n motivo de peso. La respuesta de ‚Äúes que era el que ten√≠a a mano‚Äù o ‚Äúes que me llam√≥ reci√©n y le di la hora‚Äù o ‚Äúes que viene todos los d√≠as a preguntar‚Äù no tienen cabida en un modelo √©tico de gesti√≥n. Si no se sigue el orden, un motivo debe haber y uno de peso. Este listado te permite indagar en posibles fraudes, sesgos, faltas a la probidad o, simplemente, despelote.\nSi tienes comentarios, encuentras alg√∫n bug o se te ocurren nuevas formas de aplicarlo, los puedes dejar m√°s abajo.\nSaludos!!"
  },
  {
    "objectID": "posts/factores-en-r/index.html",
    "href": "posts/factores-en-r/index.html",
    "title": "Factores en R",
    "section": "",
    "text": "Hola!!\nHoy toca ver c√≥digo. Yeahhh!!! üòÅ\nEn mi experiencia, uno de los aspectos m√°s relvantes a la hora de trabajar con datos, es saber manejar los datos categ√≥ricos o factores.\nEn la vida nos topamos a cada instante con este tipo de datos. Cuando vamos al supermercado, compramos frutas, l√°cteos, verduras y carnes. O cuando revisamos planillas de personas, vemos distintos tipos de profesiones. Si revisamos datos por pa√≠ses, finalmente √©stos son categor√≠as. Las categor√≠as est√°n por todos lados.\nPor otro lado, el conocimiento de factores es un plus muy potente para analizar datos y realizar la exploraci√≥n de los mismos. Tambi√©n, para inteligencia artificial, el manejo de datos categ√≥ricos es muy relevante. Muchas veces el transformar datos cont√≠nuos en factores ayuda a mejorar el desempe√±o de los modelos predictivos y es parte del feature engineering, uno de los pasos b√°sicos previos antes del desarrollo de cualquier modelo de inteligencia artificial."
  },
  {
    "objectID": "posts/factores-en-r/index.html#carga-de-datos",
    "href": "posts/factores-en-r/index.html#carga-de-datos",
    "title": "Factores en R",
    "section": "Carga de datos",
    "text": "Carga de datos\nOk. Ya tenemos claro que, al analizar datos, debemos conocer m√©todos para trabajar con categor√≠as o factores.\nPara ello, R y su framework tidyverse nos ofrece forcats. Una librer√≠a especializada en el manejo de factores. Te recomiendo que revises la documentaci√≥n oficial del la librer√≠a.\nAhora revisaremos muchas de sus funciones y que te ser√°n de gran utilidad en el d√≠a a d√≠a.\nPara los ejemplos usar√© estos datos disponibles en Kaggle.\nPrimero cargamos las librer√≠as que usaremos:\n\nlibrary(tidyverse)\nlibrary(here)\nlibrary(janitor)\n\nVeamos un poco los datos‚Ä¶\n\nhere::i_am(\"index.qmd\")\nsuicides <- read_csv2(\"master.csv\") |> \n  clean_names()\n\n\nglimpse(suicides)\n\nRows: 27,820\nColumns: 12\n$ country           <chr> \"Albania\", \"Albania\", \"Albania\", \"Albania\", \"Albania‚Ä¶\n$ year              <dbl> 1987, 1987, 1987, 1987, 1987, 1987, 1987, 1987, 1987‚Ä¶\n$ sex               <chr> \"male\", \"male\", \"female\", \"male\", \"male\", \"female\", ‚Ä¶\n$ age               <chr> \"15-24 years\", \"35-54 years\", \"15-24 years\", \"75+ ye‚Ä¶\n$ suicides_no       <dbl> 21, 16, 14, 1, 9, 1, 6, 4, 1, 0, 0, 0, 2, 17, 1, 14,‚Ä¶\n$ population        <dbl> 312900, 308000, 289700, 21800, 274300, 35600, 278800‚Ä¶\n$ suicides_100k_pop <chr> \"6.71\", \"5.19\", \"4.83\", \"4.59\", \"3.28\", \"2.81\", \"2.1‚Ä¶\n$ country_year      <chr> \"Albania1987\", \"Albania1987\", \"Albania1987\", \"Albani‚Ä¶\n$ hdi_for_year      <chr> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, ‚Ä¶\n$ gdp_for_year      <chr> \"2,156,624,900\", \"2,156,624,900\", \"2,156,624,900\", \"‚Ä¶\n$ gdp_per_capita    <dbl> 796, 796, 796, 796, 796, 796, 796, 796, 796, 796, 79‚Ä¶\n$ generation        <chr> \"Generation X\", \"Silent\", \"Generation X\", \"G.I. Gene‚Ä¶\n\n\n\nhead(suicides)\n\n# A tibble: 6 √ó 12\n  country  year sex    age       suici‚Ä¶¬π popul‚Ä¶¬≤ suici‚Ä¶¬≥ count‚Ä¶‚Å¥ hdi_f‚Ä¶‚Åµ gdp_f‚Ä¶‚Å∂\n  <chr>   <dbl> <chr>  <chr>       <dbl>   <dbl> <chr>   <chr>   <chr>   <chr>  \n1 Albania  1987 male   15-24 ye‚Ä¶      21  312900 6.71    Albani‚Ä¶ <NA>    2,156,‚Ä¶\n2 Albania  1987 male   35-54 ye‚Ä¶      16  308000 5.19    Albani‚Ä¶ <NA>    2,156,‚Ä¶\n3 Albania  1987 female 15-24 ye‚Ä¶      14  289700 4.83    Albani‚Ä¶ <NA>    2,156,‚Ä¶\n4 Albania  1987 male   75+ years       1   21800 4.59    Albani‚Ä¶ <NA>    2,156,‚Ä¶\n5 Albania  1987 male   25-34 ye‚Ä¶       9  274300 3.28    Albani‚Ä¶ <NA>    2,156,‚Ä¶\n6 Albania  1987 female 75+ years       1   35600 2.81    Albani‚Ä¶ <NA>    2,156,‚Ä¶\n# ‚Ä¶ with 2 more variables: gdp_per_capita <dbl>, generation <chr>, and\n#   abbreviated variable names ¬π‚Äãsuicides_no, ¬≤‚Äãpopulation, ¬≥‚Äãsuicides_100k_pop,\n#   ‚Å¥‚Äãcountry_year, ‚Åµ‚Äãhdi_for_year, ‚Å∂‚Äãgdp_for_year"
  },
  {
    "objectID": "posts/factores-en-r/index.html#as_factor",
    "href": "posts/factores-en-r/index.html#as_factor",
    "title": "Factores en R",
    "section": "as_factor",
    "text": "as_factor\nEn este art√≠culo no realizaremos an√°lisis estad√≠sticos pensando en modelos de inteligencia artificial, que es lo cl√°sico que se realiza en Kaggle. Sino que veremos el uso de la librer√≠a forcats.\nRevisemos la variable age del dataset.\nEsta variable est√° definida como string. Es decir, como una cadena de caracteres.\n\nstr(suicides$age)\n\n chr [1:27820] \"15-24 years\" \"35-54 years\" \"15-24 years\" \"75+ years\" ...\n\n\nSi queremos realizar an√°lisis de este dataset, el tener esta variable como string no es buena idea. Parece m√°s razonable transformarlo a un factor. La librer√≠a forcats contiene la funci√≥n as_factor() que nos permite hacer eso.\n\nsuicides <- suicides |> \n  mutate(age = as_factor(age))\n\nVeamos nuevamente la estructura de la variable.\n\nstr(suicides$age)\n\n Factor w/ 6 levels \"15-24 years\",..: 1 2 1 3 4 3 2 4 5 6 ...\n\n\nOk. Ahora la variable ya no es de tipo string, sino que la hemos cambiado a tipo factor con 6 niveles. Es decir, tiene 6 categor√≠as. Ve√°moslas‚Ä¶\n\nlevels(suicides$age)\n\n[1] \"15-24 years\" \"35-54 years\" \"75+ years\"   \"25-34 years\" \"55-74 years\"\n[6] \"5-14 years\""
  },
  {
    "objectID": "posts/factores-en-r/index.html#fct_relevel",
    "href": "posts/factores-en-r/index.html#fct_relevel",
    "title": "Factores en R",
    "section": "fct_relevel",
    "text": "fct_relevel\nSi te fijas bien, el orden de las categor√≠as no est√° bien. Me refiero a que no sigue un orden ascendente o descendente, sino que est√° desordenado. Pues bien, podemos reordenarlos usando la funci√≥n fct_relevel().\n\nsuicides <- suicides |>\n  mutate(age = fct_relevel(age,\n                           \"5-14 years\",\n                           \"15-24 years\",\n                           \"25-34 years\",\n                           \"35-54 years\",\n                           \"55-74 years\",\n                           \"75+ years\"\n  ))\n\nlevels(suicides$age)\n\n[1] \"5-14 years\"  \"15-24 years\" \"25-34 years\" \"35-54 years\" \"55-74 years\"\n[6] \"75+ years\"  \n\n\nPara efectos demostrativos, usar√© el argumento after de fct_relevel() para reordenar una categor√≠a.\nAc√° har√© que ‚Äú5-14 years‚Äù quede despu√©s del √≠ndice 1 (recuerda que en R, el √≠ndice parte en 1).\n\nsuicides <- suicides |> \n  mutate(age = fct_relevel(age, \"5-14 years\", after = 1))\n\nlevels(suicides$age)\n\n[1] \"15-24 years\" \"5-14 years\"  \"25-34 years\" \"35-54 years\" \"55-74 years\"\n[6] \"75+ years\"  \n\n\nVolver√© a ordenarlo de forma correcta‚Ä¶\n\nsuicides <- suicides |> \n  mutate(age = fct_relevel(age, \"5-14 years\", after = 0))\n\nlevels(suicides$age)\n\n[1] \"5-14 years\"  \"15-24 years\" \"25-34 years\" \"35-54 years\" \"55-74 years\"\n[6] \"75+ years\""
  },
  {
    "objectID": "posts/factores-en-r/index.html#fct_reorder",
    "href": "posts/factores-en-r/index.html#fct_reorder",
    "title": "Factores en R",
    "section": "fct_reorder",
    "text": "fct_reorder\nEsta funci√≥n es muy √∫til para los gr√°ficos, ya que permite reordenar los factores de modo de ajustarlos a nuestros requerimientos y necesidades de la visualizaci√≥n.\nRealicemos un gr√°fico para Chile:\n\nsuicides |>\n  filter(\n    year == 2015,\n    country == \"Chile\"\n  ) |>\n  group_by(age) |>\n  summarise(suicides_total = sum(suicides_no)) |>\n  mutate(prop = suicides_total / sum(suicides_total)) |>\n  ggplot(aes(age, suicides_total,\n    fill = suicides_total\n  )) +\n  geom_col(show.legend = FALSE) +\n  labs(\n    title = \"Suicides in Chile\", subtitle = \"Year 2015\", y = \"Total Number of Suicides\",\n    x = \"Age\", fill = \"Number of Suicides\"\n  ) +\n  coord_flip() +\n  theme_minimal()\n\n\n\n\nEste gr√°fico est√° adecuado. Es informativo para saber cu√°l es el grupo etario que presenta m√°s suicidios. F√≠jate que el eje y est√° ordenado por las categor√≠as de age. Pero quiz√°s sea m√°s interesante ordenar los datos por el total de suicidios por grupo.\nVeamos c√≥mo hacerlo usando fct_reorder.\n\nsuicides |>\n  filter(\n    year == 2015,\n    country == \"Chile\"\n  ) |>\n  group_by(age) |>\n  summarise(suicides_total = sum(suicides_no)) |>\n  mutate(prop = suicides_total / sum(suicides_total)) |>\n  ggplot(aes(fct_reorder(age, suicides_total), suicides_total,\n    fill = suicides_total\n  )) +\n  geom_col(show.legend = FALSE) +\n  labs(title = \"Suicides in Chile\", subtitle = \"Year 2015\", y = \"Total Number of Suicides\", x = \"Age\", fill = \"Number of Suicides\") +\n  coord_flip() +\n  theme_minimal()\n\n\n\n\nEste orden deja m√°s claro la cantidad de suicidios por grupo etario y la proporci√≥n entre cada uno, a diferencia del gr√°fico anterior. F√≠jate en d√≥nde us√© fct_reorder. Podemos leerlo que deseamos que el eje x (primer par√°metro de aes) que corresponde a age, sea ordenado por suicides_total. Esto por defecto se ordena de mayor a menor (orden ascendente). Si deseamos hacerlo en orden descendente, debemos agregar el argumento .desc = TRUE al interior de fct_reoder.\n\nsuicides |>\n  filter(\n    year == 2015,\n    country == \"Chile\"\n  ) |>\n  group_by(age) |>\n  summarise(suicides_total = sum(suicides_no)) |>\n  mutate(prop = suicides_total / sum(suicides_total)) |>\n  ggplot(aes(fct_reorder(age, suicides_total, .desc = TRUE), suicides_total,\n    fill = suicides_total\n  )) +\n  geom_col(show.legend = FALSE) +\n  labs(title = \"Suicides in Chile\", subtitle = \"Year 2015\", y = \"Total Number of Suicides\", x = \"Age\", fill = \"Number of Suicides\") +\n  coord_flip() +\n  theme_minimal()"
  },
  {
    "objectID": "posts/factores-en-r/index.html#fct_explicit_na",
    "href": "posts/factores-en-r/index.html#fct_explicit_na",
    "title": "Factores en R",
    "section": "fct_explicit_na",
    "text": "fct_explicit_na\nPara explicar esta funcion, vamos a usar la variable hdi_for_year (Human Development Index) que contiene datos NA.\nCreamos algunas categor√≠as:\n\nsuicides <- suicides |>\n  mutate(\n    hdi_cat = case_when(\n      hdi_for_year >= 0.80 ~ \"Very High Development\",\n      hdi_for_year >= 0.70 ~ \"High Development\",\n      hdi_for_year >= 0.55 ~ \"Medium Development\",\n      hdi_for_year >= 0.35 ~ \"Low Development\",\n      hdi_for_year < 0.35 ~ \"Very Low Development\"\n    ),\n    hdi_cat = as_factor(hdi_cat)\n  )\n\nYa sabemos que la variable hdi_for_year tiene datos NA, por lo que la nueva variable que acabamos de crear tambi√©n deber√≠a tener datos NA¬¥s.\n\nsum(is.na(suicides$hdi_cat))\n\n[1] 19456\n\n\nPara trabajar con los datos, tenerlos como NA podr√≠a complicar los an√°lisis y algunas funciones aritm√©ticas no funcionar√≠an o ser√≠a m√°s dificil de interpretar.\n\nsuicides |>\n  filter(country == \"Chile\") |>\n  group_by(country, hdi_cat) |>\n  summarise(n = n())\n\n# A tibble: 4 √ó 3\n# Groups:   country [1]\n  country hdi_cat                   n\n  <chr>   <fct>                 <int>\n1 Chile   Medium Development       24\n2 Chile   High Development         36\n3 Chile   Very High Development    60\n4 Chile   <NA>                    252\n\n\nUna opci√≥n es usar fct_explicit_na para identificar los NA¬¥s y asignarle un valor a esa categor√≠a. Esto hace m√°s legible las tablas y los an√°lisis.\n\nsuicides <- suicides |>\n  mutate(hdi_cat = fct_explicit_na(hdi_cat, na_level = \"Missing\"))\n\nsuicides |>\n  filter(country == \"Chile\") |>\n  group_by(country, hdi_cat) |>\n  summarise(n = n())\n\n# A tibble: 4 √ó 3\n# Groups:   country [1]\n  country hdi_cat                   n\n  <chr>   <fct>                 <int>\n1 Chile   Medium Development       24\n2 Chile   High Development         36\n3 Chile   Very High Development    60\n4 Chile   Missing                 252"
  },
  {
    "objectID": "posts/factores-en-r/index.html#section",
    "href": "posts/factores-en-r/index.html#section",
    "title": "Factores en R",
    "section": "",
    "text": "fct_lump\nPodemos agrupar categor√≠as, seg√∫n necesidad. Retomemos la variable que creamos hdi_cat.\nOmitimos los datos NA¬¥s, mantenemos las 2 categor√≠as con mayor cantidad de datos y el resto las agrupamos en una nueva categor√≠a ‚ÄúAverage/Low Development‚Äù usando el argumento other_level.\n\nsuicides |>\n  na.omit() |>\n  mutate(hdi_lumped = fct_lump(hdi_cat, n = 2, other_level = \"Average/Low Development\")) |>\n  count(hdi_lumped) |>\n  mutate(prop = n / sum(n)) |>\n  arrange(desc(n))\n\n# A tibble: 3 √ó 3\n  hdi_lumped                  n  prop\n  <fct>                   <int> <dbl>\n1 Very High Development    3600 0.430\n2 High Development         2952 0.353\n3 Average/Low Development  1812 0.217\n\n\nAcabamos de mantener las 2 categor√≠as con m√°s datos ( n = 2), pero tamb√©n podemos usar una proporci√≥n para hacer esa segmentaci√≥n. Para ello, usamos el argumento prop.\nVeamos un ejemplo‚Ä¶\n\nsuicides |>\n  na.omit() |>\n  mutate(hdi_relevel = fct_lump(hdi_cat, prop = 0.2, other_level = \"Below Average\")) |>\n  count(hdi_relevel) |>\n  mutate(prop = round(n / sum(n), 3))\n\n# A tibble: 4 √ó 3\n  hdi_relevel               n  prop\n  <fct>                 <int> <dbl>\n1 Medium Development     1752 0.209\n2 High Development       2952 0.353\n3 Very High Development  3600 0.43 \n4 Below Average            60 0.007\n\n\nUsamos prop = 0.20 para indicar que cualquier categor√≠a con 20% o menos se indica como ‚ÄúBelow Average‚Äù."
  },
  {
    "objectID": "posts/factores-en-r/index.html#fct_infreq",
    "href": "posts/factores-en-r/index.html#fct_infreq",
    "title": "Factores en R",
    "section": "fct_infreq",
    "text": "fct_infreq\nEsta funci√≥n se usa junto a la librer√≠a ggplot2. Por ejemplo, para una gr√°fica de conteo de datos, esta funci√≥n permite ordenar por frecuencia.\nPrimero veamos sin usarla y luego us√°ndola.\n\nsuicides |> \n  na.omit() |> \n  add_count(hdi_cat) |> \n  ggplot(aes(hdi_cat)) +\n  geom_bar(stat = \"count\") +\n  labs(x = \"HDI Level\", y = \"Count\") +\n  theme_minimal() \n\n\n\n\n\nlevels(suicides$hdi_cat)\n\n[1] \"Medium Development\"    \"High Development\"      \"Very High Development\"\n[4] \"Low Development\"       \"Missing\"              \n\n\nComo te dar√°s cuenta, el gr√°fico anterior est√° ordenado por los niveles de las categor√≠as de hdi_cat ( y que no est√° ordenado adem√°s). Podemos usar fct_infreq para order las categor√≠as por frecuencia (por defecto, lo hace en orden descendente).\n\nsuicides |> \n  na.omit() |> \n  add_count(hdi_cat) |> \n  ggplot(aes(fct_infreq(hdi_cat))) +\n  geom_bar(stat = \"count\") +\n  labs(x = \"HDI Level\", y = \"Count\") +\n  theme_minimal()"
  },
  {
    "objectID": "posts/factores-en-r/index.html#fct_rev",
    "href": "posts/factores-en-r/index.html#fct_rev",
    "title": "Factores en R",
    "section": "fct_rev",
    "text": "fct_rev\nPodemos hacer lo mismo anterior, pero ordenarlos en forma ascendente. Para ellos usamos fct_rev.\n\nsuicides |> \n  na.omit() |> \n  add_count(hdi_cat) |> \n  ggplot(aes(fct_rev(fct_infreq(hdi_cat)))) +\n  geom_bar(stat = \"count\") +\n  labs(x = \"HDI Level\", y = \"Count\") +\n  theme_minimal()"
  },
  {
    "objectID": "posts/factores-en-r/index.html#fct_count",
    "href": "posts/factores-en-r/index.html#fct_count",
    "title": "Factores en R",
    "section": "fct_count",
    "text": "fct_count\nEsta funci√≥n nos permite contar los datos por cada nivel de las categor√≠as.\n\nfct_count(suicides$hdi_cat)\n\n# A tibble: 5 √ó 2\n  f                         n\n  <fct>                 <int>\n1 Medium Development     1752\n2 High Development       2952\n3 Very High Development  3600\n4 Low Development          60\n5 Missing               19456\n\n\nEsta funci√≥n reemplaza lo que podemos hacer con la combinaci√≥n de group_by y summarise. Es lo mismo, pero con menos l√≠neas de c√≥digo. Esto podr√≠a ser √∫til en algunos casos para ser m√°s productivo, pero se pierde legibildad, pues hay que saber qu√© hace exactamente fct_count. En cambio, la combinaci√≥n de funciones es m√°s expl√≠cita. Queda a tu criterio de programador cual usar.\n\nsuicides |> \n  group_by(hdi_cat) |> \n  summarise(n = n())\n\n# A tibble: 5 √ó 2\n  hdi_cat                   n\n  <fct>                 <int>\n1 Medium Development     1752\n2 High Development       2952\n3 Very High Development  3600\n4 Low Development          60\n5 Missing               19456"
  },
  {
    "objectID": "posts/factores-en-r/index.html#fct_unique",
    "href": "posts/factores-en-r/index.html#fct_unique",
    "title": "Factores en R",
    "section": "fct_unique",
    "text": "fct_unique\nEsta funci√≥n es similar a unique de la base de R. O sea, muestra los valores √∫nicos de una variable.\n\nfct_unique(suicides$hdi_cat) \n\n[1] Medium Development    High Development      Very High Development\n[4] Low Development       Missing              \n5 Levels: Medium Development High Development ... Missing\n\n\n\nunique(suicides$hdi_cat)\n\n[1] Missing               Medium Development    High Development     \n[4] Very High Development Low Development      \n5 Levels: Medium Development High Development ... Missing"
  },
  {
    "objectID": "posts/factores-en-r/index.html#fct_collapse",
    "href": "posts/factores-en-r/index.html#fct_collapse",
    "title": "Factores en R",
    "section": "fct_collapse",
    "text": "fct_collapse\nEsta funci√≥n nos permite crear un factor a partir de otros. Es decir, colapsarlos en otro (o agruparlos).\nPara revisar esta funci√≥n, ahora usaremos la variable generation. Crearemos 2 nuevas variables que contienen distintas categor√≠as.\n\nsuicides |> \n  mutate(generation = as_factor(generation)) |> \n  mutate(generations = fct_collapse(generation,\n    \"Older Generations\" = c(\"Silent\", \"G.I. Generation\", \"Boomers\"),\n    \"Younger Generations\" = c(\"Generation X\", \"Generation Z\", \"Millenials\")\n  )) |> \n  pull(generations) |> \n  levels()\n\n[1] \"Younger Generations\" \"Older Generations\""
  },
  {
    "objectID": "posts/factores-en-r/index.html#fct_other",
    "href": "posts/factores-en-r/index.html#fct_other",
    "title": "Factores en R",
    "section": "fct_other",
    "text": "fct_other\nEsta funci√≥n permite agrupar niveles, para compararlo con uno en especial. Para definir la categor√≠a que queremos mentener usamos el argumento keep y el resto se agrupa en other.\n\nsuicides |> \n  mutate(silent_against_other = fct_other(generation, keep = \"Silent\")) |> \n  pull(silent_against_other) |> \n  levels()\n\n[1] \"Silent\" \"Other\""
  },
  {
    "objectID": "posts/factores-en-r/index.html#fct_recode",
    "href": "posts/factores-en-r/index.html#fct_recode",
    "title": "Factores en R",
    "section": "fct_recode",
    "text": "fct_recode\nEsta funci√≥n permite recodificar un nivel. Es decir, asignarle un nuevo nombre a una categor√≠a. Esto tiene varias posibilidades, como hacer m√°s entendible una variable.\n\nsuicides |> \n  mutate(age_levels = fct_recode(age,\n                                 \"Child\" = \"5-14 years\",\n                                 \"Adolescent/Young Adult\"= \"15-24 years\",\n                                 \"Adult\" = \"25-34 years\",\n                                 \"Middle-Aged Adult\"= \"35-54 years\",\n                                 \"Older Adult\" = \"55-74 years\",\n                                 \"Senior\" = \"75+ years\")) |> \n  pull(age_levels) |> \n  levels()\n\n[1] \"Child\"                  \"Adolescent/Young Adult\" \"Adult\"                 \n[4] \"Middle-Aged Adult\"      \"Older Adult\"            \"Senior\""
  },
  {
    "objectID": "posts/factores-en-r/index.html#fct_reorder2",
    "href": "posts/factores-en-r/index.html#fct_reorder2",
    "title": "Factores en R",
    "section": "fct_reorder2",
    "text": "fct_reorder2\nEsta funci√≥n se aplica para los gr√°ficos, junto a ggplot2. Lo que hace es reordenar los valores en base a un atributo.\n\nsuicides |> \n  filter(country == \"Chile\") |> \n  group_by(year, age) |> \n  summarise(suicides_total = sum(suicides_no)) |> \n  ggplot(aes(year, suicides_total, colour = fct_reorder2(age, year, suicides_total))) +\n  geom_line(size = 2) + \n  labs(title = \"Suicides in Chile\",\n       y = \"Total Number of Suicides\",\n       x = \"Year\", colour = \"Age\") +\n  theme_minimal()\n\n\n\n\nMira lo que pasa si no usamos fct_reorder2. Las l√≠neas de la gr√°fica son las mismas obviamente, pero la leyenda cambia y los colores de √©stas tambi√©n. La leyenda aparece en el orden de las categor√≠as que ordenamos antes, pero en el gr√°fco anterior, se ordenan en base al √∫ltimo valor y por tanto, la leyenda coincide con el orden de las lineas. Eso hace que sea m√°s legible y f√°cil de interpretar.\n\nsuicides |> \n  filter(country == \"Chile\") |> \n  group_by(year, age) |> \n  summarise(suicides_total = sum(suicides_no)) |> \n  ggplot(aes(year, suicides_total, colour = age)) +\n  geom_line(size = 2) + \n  labs(title = \"Suicides in Chile\",\n       y = \"Total Number of Suicides\",\n       x = \"Year\", colour = \"Age\") +\n  theme_minimal()"
  },
  {
    "objectID": "posts/factores-en-r/index.html#fct_relabel",
    "href": "posts/factores-en-r/index.html#fct_relabel",
    "title": "Factores en R",
    "section": "fct_relabel",
    "text": "fct_relabel\nEsta funci√≥n permite modificar el nombre de los niveles.\n\nsuicides$age |> \n  fct_relabel(~ str_replace_all(.x, \"years\", \" \")) |> \n  head()\n\n[1] 15-24   35-54   15-24   75+     25-34   75+    \nLevels: 5-14   15-24   25-34   35-54   55-74   75+"
  },
  {
    "objectID": "posts/factores-en-r/index.html#fct_anon",
    "href": "posts/factores-en-r/index.html#fct_anon",
    "title": "Factores en R",
    "section": "fct_anon",
    "text": "fct_anon\nEsta funci√≥n permite anonimizar una variable categ√≥rica.\n\nsuicides |> \n  mutate(generation = as_factor(generation) |> \n  fct_anon()) |> \n  group_by(generation) |> \n  count()\n\n# A tibble: 6 √ó 2\n# Groups:   generation [6]\n  generation     n\n  <fct>      <int>\n1 1           4990\n2 2           6408\n3 3           2744\n4 4           6364\n5 5           1470\n6 6           5844\n\n\nSe puede agregar un prefijo a cada categor√≠a anonimizada:\n\nsuicides |> \n  mutate(generation = as_factor(generation) |> \n  fct_anon(\"x-\")) |> \n  group_by(generation) |> \n  count()\n\n# A tibble: 6 √ó 2\n# Groups:   generation [6]\n  generation     n\n  <fct>      <int>\n1 x-1         2744\n2 x-2         6364\n3 x-3         1470\n4 x-4         6408\n5 x-5         5844\n6 x-6         4990"
  },
  {
    "objectID": "posts/factores-en-r/index.html#finalmente",
    "href": "posts/factores-en-r/index.html#finalmente",
    "title": "Factores en R",
    "section": "Finalmente‚Ä¶",
    "text": "Finalmente‚Ä¶\nHemos revisado muchas de las funciones de la librer√≠a forcats, que como ves es bastante extensa. De hecho, no revisamos todas, hay varias m√°s, pero me parece que son muy espec√≠ficas y para casos muy puntuales. De todas formas, recuerda visitar la documentaci√≥n oficial para revisar m√°s detalles.\nComo puedes ver, el conocer alguna de estas funciones es de mucha utilidad para trabajar con datos categ√≥ricos. Y como siempre, recuerda practicar y practicar. No hay otra forma de que las cosas se te queden en la cabeza.\nNos vemos!!! üòÉ"
  },
  {
    "objectID": "posts/ml-en-googlesheets/index.html",
    "href": "posts/ml-en-googlesheets/index.html",
    "title": "Usa la inteligencia artificial en tu hoja de c√°lculo",
    "section": "",
    "text": "Hola!\nBienvenido/a por estos lados. Si ya has le√≠do post anteriores, muchas gracias por volver. Si es tu primera, espero que te sea √∫til e interesante.\nHoy te traigo un tutorial que creo te va a interesar‚Ä¶ usaremos modelos de inteligencia artificial de aprendizaje supervisado para predecir algunas cosas.\nPero‚Ä¶\nUsando una hoja de c√°lculo de Google. Si, ese que se parece a Excel. Y solo con unos clics.\nTe sorprender√°s! Ser√°s capaz de usar por ti mismo/a un modelo de inteligencia artificial con tus datos e increiblemente simple. Sigue leyendo. Te voy a explicar paso a paso c√≥mo usarlo y, a grandes rasgos, c√≥mo interpretarlo."
  },
  {
    "objectID": "posts/ml-en-googlesheets/index.html#qu√©-es-la-inteligencia-artificial",
    "href": "posts/ml-en-googlesheets/index.html#qu√©-es-la-inteligencia-artificial",
    "title": "Usa la inteligencia artificial en tu hoja de c√°lculo",
    "section": "¬øQu√© es la inteligencia artificial?",
    "text": "¬øQu√© es la inteligencia artificial?\nLa inteligencia artificial (IA) es una tecnolog√≠a que permite a las m√°quinas imitar el comportamiento inteligente de los humanos. Esto significa que las m√°quinas con IA pueden hacer cosas que normalmente requerir√≠an inteligencia humana, como reconocer patrones, aprender de experiencias pasadas y tomar decisiones basadas en informaci√≥n.\nHay diferentes tipos de IA, pero en general, todos trabajan utilizando algoritmos y datos para ‚Äúense√±ar‚Äù a las m√°quinas a hacer cosas de manera m√°s eficiente y precisa. Por ejemplo, una m√°quina con IA puede analizar grandes cantidades de informaci√≥n m√©dica para ayudar a los m√©dicos a diagnosticar enfermedades, o puede ayudar a una empresa a predecir qu√© productos ser√°n m√°s populares en el futuro.\nLa IA es una tecnolog√≠a muy √∫til y puede ayudarnos a hacer muchas cosas m√°s eficientes y r√°pidas, pero tambi√©n es importante recordar que es solo una herramienta y no puede reemplazar completamente a la inteligencia humana. Adem√°s, es importante utilizar la IA de manera responsable y √©tica para asegurarnos de que est√° beneficiando a la humanidad en lugar de causar problemas.\nEn el campo de la inteligencia artificial (IA), existen diferentes tipos de aprendizaje autom√°tico. Algunos de los tipos m√°s comunes incluyen:\n\nAprendizaje supervisado.\nAprendizaje no supervisado.\nAprendizaje por refuerzo.\n\nEl modelo de IA que vamos a usar en este tutorial es de aprendizaje supervisado.\nEl aprendizaje supervisado es un tipo de aprendizaje autom√°tico en el que una m√°quina o modelo de aprendizaje autom√°tico es entrenado con un conjunto de datos etiquetados previamente. Los datos etiquetados incluyen un conjunto de entradas (tambi√©n conocidas como caracter√≠sticas) y una etiqueta de salida (tambi√©n conocida como la clase o el objetivo). La m√°quina utiliza estos datos para aprender a predecir la etiqueta de salida para nuevas entradas.\nPor ejemplo, si estamos entrenando un modelo de aprendizaje autom√°tico para reconocer rostros, podr√≠amos proporcionarle un conjunto de im√°genes de rostros etiquetadas con el nombre de la persona en cada imagen. El modelo utilizar√≠a estos datos para aprender a reconocer rostros y predecir el nombre de la persona en nuevas im√°genes.\nEl aprendizaje supervisado es un tipo de aprendizaje autom√°tico muy com√∫n y es utilizado en una amplia variedad de aplicaciones, como la clasificaci√≥n de correos electr√≥nicos como spam o no spam, la detecci√≥n de fraudes en transacciones financieras y la predicci√≥n del precio de una propiedad. Es una forma efectiva de aprender a partir de datos etiquetados, pero requiere un conjunto de datos etiquetado previamente y puede no ser tan efectivo cuando el conjunto de datos no est√° bien equilibrado o no est√° completamente limpio.\nOk‚Ä¶ Ya con estos conceptos generales en mente, avancemos en el tema. Iremos paso a paso.\nPara efectos de demostraci√≥n, usar√© los mismos datos de ejemplo que proporciona Google. Ah! Lo mencion√©, pero no lo expliqu√©. Usarmos TensorFlow de Google."
  },
  {
    "objectID": "posts/ml-en-googlesheets/index.html#section",
    "href": "posts/ml-en-googlesheets/index.html#section",
    "title": "Usa la inteligencia artificial en tu hoja de c√°lculo",
    "section": "",
    "text": "¬øQu√© es TensorFlow?\nTensorFlow es una biblioteca de software de c√≥digo abierto para el aprendizaje autom√°tico y el c√°lculo num√©rico. Fue desarrollada por Google y se utiliza ampliamente en la industria y en la investigaci√≥n para construir y entrenar modelos de aprendizaje autom√°tico.\nEs muy flexible y se puede utilizar para construir y entrenar una amplia variedad de modelos de aprendizaje autom√°tico, desde redes neuronales simples hasta modelos m√°s complejos como redes neuronales profundas. Tambi√©n es muy escalable y se puede utilizar en una amplia gama de plataformas, desde computadoras personales hasta servidores de alta escala y dispositivos m√≥viles.\nTensorFlow es ampliamente utilizado en muchas √°reas, como la visi√≥n por computadora, el procesamiento del lenguaje natural, la detecci√≥n de fraudes y la predicci√≥n del precio de las acciones. Es una de las bibliotecas de aprendizaje autom√°tico m√°s populares y ampliamente utilizadas en la industria y en la investigaci√≥n.\nPerfecto."
  },
  {
    "objectID": "posts/ml-en-googlesheets/index.html#instalar-plugin",
    "href": "posts/ml-en-googlesheets/index.html#instalar-plugin",
    "title": "Usa la inteligencia artificial en tu hoja de c√°lculo",
    "section": "Instalar plugin",
    "text": "Instalar plugin\nPara los efectos del tutorial, debemos instalar una extensi√≥n de Google Sheets para poder usar TensorFlow en ella. Para eso, tenemos que ir al Marketplace de Google e instalar el plugin Simple ML for Sheets.\n\n\n\n\n\nBueno, no lo mencion√©, pero debes tener una cuenta de Google y haber iniciado la sesi√≥n antes de instalar este plugin."
  },
  {
    "objectID": "posts/ml-en-googlesheets/index.html#veamos-los-datos",
    "href": "posts/ml-en-googlesheets/index.html#veamos-los-datos",
    "title": "Usa la inteligencia artificial en tu hoja de c√°lculo",
    "section": "Veamos los datos",
    "text": "Veamos los datos\nUsaremos los datos que nos proporciona Google.\nAnda a este link y c√≥piate la plantilla de Google Sheets.\n\n\n\n\n\nEsta plantilla tiene 4 datasets o conjuntos de datos precargados. Para nuestro tutorial, usaremos el de lo ping√ºinos, que es el caso 1. Este sirve para mostrar el uso de TensorFLow para completar los datos faltantes aplicando algunos modelos de predicci√≥n.\nLos datos mostrados corresponden a una serie de mediciones de los ping√ºinos como el largo del pico, peso, largo de sus aletas y sexo. Esto, para 3 tipos de especies de ping√∫inos de la Ant√°rtica: Chinstrap, Gentoo y Adelie.\nSeguramente si ya tienes algo de experiencia en programaci√≥n, este dataset te suene. Pues si, es uno de los cl√°sicos que se usa para ense√±ar distintos modelos de ML.\n\nOk. Con el plugin ya instalado y los datos ya disponibles, veamos algo‚Ä¶\nF√≠jate que las primeras 30 filas no contiene datos de la especie del ping√ºino. Esos espacios en blanco (o datos faltantes) son los que por medio de un modelo de ML trataremos de predecir cu√°l deber√≠a ir en cada caso."
  },
  {
    "objectID": "posts/ml-en-googlesheets/index.html#hagamos-la-predicci√≥n",
    "href": "posts/ml-en-googlesheets/index.html#hagamos-la-predicci√≥n",
    "title": "Usa la inteligencia artificial en tu hoja de c√°lculo",
    "section": "Hagamos la predicci√≥n",
    "text": "Hagamos la predicci√≥n\n\n\n\n\n\nEn la pesta√±a de arriba, vamos a Extensiones, luego a Simple ML for Sheets y hacemos clic en Start.\nSe abrir√° una peque√±a ventana a la derecha de la planilla.\n\n\n\n\n\nF√≠jate en un par de cosas‚Ä¶\nHaremos una predicci√≥n de los datos faltantes. Si haces clic en el listado te saldr√°n varias opciones. Por ahora usaremos la primera: Predict missing values.\n\n\n\n\n\nLuego, nos aseguramos que la columna indicada es la de Species, que es donde est√°n los datos vac√≠os.\nEn la parte de Souce columns podemos ver el tipo de datos de cada columna del set de datos.\n\n\n\n\n\nCat corresponde a datos categ√≥ricos y Num a datos num√©ricos.\nNuestros datos a predecir son categ√≥ricos. Este detalle es importante a la hora de dise√±ar modelos de aprendizaje autom√°ticos, pero eso se escapa de este tutorial, as√≠ que no lo veremos ac√° por ahora.\nEn la siguiente opci√≥n podemos especificar el modeloo algoritmo de ML que usaremos. Para este tutorial vamos a usar un Decision Tree.\n\n\n\n\n\nUsaremos ese modelo, pues es uno de los m√°s simples de entender, es f√°cil de graficar y de explicar. Tiene limitaciones y muchas veces hay modelos m√°s √∫tiles y con mejor desempe√±o, pero para partir est√° ok.\nUna vez ya seleccionadas todas esas opciones, apreta el bot√≥n Predict.\nLuego de unos segundos, en la plantilla se van crear 2 columnas: una con los valores de la predicci√≥n (donde est√°n vacios) y otra con un porcentaje (llamada Pred:Conf.species). Esta columna indica qu√© tan ‚Äúseguro‚Äù est√° el modelo sobre el predicciones en la columna Pred:species. Este valor va de 0% a 100%. Como este es un modelo de clasificaci√≥n, esta m√©trica corresponde al Accuracy.\n\n\n\n\n\nA continuaci√≥n te muestro los valores que me arrojaron a mi. Ojo, que esas cifras podr√≠an variar en tu caso.\n\nNota que a√∫n cuando el modelo que usamos en de los m√°s simples, sus resultados son bastante buenos.\nUn detalle relevante es que esa capacidad de predicci√≥n y accuracy se basa en los datos de entrenamiento. Es decir, el modelo se entrena con un determinado numero de datos y luego aplica el modelo sobre dato que no ha visto. Para ello, hay diversas t√©cnicas para mejorar el poder predictivo de los algoritmos y es, tambi√©n, uno de los pasos m√°s importantes en el dise√±o e implementaci√≥n de modelos de ML. Hay, adem√°s, modelos pre-entrenados. Es decir, que alguien ya los entren√≥, habotualmente con miles o millones de datos y que los puedes usar con determinados ajustes. Pero de eso, no hablaremos ahora."
  },
  {
    "objectID": "posts/ml-en-googlesheets/index.html#explicando-el-modelo",
    "href": "posts/ml-en-googlesheets/index.html#explicando-el-modelo",
    "title": "Usa la inteligencia artificial en tu hoja de c√°lculo",
    "section": "Explicando el modelo",
    "text": "Explicando el modelo\nYa, ok. Todo muy lindo. Pero esto parece magia. O sea, selecciono un par de cosas, apreto un bot√≥n y me dice los valores que faltan.\nBueno, eso es algo que no debe pasar o que de alguna forma, debemos reducir. La caja negra que est√° detr√°s de los modelos de ML puede ser muy simple de entender o imposible definitivamente. El tema de la explicabilidad y transparencia algor√≠tmica es un √°rea de estudio muy relevante en el uso √©tico de datos, m√°s a√∫n cuando cada d√≠a m√°s el uso de IA est√° m√°s extenso y de f√°cil acceso.\nEste es un tutorial nada m√°s, pero todo este tema es algo que jam√°s debemos pasar por alto. Mucho menos en salud o en la toma de decisiones de pol√≠tica p√∫blica. Dicho eso, sigamos‚Ä¶\nEl plugin de TensorFlow tiene algunas opciones para explicar el modelo y sus resultados.\nA ver, no es tan configurable como uno podr√≠a hacerlo programando el modelo, pero me parece interesante que incluya este tipo de cosas.\nVeamos‚Ä¶\nEn la primera parte, ahora seleccionamos Understand a model:\n\n\n\n\n\nLuego, en la secci√≥n de Model, elegimos el modelo para ‚Äúexplicar‚Äù. Los modelos se guardan con la fecha y hora en que fueron ejecutadoscpor defecto. Si es el primero que haces, bueno, solo tendr√°s 1 opci√≥n para seleccionar. Si has hecho varias, f√≠jate bien cu√°l seleccionas.\nHay una opci√≥n de cambiar el nombre al modelo para que se m√°s simple saber cu√°l es cu√°l. Para ello, anda a Manage models y renombra los que desees. Tambi√©n puedes eliminar los que ya no necesites.\nMarca la opci√≥n de Include sheet data y dale al bot√≥n de Understand.\nSe abrir√° una ventana con variada informaci√≥n.\n\n\n\n\n\nNo todas las pesta√±as contienen informaci√≥n, pues depende del modelo que hayamos usado.\nVeamos la importancia de las variables.\n\nLa importancia de las variables permite analizar qu√© tanto influye una variable (entiende eso como las columnas del set de datos) para predecir, en nuestro caso, la especie del ping√ºino.\nF√≠jate en la √∫ltima secci√≥n: SUM_SCORE\nVariable Importance: SUM_SCORE:\n    1. \"bill_length_mm\" 144.918849 ################\n    2.         \"island\" 98.055598 ##########\n    3.    \"body_mass_g\"  5.873762 \nPara predecir la especie, el modelo usa principalmente esas 3 variables, pero fundamentalmente el largo del pico (bill_length_mm), pues obtiene un puntaje mayor. La isla tambi√©n parece ser importante para determinar la especie.\nSi usas otros modelos, te van a aparecer muchas otras m√°s m√©tricas. Si quieres ver qu√© significan, puedes visitar su documentaci√≥n oficial.\nLa √∫ltima pesta√±a corresponde a un gr√°fico del modelo (plot).\n\nOk‚Ä¶ no es el gr√°fico m√°s lindo del mundo jajajja üòÖ\nYa, pero lo que trata de mostrar el el camino que sigui√≥ para tomar la decisi√≥n de clasificaci√≥n de las especies. Son las reglas que se usaron, finalmente,\nTree #0:\n    \"bill_length_mm\">=43.15 [s:0.493991 n:281 np:152 miss:1] ; val:\"Adelie\" prob:[0.44484, 0.359431, 0.19573]\n        ‚îú‚îÄ(pos)‚îÄ \"island\" is in [BITMAP] {Dream, Torgersen} [s:0.645103 n:152 np:57 miss:0] ; val:\"Gentoo\" prob:[0.0328947, 0.618421, 0.348684]\n        |        ‚îú‚îÄ(pos)‚îÄ \"bill_length_mm\">=46.05 [s:0.107149 n:57 np:43 miss:0] ; val:\"Chinstrap\" prob:[0.0701754, 0, 0.929825]\n        |        |        ‚îú‚îÄ(pos)‚îÄ val:\"Chinstrap\" prob:[0, 0, 1]\n        |        |        ‚îî‚îÄ(neg)‚îÄ \"body_mass_g\">=3975 [s:0.419554 n:14 np:5 miss:1] ; val:\"Chinstrap\" prob:[0.285714, 0, 0.714286]\n        |        |                 ‚îú‚îÄ(pos)‚îÄ val:\"Adelie\" prob:[0.8, 0, 0.2]\n        |        |                 ‚îî‚îÄ(neg)‚îÄ val:\"Chinstrap\" prob:[0, 0, 1]\n        |        ‚îî‚îÄ(neg)‚îÄ val:\"Gentoo\" prob:[0.0105263, 0.989474, 0]\n        ‚îî‚îÄ(neg)‚îÄ val:\"Adelie\" prob:[0.930233, 0.0542636, 0.0155039]\n¬øRecuerdas que era el largo del pico la variable m√°s importante?\nBueno, f√≠jate que ac√° est√° al inicio del √°rbol, pues es la que permite una mayor discriminaci√≥n de especies. Por ejemplo, si el largo del pico es menor de 43.14 mm con un 93% de probabilidades la especie es Adelie. Debes seguir el camino."
  },
  {
    "objectID": "posts/ml-en-googlesheets/index.html#usemos-el-modelo",
    "href": "posts/ml-en-googlesheets/index.html#usemos-el-modelo",
    "title": "Usa la inteligencia artificial en tu hoja de c√°lculo",
    "section": "Usemos el modelo",
    "text": "Usemos el modelo\nHasta ahora todo bien. Pero una de las gracias es poder usar el modelo con nuevos datos y predecir, en este caso, la especie.\nAc√° es relevante que el modelo que usemos haya tenido buenos resultados y nos de cierta seguridad de que va a predecir bien.\nEn la plantilla, crea una nueva hoja en blanco y pone algunos datos de ping√ºinos en las variables, dejando el de la especie en blanco. Los datos te los puedes inventar siquieres, pero ten ojo con no poner cosas muy extra√±as, de lo contrario, los resultados ser√°n cualquier cosa, pues no son coherentes. O sea, poner un peso de 10.000 gramos es como extramo por ejemplo. Imagina que la idea de este ejemplo es que tienes datos de unos ping√ºinos y quieres saber su especie. Son datos que deben estar dentro de cierta l√≥gica.\nUna vez que tengas los datos en la nueva hoja (aseg√∫rate que los nombres las columnas sean iguales a las originales), seleccionamos la opcion de Make predictions with a model:\n\n\n\n\n\nLuego, elegimos el modelo que usaremos para esas predicciones. Es el mismo que ten√≠amos antes o uno nuevo si es es que hiciste m√°s pruebas.\nDale al bot√≥n Predict.\nAl igual que antes, se crear√°n 2 columnas, una con la especia predicha y otra con el porcentaje de confianza que hab√≠amos visto antes.\nEstos son mis resultados:\n\n\n\n\n\nHay m√°s opciones como guardar el modelo para poder usarlo despu√©s o poder usar otros modelos de clasificaci√≥n como Random Forest o Gradient Boosted Trees.\nPrueba alguno de ellos y ve que te resulta.\nInvestiga c√≥mo usarlo con tus propios datos. Ten en mente que deber√°s entrenar el algoritmo primero antes de usar el modelo. Dale una vuelta para que veas c√≥mo se hace. Igual, hago otro tutorial al respecto m√°s adelante."
  },
  {
    "objectID": "posts/ml-en-googlesheets/index.html#conclusiones",
    "href": "posts/ml-en-googlesheets/index.html#conclusiones",
    "title": "Usa la inteligencia artificial en tu hoja de c√°lculo",
    "section": "Conclusiones",
    "text": "Conclusiones\nLa inteligencia artificial est√° cada d√≠a m√°s presente en nuestras vidas, aunque no nos demos cuenta. Por eso es importante entenderla.\nEste tutorial busca acercar la IA a las personas que lo ven muy lejano o que no saben programar este tipo de cosas. Pero ojo, como te mencion√© antes, debe usarse con responsabilidad y sentido √©tico. Este ejmplo de los ping√ºinos es entretenido, pero otra cosa es clasificar a personas para decidir si van a recibir un beneficio social o si son atendidos en la urgencia. Ve este tutorial como lo que es, un ejemplo que de cada d√≠a la IA est√° m√°s cerca. Y que se nos abre todo un mundo por delante. Posiblemente en alg√∫n tiempo m√°s (no mucho) una de las competencias m√°s buscadas ser√° la de saber aplicar modelos de ML en distintas plataformas e integrarlas, m√°s que saber escribir c√≥digo propiamente tal. Eso si, demandar√° mayor niveles de conocimientos, manejo √©tico de datos y entender en profundidad los que estamos haciendo.\nAh!!! No te dije algo importante üòÅ\nLa primera parte de este art√≠culo fue escrita casi totalmente por chatGPT, una inteligencia artificial que fue entrenada para mantener conversaciones naturales."
  },
  {
    "objectID": "posts/personalizado-el-blog/index.html",
    "href": "posts/personalizado-el-blog/index.html",
    "title": "Personalizando el blog",
    "section": "",
    "text": "Ya tenemos nuestro blog!\nY ya que al subir este nuevo post, voy a cambiar la configuraci√≥n, te dejo esta imagen de la versi√≥n original del cual parto.\nAhora toca cambia algunas cosas para que el blog sea m√°s funcional y adecuado."
  },
  {
    "objectID": "posts/personalizado-el-blog/index.html#cambiando-el-t√≠tulo",
    "href": "posts/personalizado-el-blog/index.html#cambiando-el-t√≠tulo",
    "title": "Personalizando el blog",
    "section": "Cambiando el t√≠tulo",
    "text": "Cambiando el t√≠tulo\nEntre esas cosas, quiero cambiar el t√≠tulo del blog. Ahora dice ‚Äúmi_blog‚Äù, que es el nombre del archivo en donde tengo el proyecto completo y que al crear el blog con Quarto, toma este nombre (del archico) por defecto. Pero es feo y quiero cambiarlo por algo m√°s representativo.\nDentro de las carpetas y estructura que se crean por defecto, en la raiz del proyecto hay un archivo que se llama _quarto.yml. Este es un archivo YAML que contiene algunos datos de configuraci√≥n general del blog.\n\n\n\n\n\nSi te fijas, ah√≠ est√° el t√≠tulo del blog, el ‚Äúmi blog‚Äù que deseo cambiar.\nPues bueno, entonces habr√° que modificarlo. Voy a poner mi direcci√≥n web paulovillarroel.com que es un dominio que compr√© hace unos d√≠as y lo configur√© en mi cuenta de Netlify para linkearla a mi blog. Hacer eso es bastante simple. La web de Netlify es bien intuitiva.\nPues bueno‚Ä¶\nPara ver los cambios que voy realizando en el blog, sin necesidad de renderizar y desplegar nada (no es buena idea hacer todo es cada vez que se hacen cambios, pues la idea es hacerlo cuando cuando ya se tienen varios cambios finalizados y que comprobamos que funcionan y nos gustan). Para ello, lo que har√© es previsualizar la web.\nEso se hace del men√∫ superior de RStudio, en la pesta√±a de Build est√° la opci√≥n de Preview Website. Esto abrir√° una pesta√±a del navegador con el preview de lo que estamos haciendo.\n\n\n\n\n\nOk!! Vamos avanzando!! üéâ\nYa tenemos cambiado el nombre de la web, arriba a la izquierda. Y aparace una nueva entrada a blog que es este mismo art√≠culo que estoy escribiendo.\nPero sigue apareciendo ‚Äúmi_blog‚Äù como una especia de t√≠tulo.\nPara modificar esa secci√≥n, es necesario cambiar algunas cosas de la p√°gina que estamos visualizando. En la carpeta ra√≠z del proyecto, tenemos el archivo index.qmd. Si lo abrimos, nos encontramos con lo siguiente:\n\n\n\n\n\nVemos que tiene como t√≠tulo ‚Äúmi_blog‚Äù. Maravilloso!!!\nAhora procedemos a cambiarlo. En mi caso le he puesto simplemente Blog.\nHay varias opciones de configuraci√≥n aqu√≠, pero por ahora las dejaremos igual. Ya las retomar√© m√°s adelante.\nF√≠jate que si ya tienes la pesta√±a del navegador abierta (la que se abri√≥ al hacer clic en Preview) y la actualizas, se ver√°n reflejados los cambios (debes guardarlos antes eso si). Recuerda que este preview est√° solo en tu computador y no en la versi√≥n publicada en la web. Por eso la direcci√≥n sale localhost, que es tu pc."
  },
  {
    "objectID": "posts/personalizado-el-blog/index.html#cambiando-links",
    "href": "posts/personalizado-el-blog/index.html#cambiando-links",
    "title": "Personalizando el blog",
    "section": "Cambiando links",
    "text": "Cambiando links\nSi eres curioso/a, te habr√°s dado cuenta que los links de los √≠conos que salen arriba a la derecha te llevan a las web oficiales de GitHub y Twitter. Bueno, eso es algo que debemos cambiar, para poner los link a mis redes sociales respectivas.\nEn el archivo _quarto.yml que ya vimos antes, est√°n los link para modificarlos. Pongo mis links correctos y aprovecho de agregar mi perfil de LinkedIn.\nFinalmente, me queda as√≠‚Ä¶"
  },
  {
    "objectID": "posts/personalizado-el-blog/index.html#cambiando-el-tema",
    "href": "posts/personalizado-el-blog/index.html#cambiando-el-tema",
    "title": "Personalizando el blog",
    "section": "Cambiando el tema",
    "text": "Cambiando el tema\nEl tema por defecto del blog no est√° mal, pero me gustar√≠a probar otro.\nQuarto usa Bootstrap 5 y Boostwatch para los temas. Tiene 25 temas ya inlcu√≠dos por defecto, los cuales los puedes ver y revisar en la web de Quarto.\nEn mi caso usar√© el tema darkly, ya que me gustan los temas oscuros.\nPara cambiar el tema tan solo debes cambiar el nombre que aparece en theme en el YAML. Sin embargo, podemos hacer algo interesante ac√° y es agregar la opci√≥n de tener 2 temas, uno claro y otro oscuro para que sea el usuario quien pueda elegir cual le acomode m√°s. Para eso, en la secci√≥n themes debemos hacer lo siguiente:\n\n\n\n\n\nLa verdad, es que podr√≠amos cambiar muchas m√°s cosas del tema, incluso crear un tema propio. Para ello, tendr√≠amos que tocar el CSS, pero este es un tutorial para principiantes, as√≠ que no me voy a meter en esos temas por ahora. Pero cre√©me, conocer de HTML y CSS es algo muy √∫til. Te lo dejo ah√≠.\nNota que puse la versi√≥n dark arriba de la light, pues quiero que esa sea la por defecto al entrar a la web."
  },
  {
    "objectID": "posts/personalizado-el-blog/index.html#cambiando-el-layout",
    "href": "posts/personalizado-el-blog/index.html#cambiando-el-layout",
    "title": "Personalizando el blog",
    "section": "Cambiando el layout",
    "text": "Cambiando el layout\nHasta ahora, el blog se ve de esta forma:\n\n\n\n\n\nPero esa configuraci√≥n de c√≥mo se ven los art√≠culo no me termina de convencer.\nSi revisamos la web de Quarto, tenemos varias opciones para el layout:\n\ndefault\ngrid\ntable\n\nPara modificar esta configuraci√≥n, debemos hacerlo del archivo index.qmd de la raiz del proyecto. En la secci√≥n listing/type.\nDe las 3 opciones, me gusta m√°s la opci√≥n de grid. Con ello, el blog queda algo as√≠‚Ä¶"
  },
  {
    "objectID": "posts/personalizado-el-blog/index.html#agregando-comentarios",
    "href": "posts/personalizado-el-blog/index.html#agregando-comentarios",
    "title": "Personalizando el blog",
    "section": "Agregando comentarios",
    "text": "Agregando comentarios\nUna de las cosas interesantes de un blog es la interacci√≥n con los visitantes. Y una de las opciones interesantes que te ofrece Quarto es agregar la posibilidad de que las personas puedan escribir comentarios en los art√≠culos.\nEn la web de Quarto puedes leer m√°s detalles de esta implementaci√≥n.\nEn este caso, yo usar√© Giscus, que se basa en los comentarios de GitHub. Aprovecharemos la instancia que ya tenemos nustro blog en Github para usar esta funcionalidad.\nOk, vamos con esto‚Ä¶\n\nTenemos que ir a la web de Giscus y darle al bot√≥n install.\n\n\n\n\n\n\n\nDebemos elegir el repositorio de GitHub al cual haremos referencia, en este caso, el que creamos en el art√≠culo anterior.\n\n\n\n\n\n\n\nLe damos a Install.\nEs posible que GitHub te pida algunos permisos o verificaciones.\nDebes asegurarte que tienes las discusiones habilitadas en tu repositorio. Si no has cambiado nada, por defecto la debes tener desactivadas. Para activarlas, tienes que ir a tu repositorio del blog e ir a Settings.\n\n\n\n\n\n\n\nEn la secci√≥n de configuraci√≥n (settings) baja un poco en la p√°gina. Encontrar√°s una secci√≥n llamada Features. Marca la opci√≥n de Discussions.\n\n\n\n\n\n\n\nAnda al archivo _quarto.yml y agrega este c√≥digo:\n\n\ncomments:\n  giscus:\n    repo: YOURGITHUBACCOUNT/YOURREPO\nL√≥gicamente, en donde dice repo debes poner la direcci√≥n web de tu repositorio de GitHub en donde tienes el proyecto.\nEn mi caso, queda as√≠:\n\n\n\n\n\n\nPor defecto, al agregar esta ll√≠nea de c√≥digo se pondr√° en cada p√°gina de la web la opci√≥n de discusiones. En mi caso, solo quiero √©sto para los art√≠culos, pero no para la parte en donde muestro todos los art√≠culos y otras partes como el about (que no lo hemos tocado hasta ahora, por cierto).\nPara evitar √©sto, debemos incluir en el YAML de los .qmd respectivos el siguiente c√≥digo:\n\n\ncomments: false"
  },
  {
    "objectID": "posts/personalizado-el-blog/index.html#algunos-toques-finales",
    "href": "posts/personalizado-el-blog/index.html#algunos-toques-finales",
    "title": "Personalizando el blog",
    "section": "Algunos toques finales",
    "text": "Algunos toques finales\nPara darle algunos toques finales a la personalizaci√≥n del blog, te dejo como dej√© mi archivo _quarto.yml\nproject:\n  type: website\n\nwebsite:\n  title: \"PauloVillarroel.com\"\n  site-url: \"https://www.paulovillarroel.com/\"\n  google-analytics: \"G-RNWVZS7ZL8\"\n  navbar:\n    right:\n      - about.qmd\n      - icon: github\n        href: https://github.com/paulovillarroel\n      - icon: twitter\n        href: https://twitter.com/Chazkon\n      - icon: linkedin\n        href: https://www.linkedin.com/in/paulovillarroeltapia/\nformat:\n  html:\n    smooth-scroll: true\n    theme:\n      dark: darkly\n      light: flatly\n    css: styles.css\n    link-external-newwindow: true\n\neditor: visual\n\ncomments:\n  giscus:\n    repo: paulovillarroel/mi_blog\nOk\nYa estamos terminando este art√≠culo. Espero que te haya sido √∫til.\nA√∫n faltan cosas por configurar y personalizar, pero hemos avanzado bastante por ahora.\nNos vemos!! üöÄ"
  },
  {
    "objectID": "posts/prediccion-titanic/index.html",
    "href": "posts/prediccion-titanic/index.html",
    "title": "¬øQui√©n sobrevivi√≥ al accidente del Titanic?",
    "section": "",
    "text": "Bienvendo/a!! üòõ\nHoy vamos a hacer algo distinto. Vamos a tratar de predecir qui√©n sobrevive de los pasajeros del Titanic, usando inteligencia artificial.\nOjo. No voy a ser muy exhaustivo, ni tan profundo en el an√°lisis ni usar√© modelos de machine learning muy complejos, pues quiero que este art√≠culo te sirva como ejemplo de lo que se puede llegar a hacer y sea simple de seguir. En la vida real la cosa es un poco distinta. Generalmente, se realizan m√∫ltiples pruebas con muchos modelos y se van seleccionando en base a su desempe√±o (ya veremos qu√© significa esto). Por otro lado, los datos tienden a estar mucho m√°s sucios y el trabajo de limpieza es una de las actividades a la que m√°s se le dedica tiempo y esfuerzo.\nPara efectos de este art√≠culo, usarmos los datos de una competencia de Kaggle, en donde est√° un dataset (un conjunto de datos) sobre los pasajeros del Titanic, en donde se especifican algunas variables y quien sobrevivi√≥ o no. Puedes revisar los datos en el siguiente enlace."
  },
  {
    "objectID": "posts/prediccion-titanic/index.html#los-datos",
    "href": "posts/prediccion-titanic/index.html#los-datos",
    "title": "¬øQui√©n sobrevivi√≥ al accidente del Titanic?",
    "section": "Los datos",
    "text": "Los datos\nComo te mencionada, los datos correponden a registros de pasajeros que se subieron en el Titanic. Si, el Titanic. Ese barco brit√°nico que naufrag√≥ en el oc√©ano Atl√°ntico durante la noche del 14 y la madrugada del 15 de abril de 1912, mientras realizaba su viaje inaugural desde Southampton a Nueva York, tras chocar con un iceberg. En el hundimiento la gran mayor√≠a de las personas que iban a bordo, lo que convierte a esta cat√°strofe en uno de los mayores naufragios de la historia.\nHay una pel√≠cula de como 4 horas donde sale Di Caprio, tambi√©n. La viste?\nOk. Ya sabemos de qu√© van los datos.\nLos datos incluyen 3 archivos:\n\ngender_submission.csv\ntest.csv\ntrain.csv\n\nEl primero es un ejemplo de c√≥mo se deben subir las predicciones a la web de Kaggle para participar de la competencia (Ah! La competencia tiene que ver con qui√©n logra acertar m√°s con las predicciones).\nEl archivo test contiene datos de los pasajeros para realizar las pruebas del modelo de machine learning. No contiene datos sobre la sobrevivencia. Son los registros que hay que predecir.\nEl archivo train contiene los datos de los pasajeros para realizar el entrenamiento del algoritmo de machine learninig. Este tiene especificado si el pasajero sobrevivi√≥ o no.\nVamos con el tema‚Ä¶\nCargamos las librer√≠as:\n\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(here)\nlibrary(skimr)\nlibrary(janitor)\n\nCargamos los datos:\n\nhere::i_am(\"index.qmd\")\ntrain <- read_csv(\"train.csv\")\ntest <- read_csv(\"test.csv\")"
  },
  {
    "objectID": "posts/prediccion-titanic/index.html#an√°lisis-exploratorio",
    "href": "posts/prediccion-titanic/index.html#an√°lisis-exploratorio",
    "title": "¬øQui√©n sobrevivi√≥ al accidente del Titanic?",
    "section": "An√°lisis exploratorio",
    "text": "An√°lisis exploratorio\nEsta etapa es fundamental. el an√°lisis exploratorio de datos es una de las primeras etapas en cualquier proyectos de ciencia de datos. El objetivo es analizar los datos para entender c√≥mo est√°n compuestos y qu√© representan. Habitualmente para estos fines, se pueden usar estad√≠sticas descriptivas e inferenciales. Adem√°s, el uso de visualizaciones es un elemento super valioso.\nPor lo dem√°s, de este an√°lisis exploratorio, se pueden desprender distintas acciones de Feature Engeneering. Este es un paso muy importante en el aprendizaje autom√°tico. El feature engineering (o ingenier√≠a de caracter√≠sticas) se refiere al proceso de dise√±o de caracter√≠sticas artificiales en un algoritmo. Estas caracter√≠sticas artificiales son utilizadas por ese algoritmo para mejorar su rendimiento y precisi√≥n.\nEste paso es fundamental en los modelos de machine learning. Aunque para efectos de este art√≠culo, posiblemente no har√© todo lo que se podr√≠a hacer, como expliqu√© al inicio, para privilegiar la comprensi√≥n m√°s que los resultados.\nYa que tenemos los datos, ahora veamos un poco de qu√© se tratan:\n\nstr(train)\n\nspec_tbl_df [891 √ó 12] (S3: spec_tbl_df/tbl_df/tbl/data.frame)\n $ PassengerId: num [1:891] 1 2 3 4 5 6 7 8 9 10 ...\n $ Survived   : num [1:891] 0 1 1 1 0 0 0 0 1 1 ...\n $ Pclass     : num [1:891] 3 1 3 1 3 3 1 3 3 2 ...\n $ Name       : chr [1:891] \"Braund, Mr. Owen Harris\" \"Cumings, Mrs. John Bradley (Florence Briggs Thayer)\" \"Heikkinen, Miss. Laina\" \"Futrelle, Mrs. Jacques Heath (Lily May Peel)\" ...\n $ Sex        : chr [1:891] \"male\" \"female\" \"female\" \"female\" ...\n $ Age        : num [1:891] 22 38 26 35 35 NA 54 2 27 14 ...\n $ SibSp      : num [1:891] 1 1 0 1 0 0 0 3 0 1 ...\n $ Parch      : num [1:891] 0 0 0 0 0 0 0 1 2 0 ...\n $ Ticket     : chr [1:891] \"A/5 21171\" \"PC 17599\" \"STON/O2. 3101282\" \"113803\" ...\n $ Fare       : num [1:891] 7.25 71.28 7.92 53.1 8.05 ...\n $ Cabin      : chr [1:891] NA \"C85\" NA \"C123\" ...\n $ Embarked   : chr [1:891] \"S\" \"C\" \"S\" \"S\" ...\n - attr(*, \"spec\")=\n  .. cols(\n  ..   PassengerId = col_double(),\n  ..   Survived = col_double(),\n  ..   Pclass = col_double(),\n  ..   Name = col_character(),\n  ..   Sex = col_character(),\n  ..   Age = col_double(),\n  ..   SibSp = col_double(),\n  ..   Parch = col_double(),\n  ..   Ticket = col_character(),\n  ..   Fare = col_double(),\n  ..   Cabin = col_character(),\n  ..   Embarked = col_character()\n  .. )\n - attr(*, \"problems\")=<externalptr> \n\n\n\nstr(test)\n\nspec_tbl_df [418 √ó 11] (S3: spec_tbl_df/tbl_df/tbl/data.frame)\n $ PassengerId: num [1:418] 892 893 894 895 896 897 898 899 900 901 ...\n $ Pclass     : num [1:418] 3 3 2 3 3 3 3 2 3 3 ...\n $ Name       : chr [1:418] \"Kelly, Mr. James\" \"Wilkes, Mrs. James (Ellen Needs)\" \"Myles, Mr. Thomas Francis\" \"Wirz, Mr. Albert\" ...\n $ Sex        : chr [1:418] \"male\" \"female\" \"male\" \"male\" ...\n $ Age        : num [1:418] 34.5 47 62 27 22 14 30 26 18 21 ...\n $ SibSp      : num [1:418] 0 1 0 0 1 0 0 1 0 2 ...\n $ Parch      : num [1:418] 0 0 0 0 1 0 0 1 0 0 ...\n $ Ticket     : chr [1:418] \"330911\" \"363272\" \"240276\" \"315154\" ...\n $ Fare       : num [1:418] 7.83 7 9.69 8.66 12.29 ...\n $ Cabin      : chr [1:418] NA NA NA NA ...\n $ Embarked   : chr [1:418] \"Q\" \"S\" \"Q\" \"S\" ...\n - attr(*, \"spec\")=\n  .. cols(\n  ..   PassengerId = col_double(),\n  ..   Pclass = col_double(),\n  ..   Name = col_character(),\n  ..   Sex = col_character(),\n  ..   Age = col_double(),\n  ..   SibSp = col_double(),\n  ..   Parch = col_double(),\n  ..   Ticket = col_character(),\n  ..   Fare = col_double(),\n  ..   Cabin = col_character(),\n  ..   Embarked = col_character()\n  .. )\n - attr(*, \"problems\")=<externalptr> \n\n\nPuedes notar que los 2 archivos son similares en su composici√≥n, excepto que train tiene la variable Survived y test no. Adem√°s, train contiene casi el doble de registros (filas) que test.\nEn Kaggle se explican qu√© significan los variables (diccionario) y ponen algunas notas. Estos datos son importantes para comprender el dataset y pensar en c√≥mo ajustarlos si es necesario. En todo proyecto de datos es muy relevante entender qu√© representan los datos y cu√°les son sus posibilidades.\nDiccionario:\n\n\n\n\n\nNotas:\n\n\n\n\n\nRevisemos las primeras filas de los datos:\n\nhead(train)\n\n# A tibble: 6 √ó 12\n  PassengerId Survived Pclass Name    Sex     Age SibSp Parch Ticket  Fare Cabin\n        <dbl>    <dbl>  <dbl> <chr>   <chr> <dbl> <dbl> <dbl> <chr>  <dbl> <chr>\n1           1        0      3 Braund‚Ä¶ male     22     1     0 A/5 2‚Ä¶  7.25 <NA> \n2           2        1      1 Cuming‚Ä¶ fema‚Ä¶    38     1     0 PC 17‚Ä¶ 71.3  C85  \n3           3        1      3 Heikki‚Ä¶ fema‚Ä¶    26     0     0 STON/‚Ä¶  7.92 <NA> \n4           4        1      1 Futrel‚Ä¶ fema‚Ä¶    35     1     0 113803 53.1  C123 \n5           5        0      3 Allen,‚Ä¶ male     35     0     0 373450  8.05 <NA> \n6           6        0      3 Moran,‚Ä¶ male     NA     0     0 330877  8.46 <NA> \n# ‚Ä¶ with 1 more variable: Embarked <chr>\n\n\n\nhead(test)\n\n# A tibble: 6 √ó 11\n  PassengerId Pclass Name     Sex     Age SibSp Parch Ticket  Fare Cabin Embar‚Ä¶¬π\n        <dbl>  <dbl> <chr>    <chr> <dbl> <dbl> <dbl> <chr>  <dbl> <chr> <chr>  \n1         892      3 Kelly, ‚Ä¶ male   34.5     0     0 330911  7.83 <NA>  Q      \n2         893      3 Wilkes,‚Ä¶ fema‚Ä¶  47       1     0 363272  7    <NA>  S      \n3         894      2 Myles, ‚Ä¶ male   62       0     0 240276  9.69 <NA>  Q      \n4         895      3 Wirz, M‚Ä¶ male   27       0     0 315154  8.66 <NA>  S      \n5         896      3 Hirvone‚Ä¶ fema‚Ä¶  22       1     1 31012‚Ä¶ 12.3  <NA>  S      \n6         897      3 Svensso‚Ä¶ male   14       0     0 7538    9.22 <NA>  S      \n# ‚Ä¶ with abbreviated variable name ¬π‚ÄãEmbarked\n\n\nVeamos m√°s cosas de los datos. Usaremos la librer√≠a skimr y su funci√≥n skim para explorar los datos y generar algunas primeras estad√≠sticas de las variables.\n\nskim(train)\n\n\nData summary\n\n\nName\ntrain\n\n\nNumber of rows\n891\n\n\nNumber of columns\n12\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n5\n\n\nnumeric\n7\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nName\n0\n1.00\n12\n82\n0\n891\n0\n\n\nSex\n0\n1.00\n4\n6\n0\n2\n0\n\n\nTicket\n0\n1.00\n3\n18\n0\n681\n0\n\n\nCabin\n687\n0.23\n1\n15\n0\n147\n0\n\n\nEmbarked\n2\n1.00\n1\n1\n0\n3\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nPassengerId\n0\n1.0\n446.00\n257.35\n1.00\n223.50\n446.00\n668.5\n891.00\n‚ñá‚ñá‚ñá‚ñá‚ñá\n\n\nSurvived\n0\n1.0\n0.38\n0.49\n0.00\n0.00\n0.00\n1.0\n1.00\n‚ñá‚ñÅ‚ñÅ‚ñÅ‚ñÖ\n\n\nPclass\n0\n1.0\n2.31\n0.84\n1.00\n2.00\n3.00\n3.0\n3.00\n‚ñÉ‚ñÅ‚ñÉ‚ñÅ‚ñá\n\n\nAge\n177\n0.8\n29.70\n14.53\n0.42\n20.12\n28.00\n38.0\n80.00\n‚ñÇ‚ñá‚ñÖ‚ñÇ‚ñÅ\n\n\nSibSp\n0\n1.0\n0.52\n1.10\n0.00\n0.00\n0.00\n1.0\n8.00\n‚ñá‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n\n\nParch\n0\n1.0\n0.38\n0.81\n0.00\n0.00\n0.00\n0.0\n6.00\n‚ñá‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n\n\nFare\n0\n1.0\n32.20\n49.69\n0.00\n7.91\n14.45\n31.0\n512.33\n‚ñá‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n\n\n\n\n\n\nskim(test)\n\n\nData summary\n\n\nName\ntest\n\n\nNumber of rows\n418\n\n\nNumber of columns\n11\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n5\n\n\nnumeric\n6\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nName\n0\n1.00\n13\n63\n0\n418\n0\n\n\nSex\n0\n1.00\n4\n6\n0\n2\n0\n\n\nTicket\n0\n1.00\n3\n18\n0\n363\n0\n\n\nCabin\n327\n0.22\n1\n15\n0\n76\n0\n\n\nEmbarked\n0\n1.00\n1\n1\n0\n3\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nPassengerId\n0\n1.00\n1100.50\n120.81\n892.00\n996.25\n1100.50\n1204.75\n1309.00\n‚ñá‚ñá‚ñá‚ñá‚ñá\n\n\nPclass\n0\n1.00\n2.27\n0.84\n1.00\n1.00\n3.00\n3.00\n3.00\n‚ñÉ‚ñÅ‚ñÉ‚ñÅ‚ñá\n\n\nAge\n86\n0.79\n30.27\n14.18\n0.17\n21.00\n27.00\n39.00\n76.00\n‚ñÇ‚ñá‚ñÉ‚ñÇ‚ñÅ\n\n\nSibSp\n0\n1.00\n0.45\n0.90\n0.00\n0.00\n0.00\n1.00\n8.00\n‚ñá‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n\n\nParch\n0\n1.00\n0.39\n0.98\n0.00\n0.00\n0.00\n0.00\n9.00\n‚ñá‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n\n\nFare\n1\n1.00\n35.63\n55.91\n0.00\n7.90\n14.45\n31.50\n512.33\n‚ñá‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n\n\n\n\n\nCon el an√°lisis anterior, nos damos cuenta que hay datos faltantes (missing data) o NA¬¥s. En particular, en la variable age, que es la que tiene una gran cantidad de NA¬¥s. Este dato es relevante, pues podr√≠a afectar bastante el algoritmo. Trabajar los datos faltantes es un √°rea muy importante, por lo que no podemos dejar pasar este hecho. Ya abordaremos esto m√°s adelante.\nPor ahora, dado que ambos set de datos son muy similares, los voy a unir para dejarlos en solo 1 objeto. Esto me ser√° √∫til para el an√°lisis y feature engineering de todo el conjunto. Luego lo dividir√© nuevamente para efectos del algoritmo.\n\nall_data <- bind_rows(list(\"train\" = train, \"test\" = test), .id = \"id\")\n\n\nglimpse(all_data)\n\nRows: 1,309\nColumns: 13\n$ id          <chr> \"train\", \"train\", \"train\", \"train\", \"train\", \"train\", \"tra‚Ä¶\n$ PassengerId <dbl> 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17,‚Ä¶\n$ Survived    <dbl> 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1‚Ä¶\n$ Pclass      <dbl> 3, 1, 3, 1, 3, 3, 1, 3, 3, 2, 3, 1, 3, 3, 3, 2, 3, 2, 3, 3‚Ä¶\n$ Name        <chr> \"Braund, Mr. Owen Harris\", \"Cumings, Mrs. John Bradley (Fl‚Ä¶\n$ Sex         <chr> \"male\", \"female\", \"female\", \"female\", \"male\", \"male\", \"mal‚Ä¶\n$ Age         <dbl> 22, 38, 26, 35, 35, NA, 54, 2, 27, 14, 4, 58, 20, 39, 14, ‚Ä¶\n$ SibSp       <dbl> 1, 1, 0, 1, 0, 0, 0, 3, 0, 1, 1, 0, 0, 1, 0, 0, 4, 0, 1, 0‚Ä¶\n$ Parch       <dbl> 0, 0, 0, 0, 0, 0, 0, 1, 2, 0, 1, 0, 0, 5, 0, 0, 1, 0, 0, 0‚Ä¶\n$ Ticket      <chr> \"A/5 21171\", \"PC 17599\", \"STON/O2. 3101282\", \"113803\", \"37‚Ä¶\n$ Fare        <dbl> 7.2500, 71.2833, 7.9250, 53.1000, 8.0500, 8.4583, 51.8625,‚Ä¶\n$ Cabin       <chr> NA, \"C85\", NA, \"C123\", NA, NA, \"E46\", NA, NA, NA, \"G6\", \"C‚Ä¶\n$ Embarked    <chr> \"S\", \"C\", \"S\", \"S\", \"S\", \"Q\", \"S\", \"S\", \"S\", \"C\", \"S\", \"S\"‚Ä¶\n\n\n\nskim(all_data)\n\n\nData summary\n\n\nName\nall_data\n\n\nNumber of rows\n1309\n\n\nNumber of columns\n13\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n6\n\n\nnumeric\n7\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nid\n0\n1.00\n4\n5\n0\n2\n0\n\n\nName\n0\n1.00\n12\n82\n0\n1307\n0\n\n\nSex\n0\n1.00\n4\n6\n0\n2\n0\n\n\nTicket\n0\n1.00\n3\n18\n0\n929\n0\n\n\nCabin\n1014\n0.23\n1\n15\n0\n186\n0\n\n\nEmbarked\n2\n1.00\n1\n1\n0\n3\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nPassengerId\n0\n1.00\n655.00\n378.02\n1.00\n328.0\n655.00\n982.00\n1309.00\n‚ñá‚ñá‚ñá‚ñá‚ñá\n\n\nSurvived\n418\n0.68\n0.38\n0.49\n0.00\n0.0\n0.00\n1.00\n1.00\n‚ñá‚ñÅ‚ñÅ‚ñÅ‚ñÖ\n\n\nPclass\n0\n1.00\n2.29\n0.84\n1.00\n2.0\n3.00\n3.00\n3.00\n‚ñÉ‚ñÅ‚ñÉ‚ñÅ‚ñá\n\n\nAge\n263\n0.80\n29.88\n14.41\n0.17\n21.0\n28.00\n39.00\n80.00\n‚ñÇ‚ñá‚ñÖ‚ñÇ‚ñÅ\n\n\nSibSp\n0\n1.00\n0.50\n1.04\n0.00\n0.0\n0.00\n1.00\n8.00\n‚ñá‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n\n\nParch\n0\n1.00\n0.39\n0.87\n0.00\n0.0\n0.00\n0.00\n9.00\n‚ñá‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n\n\nFare\n1\n1.00\n33.30\n51.76\n0.00\n7.9\n14.45\n31.27\n512.33\n‚ñá‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n\n\n\n\n\nF√≠jate que hice algo extra al unir los datos. Inclu√≠ una nueva variable id en donde se se√±ala si los registros son de test o train.\nVeamos cu√°ntas personas sobrevivieron (de los datos, que son una muestra de todos los pasajeros del Titanic, que eran m√°s de 2200 en total).\n\ntable(all_data$Survived)\n\n\n  0   1 \n549 342 \n\n\nLos datos clasificados como 0 no sobrevivieron y los 1; si. Podemos ver la misma tabla, pero en proporci√≥n.\n\nround(prop.table(table(all_data$Survived)), 4) * 100\n\n\n    0     1 \n61.62 38.38 \n\n\nVeamos si el sexo nos da alguna informaci√≥n importante.\n\ntable(all_data$Sex)\n\n\nfemale   male \n   466    843 \n\n\n\ntable(all_data$Sex, all_data$Survived)\n\n        \n           0   1\n  female  81 233\n  male   468 109\n\n\n\nall_data |> \n  filter(!is.na(Survived)) |> \n  mutate(Survived = factor(Survived)) |> \n  group_by(Sex, Survived) |> \n  summarise(n = n()) |> \n  ggplot(aes(Survived, n, fill = Survived)) +\n  geom_col(show.legend = FALSE)\n\n\n\n\n\nall_data |> \n  filter(!is.na(Survived)) |> \n  mutate(Survived = factor(Survived)) |> \n  group_by(Sex, Survived) |> \n  summarise(n = n()) |> \n  ggplot(aes(Survived, n, fill = Sex)) +\n  geom_col(position = \"dodge\")\n\n\n\n\n\nall_data |> \n  filter(Survived == 1) |> # Si sobreviven\n  tabyl(Sex)\n\n    Sex   n   percent\n female 233 0.6812865\n   male 109 0.3187135\n\n\n\nall_data |> \n  filter(Survived == 0) |> # No sobreviven\n  tabyl(Sex)\n\n    Sex   n  percent\n female  81 0.147541\n   male 468 0.852459\n\n\n\nall_data |> \n  filter(!is.na(Age)) |> \n  group_by(Sex) |> \n  summarise(mean = mean(Age))\n\n# A tibble: 2 √ó 2\n  Sex     mean\n  <chr>  <dbl>\n1 female  28.7\n2 male    30.6\n\n\n\nall_data |> \n  filter(!is.na(Age), !is.na(Survived)) |> \n  group_by(Sex, Survived) |> \n  summarise(mean = mean(Age))\n\n# A tibble: 4 √ó 3\n# Groups:   Sex [2]\n  Sex    Survived  mean\n  <chr>     <dbl> <dbl>\n1 female        0  25.0\n2 female        1  28.8\n3 male          0  31.6\n4 male          1  27.3\n\n\n\nall_data |> \n  filter(!is.na(Age), !is.na(Survived)) |> \n  ggplot(aes(Age, Sex)) +\n  geom_boxplot()\n\n\n\n\n\nall_data |> \n  filter(!is.na(Age), !is.na(Survived)) |> \n  ggplot(aes(Age, factor(Survived), fill = Sex)) +\n  geom_boxplot()\n\n\n\n\nRevisemos algo de los datos las clases:\n\nall_data |>\n  ggplot(aes(factor(Pclass), fill = factor(Pclass))) +\n  geom_bar() +\n  scale_fill_discrete(name = \"Class\", labels = c(\"1st\", \"2nd\", \"3rd\"))\n\n\n\n\n\nall_data |> \n  filter(!is.na(Survived)) |> \n  ggplot(aes(factor(Pclass), fill = factor(Survived))) +\n  geom_bar(position = \"dodge\")\n\n\n\n\nAc√° aparece algo interesate. Las personas de tercera clase, en su mayor√≠a, no sobrevivieron. A diferencia de los de primera clase, donde hubo m√°s sobrevivientes que fallecidos (proporcionalmente).\nRevisemos el costo de los tickets.\n\nall_data |> \n  ggplot(aes(Fare)) + \n  geom_histogram(bins = 30)\n\n\n\n\n\nall_data |> \n  filter(!is.na(Survived)) |> \n  ggplot(aes(Fare, fill = factor(Survived))) + \n  geom_density(alpha = 0.6)\n\n\n\n\nEstos datos se√±alan en el mismo sentido de que las personas de clases m√°s altas y con tickets m√°s caros, sobrevivieron m√°s."
  },
  {
    "objectID": "posts/prediccion-titanic/index.html#manejo-de-nas",
    "href": "posts/prediccion-titanic/index.html#manejo-de-nas",
    "title": "¬øQui√©n sobrevivi√≥ al accidente del Titanic?",
    "section": "Manejo de NA¬¥s",
    "text": "Manejo de NA¬¥s\nVolvamos un poco a revisar los datos faltantes (NA¬¥s).\n\nsummary(VIM::aggr(all_data))\n\n\n\n\n\n Missings per variable: \n    Variable Count\n          id     0\n PassengerId     0\n    Survived   418\n      Pclass     0\n        Name     0\n         Sex     0\n         Age   263\n       SibSp     0\n       Parch     0\n      Ticket     0\n        Fare     1\n       Cabin  1014\n    Embarked     2\n\n Missings in combinations of variables: \n              Combinations Count     Percent\n 0:0:0:0:0:0:0:0:0:0:0:0:0   183 13.98013751\n 0:0:0:0:0:0:0:0:0:0:0:0:1     2  0.15278839\n 0:0:0:0:0:0:0:0:0:0:0:1:0   529 40.41252865\n 0:0:0:0:0:0:1:0:0:0:0:0:0    19  1.45148969\n 0:0:0:0:0:0:1:0:0:0:0:1:0   158 12.07028266\n 0:0:1:0:0:0:0:0:0:0:0:0:0    87  6.64629488\n 0:0:1:0:0:0:0:0:0:0:0:1:0   244 18.64018335\n 0:0:1:0:0:0:0:0:0:0:1:1:0     1  0.07639419\n 0:0:1:0:0:0:1:0:0:0:0:0:0     4  0.30557678\n 0:0:1:0:0:0:1:0:0:0:0:1:0    82  6.26432391\n\n\nYa lo hab√≠amos visto antes. Los registros tienen bastantes datos faltantes (missing data). Estos datos se presentan especialmente en las variables Age y Cabin. Survived tienen muchos NA¬¥s, pues unimos los regitros al inicio por temas metodol√≥gicos, pero ac√° no tiene relevancia.\nAc√° tenemos un tema muy relevante en proyectos de datos y es el manejar los datos faltantes y tomar decisiones al respecto. En este art√≠culo no profundizar√© demasiado en este tema, pues se escapa del objetivo de ser m√°s demostrativo. Pero es un √°rea relevante de estudio que posiblemente aborde en otro art√≠culo m√°s adelante.\nEn este caso, tomar√© 2 decisiones. Con respecto a la variable Cabin no la considar√© para el desarrollo y entrenamiento del algoritmo, pues tiene un porcentaje muy alto de p√©rdida y adaptarlo a algo √∫til parece ser poco factible, en un primer momento. Con la variable Age pasa algo distinto, pues parece tener m√°s relevancia en la probabilidad de sobrevivir.\nEntonces, haremos algo para completar esos datos. Una alternativa es imputar datos. Es decir, asumir la edad en base a algunos supuestos. Existen varias metodolog√≠as como calcular la media o mediana de todos los datos de edad y pon√©rselo a los NA¬¥s. Otro m√©todo es aplicar algoritmos de machine learning para predecir ese dato e imputarlo.\nVeamos algunos datos de la edad:\n\nsummary(all_data$Age)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n   0.17   21.00   28.00   29.88   39.00   80.00     263 \n\n\n\nall_data |> \n  ggplot(aes(Age)) +\n  geom_histogram()\n\n\n\n\nDebido a que la distribuci√≥n de la edad se acerca a una distribuci√≥n normal (histograma) y a que la media y mediana se a cercan bastante, podriamos usar el primer enfoque de amputaci√≥n, que es usar la mediana para completar los datos faltantes. Te recuerdo que este es solo un enfoque posible. En proyectos reales, lo que se hace es probar muchos modelos y m√©todos y analizar su desempe√±o. Insisto, para efectos de este art√≠culo, usar√© este acercamiento.\nVerifiquemos las normalidad de los datos de edad. Primero, realizaremos un gr√°fico QQ plot, el cual consiste en comparar los cuantiles de la distribuci√≥n observada con los cuantiles te√≥ricos de una distribuci√≥n normal con la misma media y desviaci√≥n est√°ndar que los datos. En la medida que los datos se ajusten a la l√≠nea proyectada, m√°s se aproximan a una distribuci√≥n normal.\nAplicar√© la funci√≥n lillie.test de la librer√≠a nortest para usar el test de Lilliefors. Este test nace con la idea de resolver uno de los problemas del test Kolmogorov-Smirnov que es al no conocer la media y la varianza poblacional, tiene poca potencia estad√≠stica. El test Lilliefors asume que la media y varianza son desconocidas y es especialmente √∫til en muestras grandes.\n\nqqnorm(all_data$Age, pch = 19, col = \"gray50\")\nqqline(all_data$Age)\n\n\n\n\n\nnortest::lillie.test(all_data$Age)\n\n\n    Lilliefors (Kolmogorov-Smirnov) normality test\n\ndata:  all_data$Age\nD = 0.078928, p-value < 2.2e-16\n\n\nVemos que tanto QQ plot y el test Lilliefors apuntan a que los datos tienen una distribuci√≥n normal.\nVolvamos a ver los datos, separados por sexo:\n\nall_data |> \n  ggplot(aes(Age)) +\n  geom_histogram() +\n  facet_grid(~ Sex)\n\n\n\n\nImputaci√≥n de datos.\nPodr√≠amos imputar los datos faltantes usando las siguientes l√≠neas de c√≥digo.\n\nall_data$Age[is.na(all_data$Age)] <- median(all_data$Age, na.rm = TRUE)\nall_data$Fare[is.na(all_data$Fare)] <- mean(all_data$Fare, na.rm = TRUE)\nall_data$Embarked[is.na(all_data$Embarked)] <- mode(all_data$Embarked)"
  },
  {
    "objectID": "posts/prediccion-titanic/index.html#feature-engineering",
    "href": "posts/prediccion-titanic/index.html#feature-engineering",
    "title": "¬øQui√©n sobrevivi√≥ al accidente del Titanic?",
    "section": "Feature Engineering",
    "text": "Feature Engineering\nEl dataset tiene algunas variables que son de dudosa ayuda para predecir y generar el algoritmo, al menos, a priori. Casi por sentido com√∫n. A√∫n cuando tomar decisiones por sentido com√∫n podr√≠a no ser la mejor alternativa, muchas veces nos equivocamos o traspasamos nuestros sesgos a los an√°lisis. La idea siempre es tratar de dejar que ‚Äúlos datos hablen‚Äù. En este caso, como es un art√≠culo m√°s referencial, no profundizar√© en la generaci√≥n y manipulaci√≥n de las variables para no confundir tanto, pero si har√© un par de cosas que me parecen interesantes y que muestra las posibilidades del feature engineering.\nLas variables Ticket, Name y Cabin parecen ser datos poco informativos para saber si alguien sobrevivi√≥ o no al accidente. Sin embargo, no ser√≠a tan as√≠. Es posible que la cabina tenga alguna relaci√≥n con la sobrevivencia, quiz√°s por cercan√≠a a un pasillo o algo as√≠. El n¬∞ de ticket tambi√©n quiz√°s se asocia a algo, no lo sabemos, es necesario investigar. Que como ya te mencion√©, no lo haremos en esta ocasi√≥n jaja üòÑ\nPero si revisemos un poco m√°s el nombre de los pasajeros. Si revisas, el registro contiene el nombre de la personas y su t√≠tulo (Mr, Miss, Dr, Col, etc). Ese dato podr√≠a ser √∫til para la predicci√≥n. Quiz√°s hab√≠a alg√∫n tipo de jerarqu√≠a que favoreciera que se salvaran. Tambi√©n el apellido de las personas podr√≠a ser √∫til, m√°s que nada por temas de agrupar a familias.\nEntonces, crearemos 2 nuevas variables: Title y Surname. Para ello, usaremos la funci√≥n mutate y nos ayudaremos de las REGEX (expresiones regulares) para extraer los datos desde el nombre.\nConsejo: aprende REGEX. Es una gran herramienta en el an√°lisis de datos y aplica no solo para programaci√≥n en R, sino que para cualquier otro. Incluso se usa en navegadores web, email y diversas aplicaciones.\n\nall_data <- all_data |>\n  mutate(\n    Title = str_extract(Name, \"(?<=,[:space:])(.*?)[.]\"),\n    Surname = str_extract(Name, \".*(?=[,])\")\n  )\n\nVeamos como nos quedaron los datos‚Ä¶\n\nall_data |> \n  head()\n\n# A tibble: 6 √ó 15\n  id    Passen‚Ä¶¬π Survi‚Ä¶¬≤ Pclass Name  Sex     Age SibSp Parch Ticket  Fare Cabin\n  <chr>    <dbl>   <dbl>  <dbl> <chr> <chr> <dbl> <dbl> <dbl> <chr>  <dbl> <chr>\n1 train        1       0      3 Brau‚Ä¶ male     22     1     0 A/5 2‚Ä¶  7.25 <NA> \n2 train        2       1      1 Cumi‚Ä¶ fema‚Ä¶    38     1     0 PC 17‚Ä¶ 71.3  C85  \n3 train        3       1      3 Heik‚Ä¶ fema‚Ä¶    26     0     0 STON/‚Ä¶  7.92 <NA> \n4 train        4       1      1 Futr‚Ä¶ fema‚Ä¶    35     1     0 113803 53.1  C123 \n5 train        5       0      3 Alle‚Ä¶ male     35     0     0 373450  8.05 <NA> \n6 train        6       0      3 Mora‚Ä¶ male     28     0     0 330877  8.46 <NA> \n# ‚Ä¶ with 3 more variables: Embarked <chr>, Title <chr>, Surname <chr>, and\n#   abbreviated variable names ¬π‚ÄãPassengerId, ¬≤‚ÄãSurvived\n\n\n\nunique(all_data$Title)\n\n [1] \"Mr.\"           \"Mrs.\"          \"Miss.\"         \"Master.\"      \n [5] \"Don.\"          \"Rev.\"          \"Dr.\"           \"Mme.\"         \n [9] \"Ms.\"           \"Major.\"        \"Lady.\"         \"Sir.\"         \n[13] \"Mlle.\"         \"Col.\"          \"Capt.\"         \"the Countess.\"\n[17] \"Jonkheer.\"     \"Dona.\"        \n\n\nPodemos ampliar la variable Sex en base a los t√≠tulos. En especial, agregar algo que identifique a los ni√±os. Si te fijaste, los sobrevivientes son en general m√°s j√≥venes. Adem√°s, existe esta frase t√≠pica de salvar a los ni√±os y las mujeres primero. No tengo claro si eso es tan real, pero podr√≠a serlo. As√≠ que agreguemos la posibilidad.\n\nMan <- c(\n  \"Mr.\", \"Sir.\", \"Don.\", \"Rev.\", \"Major.\",\n  \"Col.\", \"Capt.\", \"Jonkheer.\",\n  \"Dr.\", \"Nobel.\"\n)\nFemale <- c(\"Mrs.\", \"Miss.\", \"Mme.\", \"Ms.\", \"Lady.\", \"Mlle.\", \"the Countess.\", \"Dona.\")\nBoy <- c(\"Master.\")\n\nall_data <- all_data |>\n  rowwise() |>\n  mutate(\n    Sex =\n      case_when(\n        (Title %in% Man) ~ \"Man\",\n        (Title %in% Female) ~ \"Female\",\n        (Title %in% Boy) ~ \"Boy\",\n        TRUE ~ Title\n      )\n  )\n\nRevisemos c√≥mo quedan los grupos con esta nueva clasificaci√≥n:\n\nall_data |> \n  filter(!is.na(Survived)) |> \n  tabyl(Sex, Survived)\n\n    Sex   0   1\n    Boy  17  23\n Female  81 232\n    Man 451  87\n\n\n\nall_data |> \n  filter(!is.na(Survived)) |> \n  mutate(Survived = factor(Survived)) |> \n  group_by(Sex, Survived) |> \n  summarise(n = n()) |> \n  ggplot(aes(Sex, n, fill = Survived)) +\n  geom_col(position = \"dodge\")\n\n\n\n\nAhora vamos a crear una nueva variable, que es el tama√±o de la familia. Para eso, sumaremos las variables SibSp (n¬∞ de hermanos y esposas/os) y Parch (n¬∞ de hijos y padres). A la suma le agregamos 1, pues agregamos al mismo pasajero al grupo familiar.\n\nall_data <- all_data |>\n  mutate(\n    Family_size = as.numeric(SibSp) + as.numeric(Parch) + 1,\n    Family_type = factor(ifelse(Family_size == 1, \"Single\",\n      ifelse(Family_size <= 3, \"Small\", \"Large\")\n    ))\n  ) |> \n  unnest(cols = c())\n\nRevisemos nuevamente los datos faltantes. Esto es importante, pues muchos modelos de machine learning no soportan variables con NA¬¥s o pueden alterar las predicciones.\n\nsummary(VIM::aggr(all_data))\n\n\n\n\n\n Missings per variable: \n    Variable Count\n          id     0\n PassengerId     0\n    Survived   418\n      Pclass     0\n        Name     0\n         Sex     0\n         Age     0\n       SibSp     0\n       Parch     0\n      Ticket     0\n        Fare     0\n       Cabin  1014\n    Embarked     0\n       Title     0\n     Surname     0\n Family_size     0\n Family_type     0\n\n Missings in combinations of variables: \n                      Combinations Count   Percent\n 0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0   204 15.584416\n 0:0:0:0:0:0:0:0:0:0:0:1:0:0:0:0:0   687 52.482811\n 0:0:1:0:0:0:0:0:0:0:0:0:0:0:0:0:0    91  6.951872\n 0:0:1:0:0:0:0:0:0:0:0:1:0:0:0:0:0   327 24.980901\n\n\nPor ahora, voy a dejar el an√°lisis exploratorio hasta ac√°. Podr√≠amos estar mucho tiempo m√°s revisando y d√°ndole vueltas al tema, pero creo que con lo que hemos visto ya te has hecho una buena idea de qu√© va √©sto.\nHabitualmente, los proyecto de Ciencia de Datos destinan cerca del 80% del tiempo en este tipo de actividades."
  },
  {
    "objectID": "posts/prediccion-titanic/index.html#modelamiento-ml",
    "href": "posts/prediccion-titanic/index.html#modelamiento-ml",
    "title": "¬øQui√©n sobrevivi√≥ al accidente del Titanic?",
    "section": "Modelamiento ML",
    "text": "Modelamiento ML\nPara el modelamiento del algoritmo de machine learning (ML) usaremos la librer√≠a H2O.\nH2O funciona bastante distinto que otras librer√≠as, pues lo que hace es levantar un servidor y nos conoctamos a √©l, y es en ese servidor donde se ejecutan los algoritmos. H2O es una plataforma de autoML o de modelos de machine learning pre-entrenados. En palabras simples, la configuraci√≥n de los algoritmos viene ya lista ‚Äúde f√°brica‚Äù. En palabras m√°s complejas, los modelos de autoML ajustan el tuneo de los hiperpar√°metros de forma autom√°tica o ya cuentan con valores predefinidos.\n\n# Separamos el dataset para los datos de entrenamiento\ntitanic_train <- all_data |>\n  filter(id == \"train\") |>\n  select(-c(id, PassengerId, Name, Ticket, Cabin))\n\nAnalicemos los tipos de datos de titanic_train:\n\nstr(titanic_train)\n\ntibble [891 √ó 12] (S3: tbl_df/tbl/data.frame)\n $ Survived   : num [1:891] 0 1 1 1 0 0 0 0 1 1 ...\n $ Pclass     : num [1:891] 3 1 3 1 3 3 1 3 3 2 ...\n $ Sex        : chr [1:891] \"Man\" \"Female\" \"Female\" \"Female\" ...\n $ Age        : num [1:891] 22 38 26 35 35 28 54 2 27 14 ...\n $ SibSp      : num [1:891] 1 1 0 1 0 0 0 3 0 1 ...\n $ Parch      : num [1:891] 0 0 0 0 0 0 0 1 2 0 ...\n $ Fare       : num [1:891] 7.25 71.28 7.92 53.1 8.05 ...\n $ Embarked   : chr [1:891] \"S\" \"C\" \"S\" \"S\" ...\n $ Title      : chr [1:891] \"Mr.\" \"Mrs.\" \"Miss.\" \"Mrs.\" ...\n $ Surname    : chr [1:891] \"Braund\" \"Cumings\" \"Heikkinen\" \"Futrelle\" ...\n $ Family_size: num [1:891] 2 2 1 2 1 1 1 5 3 2 ...\n $ Family_type: Factor w/ 3 levels \"Small\",\"Single\",..: 1 1 2 1 2 2 2 3 1 1 ...\n\n\nCambiaremos algunos tipos de datos por algo m√°s √∫til.\n\ntitanic_train <- titanic_train |>\n  mutate(\n    Survived = factor(Survived),\n    Pclass = factor(Pclass),\n    Sex = factor(Sex),\n    Embarked = factor(Embarked),\n    Title = factor(Title)\n  )\n\nOk. Tenemos nuestros datos de entrenamiento listos o, al menos, m√°s preparados para ser analizados.\n\n# Cargamos la librer√≠a de H2O\nlibrary(h2o)\n\n# Creaci√≥n de un cluster local con todos los cores disponibles\nh2o.init(\n  ip = \"localhost\",\n  # -1 indica que se empleen todos los cores disponibles\n  nthreads = -1,\n  # M√°xima memoria disponible para el cluster\n  max_mem_size = \"3g\"\n)\n\n\nH2O is not running yet, starting it now...\n\nNote:  In case of errors look at the following log files:\n    C:\\Users\\pvill\\AppData\\Local\\Temp\\RtmpIffI42\\file5c1c27484d8d/h2o_pvill_started_from_r.out\n    C:\\Users\\pvill\\AppData\\Local\\Temp\\RtmpIffI42\\file5c1c3c976d3e/h2o_pvill_started_from_r.err\n\n\nStarting H2O JVM and connecting:  Connection successful!\n\nR is connected to the H2O cluster: \n    H2O cluster uptime:         2 seconds 801 milliseconds \n    H2O cluster timezone:       America/Santiago \n    H2O data parsing timezone:  UTC \n    H2O cluster version:        3.36.1.2 \n    H2O cluster version age:    4 months and 14 days !!! \n    H2O cluster name:           H2O_started_from_R_pvill_ufb539 \n    H2O cluster total nodes:    1 \n    H2O cluster total memory:   2.98 GB \n    H2O cluster total cores:    16 \n    H2O cluster allowed cores:  16 \n    H2O cluster healthy:        TRUE \n    H2O Connection ip:          localhost \n    H2O Connection port:        54321 \n    H2O Connection proxy:       NA \n    H2O Internal Security:      FALSE \n    R Version:                  R version 4.2.1 (2022-06-23 ucrt) \n\n# Se eliminan los datos del cluster por si ya hab√≠a sido iniciado\nh2o.removeAll()\n\n# Para que no se muestre la barra de progreso\nh2o.no_progress()\n\n\ntitanic_h2o = as.h2o(titanic_train)\n\nPara nuestro caso, aplicaremos autoML o machine learning automatizado.\nEn la documentaci√≥n oficial de H2O puedes ver m√°s detalles.\nVamos al tema y veamos c√≥mo se podr√≠a implementar una soluci√≥n simple de autoML.\n\n# Separaci√≥n de las observaciones en conjunto de entrenamiento y test\nsplits <- h2o.splitFrame(data = titanic_h2o, ratios = 0.8, seed = 123)\ntitanic_train_h2o <- splits[[1]]\ntitanic_test_h2o  <- splits[[2]]\n\n# Se define la variable respuesta y los predictores\nresponse <- \"Survived\"\n\n# Para este modelo se emplean todos los predictores disponibles\npredictors  <- setdiff(h2o.colnames(titanic_h2o), response)\n\n# Ejectutamos 20 modelos de autoML\naml <- h2o.automl(\n  x = predictors, y = response,\n  training_frame = titanic_train_h2o,\n  max_models = 20,\n  seed = 123\n)\n\n\n22:00:07.918: AutoML: XGBoost is not available; skipping it.\n22:00:07.948: _train param, Dropping bad and constant columns: [Surname]\n22:00:09.7: _train param, Dropping bad and constant columns: [Surname]\n22:00:09.834: _train param, Dropping bad and constant columns: [Surname]\n22:00:11.678: _train param, Dropping bad and constant columns: [Surname]\n22:00:14.686: _train param, Dropping bad and constant columns: [Surname]\n22:00:15.69: _train param, Dropping bad and constant columns: [Surname]\n22:00:15.384: _train param, Dropping bad and constant columns: [Surname]\n22:00:15.918: _train param, Dropping bad and constant columns: [Surname]\n22:00:16.213: _train param, Dropping bad and constant columns: [Surname]\n22:03:33.246: _train param, Dropping unused columns: [Surname]\n22:03:33.966: _train param, Dropping unused columns: [Surname]\n\n\n\n# Ver autoML Leaderboard\nlb <- aml@leaderboard\nprint(lb, n = nrow(lb)) \n\n                                                  model_id       auc   logloss\n1                           XRT_1_AutoML_1_20221010_220007 0.8709619 0.4280390\n2  StackedEnsemble_BestOfFamily_1_AutoML_1_20221010_220007 0.8705471 0.4134399\n3                           GBM_2_AutoML_1_20221010_220007 0.8684124 0.4212379\n4                           GBM_4_AutoML_1_20221010_220007 0.8644931 0.4242718\n5              GBM_grid_1_AutoML_1_20221010_220007_model_3 0.8636030 0.4252027\n6              GBM_grid_1_AutoML_1_20221010_220007_model_1 0.8635338 0.4239069\n7     StackedEnsemble_AllModels_1_AutoML_1_20221010_220007 0.8633567 0.4180044\n8                           GBM_5_AutoML_1_20221010_220007 0.8622764 0.4245518\n9                           GLM_1_AutoML_1_20221010_220007 0.8607726 0.4308457\n10             GBM_grid_1_AutoML_1_20221010_220007_model_4 0.8603405 0.4310881\n11                          GBM_3_AutoML_1_20221010_220007 0.8595238 0.4300964\n12                          DRF_1_AutoML_1_20221010_220007 0.8591176 0.4954942\n13             GBM_grid_1_AutoML_1_20221010_220007_model_5 0.8572120 0.4396899\n14                 DeepLearning_1_AutoML_1_20221010_220007 0.8554533 0.4421416\n15                          GBM_1_AutoML_1_20221010_220007 0.8477487 0.4579601\n16             GBM_grid_1_AutoML_1_20221010_220007_model_2 0.8461974 0.4603829\n17    DeepLearning_grid_1_AutoML_1_20221010_220007_model_1 0.8459338 0.4700233\n18    DeepLearning_grid_3_AutoML_1_20221010_220007_model_2 0.8442010 0.4643837\n19    DeepLearning_grid_3_AutoML_1_20221010_220007_model_1 0.8430386 0.4639954\n20    DeepLearning_grid_2_AutoML_1_20221010_220007_model_1 0.8421658 0.5036609\n21    DeepLearning_grid_2_AutoML_1_20221010_220007_model_2 0.8417077 0.4876815\n22    DeepLearning_grid_1_AutoML_1_20221010_220007_model_2 0.8367168 0.4939174\n       aucpr mean_per_class_error      rmse       mse\n1  0.8346343            0.1859390 0.3661840 0.1340907\n2  0.8431666            0.1848025 0.3583348 0.1284039\n3  0.8327107            0.1849797 0.3617789 0.1308840\n4  0.8276144            0.1848846 0.3614534 0.1306485\n5  0.8311913            0.1859260 0.3635181 0.1321454\n6  0.8353353            0.1885360 0.3626827 0.1315388\n7  0.8405026            0.1884539 0.3599988 0.1295991\n8  0.8292195            0.1808141 0.3626516 0.1315162\n9  0.8366766            0.2003414 0.3684605 0.1357631\n10 0.8290372            0.1826938 0.3667785 0.1345264\n11 0.8246865            0.1873866 0.3651803 0.1333567\n12 0.8311538            0.1900095 0.3704124 0.1372053\n13 0.8172740            0.1861291 0.3691785 0.1362927\n14 0.8239047            0.2093596 0.3734248 0.1394461\n15 0.8225336            0.2232089 0.3811518 0.1452767\n16 0.8238082            0.2195575 0.3818452 0.1458057\n17 0.8146578            0.2207329 0.3826731 0.1464387\n18 0.8205795            0.2111875 0.3775377 0.1425347\n19 0.8131408            0.1991919 0.3781846 0.1430236\n20 0.7876542            0.2118356 0.3902014 0.1522571\n21 0.8254982            0.1996111 0.3762526 0.1415660\n22 0.8212612            0.1992049 0.3808260 0.1450284\n\n[22 rows x 7 columns] \n\n\n\n# Podemos revisar el modelo ganador\naml@leader\n\nModel Details:\n==============\n\nH2OBinomialModel: drf\nModel ID:  XRT_1_AutoML_1_20221010_220007 \nModel Summary: \n  number_of_trees number_of_internal_trees model_size_in_bytes min_depth\n1              33                       33               39118        13\n  max_depth mean_depth min_leaves max_leaves mean_leaves\n1        20   17.72727         42        137    89.30303\n\n\nH2OBinomialMetrics: drf\n** Reported on training data. **\n** Metrics reported on Out-Of-Bag training samples **\n\nMSE:  0.1491174\nRMSE:  0.3861573\nLogLoss:  0.5119568\nMean Per-Class Error:  0.2038026\nAUC:  0.8463313\nAUCPR:  0.8090878\nGini:  0.6926627\nR^2:  0.3667232\n\nConfusion Matrix (vertical: actual; across: predicted) for F1-optimal threshold:\n         0   1    Error      Rate\n0      346  89 0.204598   =89/435\n1       54 212 0.203008   =54/266\nTotals 400 301 0.203994  =143/701\n\nMaximum Metrics: Maximum metrics at their respective thresholds\n                        metric threshold      value idx\n1                       max f1  0.437500   0.747795 179\n2                       max f2  0.378927   0.795292 202\n3                 max f0point5  0.755055   0.782353  73\n4                 max accuracy  0.651129   0.803138 103\n5                max precision  0.879121   0.963855  37\n6                   max recall  0.026050   1.000000 393\n7              max specificity  1.000000   0.997701   0\n8             max absolute_mcc  0.459596   0.585658 169\n9   max min_per_class_accuracy  0.437500   0.795402 179\n10 max mean_per_class_accuracy  0.459596   0.796413 169\n11                     max tns  1.000000 434.000000   0\n12                     max fns  1.000000 250.000000   0\n13                     max fps  0.000000 435.000000 399\n14                     max tps  0.026050 266.000000 393\n15                     max tnr  1.000000   0.997701   0\n16                     max fnr  1.000000   0.939850   0\n17                     max fpr  0.000000   1.000000 399\n18                     max tpr  0.026050   1.000000 393\n\nGains/Lift Table: Extract with `h2o.gainsLift(<model>, <data>)` or `h2o.gainsLift(<model>, valid=<T/F>, xval=<T/F>)`\n\nH2OBinomialMetrics: drf\n** Reported on cross-validation data. **\n** 5-fold cross-validation on training data (Metrics computed for combined holdout predictions) **\n\nMSE:  0.1340907\nRMSE:  0.366184\nLogLoss:  0.428039\nMean Per-Class Error:  0.185939\nAUC:  0.8709619\nAUCPR:  0.8346343\nGini:  0.7419238\nR^2:  0.4305393\n\nConfusion Matrix (vertical: actual; across: predicted) for F1-optimal threshold:\n         0   1    Error      Rate\n0      355  80 0.183908   =80/435\n1       50 216 0.187970   =50/266\nTotals 405 296 0.185449  =130/701\n\nMaximum Metrics: Maximum metrics at their respective thresholds\n                        metric threshold      value idx\n1                       max f1  0.418827   0.768683 202\n2                       max f2  0.274074   0.821104 257\n3                 max f0point5  0.628670   0.789720 135\n4                 max accuracy  0.497812   0.823110 180\n5                max precision  1.000000   1.000000   0\n6                   max recall  0.046593   1.000000 394\n7              max specificity  1.000000   1.000000   0\n8             max absolute_mcc  0.496049   0.624389 181\n9   max min_per_class_accuracy  0.418827   0.812030 202\n10 max mean_per_class_accuracy  0.418827   0.814061 202\n11                     max tns  1.000000 435.000000   0\n12                     max fns  1.000000 263.000000   0\n13                     max fps  0.023070 435.000000 399\n14                     max tps  0.046593 266.000000 394\n15                     max tnr  1.000000   1.000000   0\n16                     max fnr  1.000000   0.988722   0\n17                     max fpr  0.023070   1.000000 399\n18                     max tpr  0.046593   1.000000 394\n\nGains/Lift Table: Extract with `h2o.gainsLift(<model>, <data>)` or `h2o.gainsLift(<model>, valid=<T/F>, xval=<T/F>)`\nCross-Validation Metrics Summary: \n                             mean       sd cv_1_valid cv_2_valid cv_3_valid\naccuracy                 0.830233 0.029251   0.836879   0.871429   0.792857\nauc                      0.874299 0.027893   0.851602   0.907378   0.841389\nerr                      0.169767 0.029251   0.163121   0.128571   0.207143\nerr_count               23.800000 4.086563  23.000000  18.000000  29.000000\nf0point5                 0.763671 0.029804   0.733083   0.810277   0.743034\nf1                       0.789989 0.020935   0.772277   0.820000   0.768000\nf2                       0.818491 0.014345   0.815900   0.829959   0.794702\nlift_top_group           2.664176 0.313517   3.065218   2.857143   2.372881\nlogloss                  0.432077 0.031004   0.432689   0.391192   0.478483\nmax_per_class_error      0.187373 0.027715   0.168421   0.163265   0.222222\nmcc                      0.650161 0.049185   0.653039   0.720411   0.584946\nmean_per_class_accuracy  0.831079 0.025358   0.839703   0.863422   0.795669\nmean_per_class_error     0.168921 0.025358   0.160297   0.136578   0.204331\nmse                      0.135078 0.011345   0.135005   0.120964   0.152672\npr_auc                   0.834787 0.042016   0.773003   0.882346   0.814953\nprecision                0.747210 0.035625   0.709091   0.803922   0.727273\nr2                       0.422426 0.041388   0.385804   0.468290   0.373851\nrecall                   0.838855 0.015019   0.847826   0.836735   0.813559\nrmse                     0.367275 0.015319   0.367430   0.347799   0.390732\nspecificity              0.823302 0.044511   0.831579   0.890110   0.777778\n                        cv_4_valid cv_5_valid\naccuracy                  0.814286   0.835714\nauc                       0.876458   0.894668\nerr                       0.185714   0.164286\nerr_count                26.000000  23.000000\nf0point5                  0.768072   0.763889\nf1                        0.796875   0.792793\nf2                        0.827922   0.823970\nlift_top_group            2.333333   2.692308\nlogloss                   0.429635   0.428385\nmax_per_class_error       0.212500   0.170455\nmcc                       0.631219   0.661193\nmean_per_class_accuracy   0.818750   0.837850\nmean_per_class_error      0.181250   0.162150\nmse                       0.133763   0.132989\npr_auc                    0.849705   0.853930\nprecision                 0.750000   0.745763\nr2                        0.453803   0.430380\nrecall                    0.850000   0.846154\nrmse                      0.365736   0.364676\nspecificity               0.787500   0.829545\n\n\n\n# Importancia de las variables\nh2o.varimp(aml@leader)\n\nVariable Importances: \n      variable relative_importance scaled_importance percentage\n1        Title          940.399963          1.000000   0.283786\n2          Sex          586.606567          0.623784   0.177021\n3         Fare          567.343628          0.603300   0.171208\n4          Age          460.132050          0.489294   0.138855\n5       Pclass          328.859924          0.349702   0.099241\n6  Family_size          121.330360          0.129020   0.036614\n7        SibSp           91.768562          0.097585   0.027693\n8  Family_type           87.194229          0.092720   0.026313\n9     Embarked           85.729736          0.091163   0.025871\n10       Parch           44.400452          0.047214   0.013399\n\nh2o.varimp_plot(aml@leader)\n\n\n\n\n\n# √Årea bajo la curva AUC para los datos de entrenamiento\nh2o.auc(aml@leader, train = TRUE)\n\n[1] 0.8463313\n\nh2o.performance(model = aml@leader, train = TRUE)\n\nH2OBinomialMetrics: drf\n** Reported on training data. **\n** Metrics reported on Out-Of-Bag training samples **\n\nMSE:  0.1491174\nRMSE:  0.3861573\nLogLoss:  0.5119568\nMean Per-Class Error:  0.2038026\nAUC:  0.8463313\nAUCPR:  0.8090878\nGini:  0.6926627\nR^2:  0.3667232\n\nConfusion Matrix (vertical: actual; across: predicted) for F1-optimal threshold:\n         0   1    Error      Rate\n0      346  89 0.204598   =89/435\n1       54 212 0.203008   =54/266\nTotals 400 301 0.203994  =143/701\n\nMaximum Metrics: Maximum metrics at their respective thresholds\n                        metric threshold      value idx\n1                       max f1  0.437500   0.747795 179\n2                       max f2  0.378927   0.795292 202\n3                 max f0point5  0.755055   0.782353  73\n4                 max accuracy  0.651129   0.803138 103\n5                max precision  0.879121   0.963855  37\n6                   max recall  0.026050   1.000000 393\n7              max specificity  1.000000   0.997701   0\n8             max absolute_mcc  0.459596   0.585658 169\n9   max min_per_class_accuracy  0.437500   0.795402 179\n10 max mean_per_class_accuracy  0.459596   0.796413 169\n11                     max tns  1.000000 434.000000   0\n12                     max fns  1.000000 250.000000   0\n13                     max fps  0.000000 435.000000 399\n14                     max tps  0.026050 266.000000 393\n15                     max tnr  1.000000   0.997701   0\n16                     max fnr  1.000000   0.939850   0\n17                     max fpr  0.000000   1.000000 399\n18                     max tpr  0.026050   1.000000 393\n\nGains/Lift Table: Extract with `h2o.gainsLift(<model>, <data>)` or `h2o.gainsLift(<model>, valid=<T/F>, xval=<T/F>)`\n\n\n\n# Performance para datos de validaci√≥n\nh2o.auc(aml@leader, xval = TRUE)\n\n[1] 0.8709619\n\n\nRevisemos la curva entre Recall vs Precision.\nEstos 2 indicadores son de gran relevancia en la evaluaci√≥n del desempe√±o de machine learninig. La Precisi√≥n (tambi√©n llamada valor predictivo positivo) es la fracci√≥n de instancias relevantes entre las instancias recuperadas, mientras que el Recall (tambi√©n conocida como sensibilidad) es la fracci√≥n de instancias relevantes que se recuperaron.\n\n\n\n\n\nPuedes profundizar un poco m√°s sobre este tema en este art√≠culo.\n\nperf <- h2o.performance(model = aml@leader, xval = TRUE)\n\nmetrics <- as.data.frame(h2o.metric(perf))\n\nmetrics |>\n  ggplot(aes(recall, precision)) +\n  geom_line() +\n  theme_minimal()\n\n\n\n\n\n# Predicciones para test\npredictions <- h2o.predict(object = aml@leader, newdata = titanic_test_h2o)\npredictions\n\n  predict         p0        p1\n1       1 0.33766234 0.6623377\n2       1 0.07670676 0.9232932\n3       1 0.16081239 0.8391876\n4       0 0.62626263 0.3737374\n5       0 0.76809858 0.2319014\n6       1 0.21647427 0.7835257\n\n[190 rows x 3 columns] \n\n\nPara guardar el modelo generado en el directorio actual de tu computador y usarlo despu√©s, puedes usar este c√≥digo:\n\nh2o.saveModel(object = aml@leader, path = getwd(), force = TRUE)\n\n\n# Separamos dataset en test\ntitanic_test <- all_data |>\n  filter(id == \"test\") |>\n  select(-c(id, Survived, PassengerId, Name, Ticket, Cabin))\n\n# Creamos el objeto H2O\ntitanic_test_h2o <- as.h2o(titanic_test)\n\n\n# Predicciones en nuevo set de datos.\npredictions_test <- h2o.predict(object = aml@leader, newdata = titanic_test_h2o)\n\n# Separamos solo las predicciones.\nsurvived <- as.data.frame(predictions_test$predict)\n\nYa que tenemos nuestas predicciones realizadas, generaremos el archivo para luego subir a Kaggle.\n\nsubmission <- read_csv(\"gender_submission.csv\") |>\n  select(1)\n\ngender_submission <- as.data.frame(cbind(submission, survived)) |>\n  rename(\"Survived\" = predict)\n\nwrite.csv(gender_submission, \"submission_autoML_leader_h2o.csv\", row.names = FALSE)\n\n\n# Se apaga el cluster H2O\nh2o.shutdown(prompt = FALSE)"
  },
  {
    "objectID": "posts/prediccion-titanic/index.html#subir-a-kaggle",
    "href": "posts/prediccion-titanic/index.html#subir-a-kaggle",
    "title": "¬øQui√©n sobrevivi√≥ al accidente del Titanic?",
    "section": "Subir a Kaggle",
    "text": "Subir a Kaggle\nSubimos nuestro archivo a Kaggle y btenemos una puntuaci√≥n de 0.76076\nNo esta mal, pero podr√≠a ser mejor. Hace unos meses obtuve 0.79 usando un modelo de ML ensamblado GLM + GBM (Gradient Boosting Machine).\nEn este caso, para mejorar el desempe√±o del algoritmo que fue el seleccionado como l√≠der de la comparaci√≥n de autoML, habr√≠a que tocar los hiperpar√°metros y reanalizar el feature engineering. Adem√°s, de ver opciones de otros modelos de machine learning. Pero considerar que el modelo que usamos para este art√≠culo lo realizamos aplicando autoML. Es decir, usamos modelos pre-entrenados y con auto ajustes de hiperpar√°metros ‚Äúout of the box‚Äù (predefinidos)."
  },
  {
    "objectID": "posts/prediccion-titanic/index.html#conclusiones",
    "href": "posts/prediccion-titanic/index.html#conclusiones",
    "title": "¬øQui√©n sobrevivi√≥ al accidente del Titanic?",
    "section": "Conclusiones",
    "text": "Conclusiones\nEste art√≠culo tiene fines demostrativos, no lo olvides. En la vida real, todo este proceso es (mucho) m√°s largo y complejo. Existen muchas instancias de an√°lisis estad√≠sticos y matem√©ticos sobre los datos dsponibles. Adem√°s, es un proceso iterativo. O sea, generalmente se vuelve a etapas previas frecuentemente, se revisa lo que hay. Incluso, muchas veces es necesario investigar sobre la generaci√≥n y captura de los datos. Del mismo modo, se generan muchos modelos y se realizan muchas pruebas hasta encontrar los de mejor desempe√±o y que tengan m√°s explicabilidad (seg√∫n sea el caso).\nRevisamos, adem√°s, el uso de una tecnolog√≠a emergente que son los modelos de machine learning pre-entrenados o autoML. Este tipo de modelos est√°n cada vez m√°s difundidos, en especial, en ambientes cloud. Por ejemplo, Google y Microsoft Azure tienen los suyos.\nEl desarrollo de modelos de machine learning tradicional consume bastantes recursos, y que requieren un conocimiento del dominio y tiempo significativos para generar y comparar docenas de modelos. El autoML reduce el tiempo necesario para obtener modelos de aprendizaje autom√°tico listos para producci√≥n con gran eficiencia y facilidad. Estos elementos son vitales en la actualidad para muchas startups e industrias. Adem√°s, amplia las opciones en el uso de este tipo de tecnolog√≠as a m√°s personas y organizaciones.\nA pesar de que parece magia todo √©sto (o una caja negra que nadie sabe lo que pasa dentro), el uso de autoML ayuda a generar modelos de forma m√°s r√°pida y facilita el desarrollo de modelos m√°s completos en el corto plazo. Es una buena estrategia para screening.\nOk. Entonces no necesitamos ingenieros, matem√°ticos o cient√≠ficos de datos?\nNo, para nada. Ciertamente estas tecnolog√≠as (autoML) democratizan el acceso a m√°s personas, pero a√∫n son necesarios los conocimientos y reflexiones de los expertos en el √°rea para ajustar los modelos, controlar sesgos o problemas de equidad. Y, por cierto, que conozcan del negocio, pues finalmente el desarrollo de estas tecnolog√≠as buscan resolver problem√°ticas concretas. Sin mencionar todo el trabajo de comunicaci√≥n, explicaci√≥n y disfusi√≥n de los resultados, para los cual esos perfiles profesionales son de gran valor.\nFinalmente, este ejercicio que hemos visto ac√° es m√°s demostrativo que otra cosa, pero espero haberte mostrado c√≥mo se pueden entrenar algoritmos y usar modelos de ML para generar predicciones.\nNos vemos!! ü§™"
  },
  {
    "objectID": "posts/ruta-aprendizaje-python/index.html",
    "href": "posts/ruta-aprendizaje-python/index.html",
    "title": "Mi ruta de aprendizaje en Python",
    "section": "",
    "text": "Hola!\nTe quiero contar que, desde hace varias semanas, que estoy estudiando Python. Este ser√≠a mi tercer lenguaje de programaci√≥n, luego de R y SQL.\nYo me dedico, principalmente, al an√°lisis de datos y aplicaciones de inteligencia artificial, por lo cual este stack de tecnolog√≠a me parece bastante potente.\nA continuaci√≥n te explico porqu√© decid√≠ aprender Python y hacerlo de esta forma."
  },
  {
    "objectID": "posts/ruta-aprendizaje-python/index.html#por-qu√©-aprender-python",
    "href": "posts/ruta-aprendizaje-python/index.html#por-qu√©-aprender-python",
    "title": "Mi ruta de aprendizaje en Python",
    "section": "¬øPor qu√© aprender Python?",
    "text": "¬øPor qu√© aprender Python?\nPython es el lenguaje de programaci√≥n ‚Äúm√°s famoso‚Äù en la actualidad a nivel mundial. Bueno, si no es el primero, est√° entre los 3 primeros seguro, va a depender de la encuesta que se mire (tiene una lucha a muerte con JavaScript jajaj). Pero es innegable que este lenguaje es ampliamente buscado y las ofertas laborales son cada vez m√°s necesitadas, en especial, en temas de an√°lisis de datos, inteligencia artificial, IoT y backend.\nSi miramos Google Trends, podemos ver que el inter√©s por este lenguaje de programaci√≥n ha ido creciendo, en especial, desde el 2015 en adelante.\n\n\nPosiblemente eso tenga relaci√≥n con el auge de la inteligencia artificial, en donde Python es uno de los lenguajes m√°s usados.\nPor otra parte, si revisamos el TIOBE Index para el mes de septiembre 2022, vemos que Python se posiciona en el primer puesto del ranking, con tendencia al alza respecto del mes pasado.\n\n\n\n\n\nEn la encuesta 2022 de desarrolladores que hace StackOverflow, nuevamente vemos a Python entre los primeros lugares de los lenguajes m√°s usados.\n\n\n\n\n\nA ver, que un lenguaje sea el m√°s buscado, m√°s famoso y esas cosas no es tan importante. La verdad que lo que deber√≠a ser determinante es la utilidad que te ofrece aprenderlo. En mi caso, me sirve üòé, pues para temas de an√°lisis de datos es uno de los m√°s relevantes en la actualidad."
  },
  {
    "objectID": "posts/ruta-aprendizaje-python/index.html#por-qu√©-hacer-una-web",
    "href": "posts/ruta-aprendizaje-python/index.html#por-qu√©-hacer-una-web",
    "title": "Mi ruta de aprendizaje en Python",
    "section": "¬øPor qu√© hacer una web?",
    "text": "¬øPor qu√© hacer una web?\nEn aprendiendopython.com estoy documentando gran parte de lo que voy estudiando sobre Python.\nDecid√≠ hacer eso por varios motivos‚Ä¶\n\nMe gusta ense√±ar y me parece que puedo contribuir en ese sentido.\nLa mejor forma de estudiar y aprender algo, es ense√±√°ndolo. Al explic√°rselo a otras personas, el aprendizaje es mucho m√°s significativo y la curva es mucho m√°s r√°pida. Esta es una metodolog√≠a que uso bastante.\nA pesar de que estoy reci√©n aprendiendo Python, ya tengo conocimientos en programaci√≥n (soy programador desde hace un par de a√±os), lo cual hace que aprender un nuevo lenguaje sea mucho m√°s simple. Sin embargo, en este proyecto decid√≠ obviar un poco mis conocimientos previos e intentar explicar todo desde los m√°s simple y b√°sico, de modo que cualquiera pueda entenderlo. O, al menos, esa es mi intenci√≥n.\nIr documentando mi aprendizaje me ayuda a ir dejando un manual de estudio y notas para el futuro. En general, uno se puede aprender gran parte de las cosas de memoria y entender la l√≥gica, pero la sintaxis puede que se olvide. El contar con este material de apoyo me permitir√° tener acceso simple a esas cosas si es que lo llegara a necesitar."
  },
  {
    "objectID": "posts/ruta-aprendizaje-python/index.html#basado-en-proyectos",
    "href": "posts/ruta-aprendizaje-python/index.html#basado-en-proyectos",
    "title": "Mi ruta de aprendizaje en Python",
    "section": "Basado en proyectos",
    "text": "Basado en proyectos\nAdem√°s de ense√±ar lo que se va aprendiendo, otra forma de ir consolidando de mejor forma los conocimientos, es el realizar proyectos.\nLlevar a la pr√°ctica distintas cosas, en proyectos peque√±os, pero que permitan entender el funcionamiento del lenguaje.\nYa hice uno y lo tengo publicado. Es un peque√±o script para identificar el signo del zodiaco chino que te corresponde, seg√∫n el a√±o de nacimiento.\nyear = int(input(\"Ingresa el a√±o de nacimiento: \"))\n\ndef chinese_zodiac(year:int):\n    elements = (\"madera\", \"fuego\", \"tierra\", \"metal\", \"agua\")\n    animals = (\"rata\", \"buey\", \"tigre\", \"conejo\", \"drag√≥n\", \"serpiente\", \"caballo\", \"oveja\", \"mono\", \"gallo\", \"perro\", \"cerdo\")\n\n    if year < 604:\n        print(\"El ciclo sexagenario chino comenz√≥ en el a√±o 604. Debes introducir un a√±o adecuado.\")\n    else:\n        sexagenary_year = (year - 4) % 60\n        element = elements[int((sexagenary_year % 10) / 2)]\n        animal = animals[int(sexagenary_year % 12)]\n\n        print(f\"A√±o: {year} / Zodiaco: {animal} de {element}\")\n\nchinese_zodiac(year)"
  },
  {
    "objectID": "posts/ruta-aprendizaje-python/index.html#pero-est√°-lleno-de-otros-cursos",
    "href": "posts/ruta-aprendizaje-python/index.html#pero-est√°-lleno-de-otros-cursos",
    "title": "Mi ruta de aprendizaje en Python",
    "section": "Pero est√° lleno de otros cursos‚Ä¶",
    "text": "Pero est√° lleno de otros cursos‚Ä¶\nEfectivamente. Python, al ser uno de los lenguajes m√°s famosos, est√° lleno de tutoriales, cursos, videos en YouTube y libros, tanto de pago como, en su mayor√≠a, gratuitos. Adem√°s, la documentaci√≥n oficial de Python es bastante buena.\nEntonces, ¬øQu√© aporta aprendiendopython a la comunidad?\nPrimero. Yo no lo llamar√≠a un curso. Porque no est√° dise√±ado para ser eso. Como te mencionaba antes, lo que estoy documentando es mi ruta de aprendizaje, no estoy haciendo un curso. Pero, a pesar de eso, puede cumplir esa funci√≥n, principalmente, pues me he dedicado bastante a explicar los conceptos, dar muchos ejemplos y no dar cosas por obvias. Me parece que muchos cursos fallan en eso. Explican cosas muy por encima y no se detienen a explicarlas de forma clara.\nPor otro lado, el ritmo no es muy r√°pido. No es como esos cursos que te prometen que en 4 horas aprendes a programar y puedes tener el trabajo de tus sue√±os. Eso es mucha fantas√≠a y te venden algo que no es as√≠. Como todo en la vida, si quieres lograr las metas, debes dedicarle mucho tiempo, ser perseverante y seguir adelante, a√∫n cuando las cosas no te salgan. Como ac√° estoy documentando mis estudios, puede que le dedique m√°s tiempo a algunos temas que a otros, o que sea repetitivo en varias ocasiones.\nSin embargo, esto es j√∫stamente lo que lo hace diferente y especial. Pues yo tambi√©n lo estoy estudiando, por lo que esa expereincia de estudio y descubrimiento es muy valiosa, cosa que no ocurre en cursos o libros, donde todo est√° maqueteado. Ac√° te comento cosas que me funcionan y las que no, te hablo desde mi experiencia. Esto lo hace una forma muy especial de aprender. Es como ir al cine a ver una pel√≠culay comentar con un amigo mas mejores escenas, fallas y opiniones. Esa es una experiencia mucho m√°s entretenida que solo ir al cine. Bueno, ac√° es lo mismo.\nOk.\nTe dejo cordialmente invtada/o a seguir el aprendizaje de Python conmigo.\nüëâ Recuerda visitar aprendiendopython.com\nNos vemos!!"
  }
]