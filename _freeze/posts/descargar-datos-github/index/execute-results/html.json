{
  "hash": "51cc475ab215ac616d96bd748469687c",
  "result": {
    "markdown": "---\ntitle: \"¿Cómo descargar datos desde GitHub más rápido?\"\ndescription: \"Vemos paso a paso cómo descargar archivos desde GitHub usando Python y R. Además, tomamos el tiempo de cuánto demoran!\"\nauthor: \"Paulo Villarroel\"\ndate: \"2022-10-27\"\ncategories: [tutorial, github, python, r]\nimage: \"descarga.jpg\"\n---\n\n\nHola!!!\n\nEn esta ocasión veremos un tema que me preguntan mucho siempre. Habitualmente cuando hago clases o algún taller de programación, sale esta duda...\n\n¿Cómo descargar los archivos desde GitHub?\n\nMe refiero a descargar los archivos csv alojados en GitHub para poder usarlos.\n\nOk.\n\nRevisemos paso a paso cómo hacerlo. Y para ello, usaré 2 lenguajes de programación que me gustan mucho: Python y R. Además, que si estás interesado/a en la ciencia de datos, los más seguro es que uses alguno de ellos.\n\nAh! Acá no voy a revisar cómo descargar todo el repositorio remoto desde GitHub (clonarlo), sino a solo un archivo que esté ahí disponible. Tampoco haré un análisis de los datos. Ya habrá momento para eso.\n\nPero haremos algo entretenido, que es medir cuánto demoran cada uno!! 😎\n\n## Accediendo a GitHub\n\nPara este tutorial, usaré el [repositorio del Ministerio de Ciencias y Tecnología de Chile](https://github.com/MinCiencia/Datos-COVID19), que contiene muchos datos de los casos COVID-19.\n\nEn particular, usaremos uno de los data products, que es sobre las camas críticas disponibles a nivel nacional. Para revisar los datos, debes entrar a [esta dirección](https://github.com/MinCiencia/Datos-COVID19/tree/master/output/producto20).\n\nPara esta parte, no necesitas tener cuenta creada en GitHub.\n\nCuando entres a esa sección, te encontrarás con algo similar a ésto:\n\n![](repo1.png){fig-align=\"center\" width=\"800\"}\n\nAdemás del README con los detalles de los datos acá disponibles, verás 3 archivos .csv (comma separate values), que son básicamente los dataset con los datos. Usaremos el que tiene el sufijo `_std`, pues tiene un formato más fácil de comprender para nuestros efectos.\n\nPara poder ver esos datos, hacemos clic sobre el enlace [`NumeroVentiladores_std.csv`](https://github.com/MinCiencia/Datos-COVID19/blob/master/output/producto20/NumeroVentiladores_std.csv \"NumeroVentiladores_std.csv\")`.`\n\nSe te mostrará la siguiente información:\n\n![](repo2.png){fig-align=\"center\" width=\"800\"}\n\nSale una previsualización de los datos.\n\nMe ha tocado ver a muchos que lo que hacen acá es copiar y pegar, sin más. Es decir, seleccionan con el ratón los datos y los copian en un Excel o algo así.\n\nPor favor... No hagas eso!!!! 🤢\n\nJustamente, este artículo es para ello y que no cometas ese error.\n\nVamos...\n\nFíjate que arriba a la derecha hay unos botones. Uno de ellos dice `Raw`. Dale clic.\n\n![](repo3.png){fig-align=\"center\" width=\"300\"}\n\nSe mostrarán los datos de una forma extraña, como todos juntos. No te asustes!!! Es normal, pues es el formato de los archivos csv.\n\n![](repo4.png){fig-align=\"center\" width=\"800\"}\n\nLo relevante acá es la dirección web que sale en tu navegador. Esa dirección es la que usaremos para nuestro código y descargar los datos, pues desde las pantallas anteriores no podrás hacerlo de forma adecuada.\n\nAhora veamos cómo descargar esos datos a nuestro computador para poder usarlos en los análisis. Como te mencioné, pondré ejemplos tanto de Python como de R. Además, haré un breve benchmark para revisar cuánto se demoran los scripts en ejecutar las funciones.\n\n## Usando Python\n\nPara descargar los datos, usaremos `Pandas` y su función `read_csv()` a la cual le debemos pasar la dirección web (url) donde se encuentran los datos que vimos anteriormente en formato raw.\n\nFíjate que la url se la pasamos como un dato de tipo *string* (cadena de texto), por lo cual la tenemos que envolver con comillas.\n\nRevisemos el código:\n\n\n::: {.cell}\n\n```{.python .cell-code}\nimport pandas as pd\n\nurl = 'https://raw.githubusercontent.com/MinCiencia/Datos-COVID19/master/output/producto20/NumeroVentiladores_std.csv'\nbeds_py = pd.read_csv(url)\n```\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\nprint(beds_py.head())\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n  Ventiladores       fecha  numero\n0        total  2020-04-14    1550\n1  disponibles  2020-04-14     564\n2     ocupados  2020-04-14     986\n3        total  2020-04-15    1563\n4  disponibles  2020-04-15     577\n```\n:::\n:::\n\n\nY eso es todo!!\n\nYa creamos un objeto (dataframe) con los datos desde GitHub.\n\n## Usando R\n\nAhora vemos cómo descargar los datos con R. La sintaxis es muy similar.\n\nUsaremos la librería `reader` y su función `read_csv()`, a la cual le pasamos la url de los datos en raw. Igual que antes, se coloca la dirección como un *string* (entre comillas)\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(readr)\n\nurl <- \"https://raw.githubusercontent.com/MinCiencia/Datos-COVID19/master/output/producto20/NumeroVentiladores_std.csv\"\nbeds_r <- read_csv(url) \n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nhead(beds_r)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 6 × 3\n  Ventiladores fecha      numero\n  <chr>        <date>      <dbl>\n1 total        2020-04-14   1550\n2 disponibles  2020-04-14    564\n3 ocupados     2020-04-14    986\n4 total        2020-04-15   1563\n5 disponibles  2020-04-15    577\n6 ocupados     2020-04-15    986\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nclass(beds_r)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"spec_tbl_df\" \"tbl_df\"      \"tbl\"         \"data.frame\" \n```\n:::\n:::\n\n\nListo!!! Ya tenemos los datos.\n\nAl igual que antes, tenemos un objeto de tipo dataframe, aunque con unos pequeños detalles, pues la esta función nos devuelve un `tibble`. Si quieres indagar más sobre las diferencias entre un tibble y un dataframe clásico, puedes revisar [este enlace](https://r4ds.had.co.nz/tibbles.html). Pero para nuestros objetivos, da igual.\n\nComo puedes ver, descargar archivos desde GitHub es bastante simple y no requiere gran esfuerzo ni complicaciones. Pero quizás te estás preguntando y para qué quiero hacer eso? Si finalmente podría tener los datos en el computador y bastaría.\n\nLa gracia de tener esta \"conexión\" con el repositorio remoto de GitHub es que en la medida que los datos sean modificados, cada vez que ejecute el código, voy a descargar la última versión del archivo. Esto es muy útil para temas de automatizaciones y esas cosas. Pero como te estarás imaginando, no lo veré ahora y ya habrá momento para revisar ese tema.\n\n## Desempeño\n\nVimos cómo descargar los datos usando Python y R usando las funciones que más comunmente se suelen usar para ello. Los datos que descargamos son pequeños y no representan gran trabajo. La cosa se podría poner más compleja si la cantidad de datos es mayor. Esto es importante a la hora de desplegar un script, por lo cual es interesante evaluar cuánto demora cada función en realizar esta tarea.\n\nComo ya estamos usando estos datos, veamos cómo hacerlo en cada lenguaje.\n\n### Python\n\nForma 1: usando `time`\n\n\n::: {.cell}\n\n```{.python .cell-code}\nimport time\n\ntic = time.perf_counter()\nurl = 'https://raw.githubusercontent.com/MinCiencia/Datos-COVID19/master/output/producto20/NumeroVentiladores_std.csv'\nbeds_py_bench1 = pd.read_csv(url)\ntoc = time.perf_counter()\n\nprint(\"Segundos: \", toc - tic) # Resulstado en milisegundos\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nSegundos:  0.42979449999999986\n```\n:::\n:::\n\n\nForma 2: usando `ttictoc`\n\n\n::: {.cell}\n\n```{.python .cell-code}\nfrom ttictoc import tic,toc\n\ntic()\nurl = 'https://raw.githubusercontent.com/MinCiencia/Datos-COVID19/master/output/producto20/NumeroVentiladores_std.csv'\nbeds_py_bench2 = pd.read_csv(url)\nelapsed = toc()\n\nprint(\"Segundos: \", elapsed)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nSegundos:  0.1373408999999981\n```\n:::\n:::\n\n\nLos resultados pueden variar entre una ejecución y otra y se ven afectado por la calidad de la conexión a internet, pero vemos que entre ambos ejemplos no hay muchas diferencias en tiempo.\n\n### R\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tictoc)\n\ntic()\nurl <- \"https://raw.githubusercontent.com/MinCiencia/Datos-COVID19/master/output/producto20/NumeroVentiladores_std.csv\"\nbeds_r_bench <- read_csv(url) \ntoc()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n0.59 sec elapsed\n```\n:::\n:::\n\n\nAl igual que lo mencionado antes, los resultados pueden variar entre una ejecución y otra del códico. Sin embargo, al parecer esta forma es más lenta que en Python. Bueno, el `tidyverse` a pesar de que me encanta, no es la cosa más rápida en general. Para ello, hay otras librerías con mejor desempeño como `vroom` (que es parte de `tidyverse` de igual forma) y `data.table`.\n\nUsando vroom:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntic()\nurl <- \"https://raw.githubusercontent.com/MinCiencia/Datos-COVID19/master/output/producto20/NumeroVentiladores_std.csv\"\nbeds_r_bench1 <- vroom::vroom(url) \ntoc()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n0.31 sec elapsed\n```\n:::\n:::\n\n\nUsando data.table:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(data.table)\n\ntic()\nurl <- \"https://raw.githubusercontent.com/MinCiencia/Datos-COVID19/master/output/producto20/NumeroVentiladores_std.csv\"\nbeds_r_bench2 <- data.table::fread(url) \ntoc()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n0.47 sec elapsed\n```\n:::\n:::\n\n\nAl parecer `data.table` es más rápido, al menos en este ejemplo. Hagamos algo loco... por que hasta ahora solo han sido unas cuantas ejecuciones de cada código y puede no ser muy representativo.\n\nQué tal si corremos el código 1000 veces?\n\nSi, mil.\n\nPara suerte de nosotros, no tenemos que estar apretando el botón todas esas veces, sino que podemos usar alguna librería de *bencharking* para eso.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(microbenchmark)\n\nbench_fread <- microbenchmark(\n  url <- \"https://raw.githubusercontent.com/MinCiencia/Datos-COVID19/master/output/producto20/NumeroVentiladores_std.csv\",\n  beds_fread_bench <- data.table::fread(url),\n  times = 1000\n)\n\nbench_fread\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nUnit: nanoseconds\n                                                                                                                    expr\n url <- \"https://raw.githubusercontent.com/MinCiencia/Datos-COVID19/master/output/producto20/NumeroVentiladores_std.csv\"\n                                                                              beds_fread_bench <- data.table::fread(url)\n      min       lq     mean   median       uq       max neval cld\n        0        0     1277     1200     1800    169500  1000  a \n 11403400 14353300 20653423 16403350 18825850 466079000  1000   b\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nbench_vroom <- microbenchmark(\n  url <- \"https://raw.githubusercontent.com/MinCiencia/Datos-COVID19/master/output/producto20/NumeroVentiladores_std.csv\",\n  beds_vroom_bench <- vroom::vroom(url),\n  times = 1000\n)\n\nbench_vroom\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nUnit: nanoseconds\n                                                                                                                    expr\n url <- \"https://raw.githubusercontent.com/MinCiencia/Datos-COVID19/master/output/producto20/NumeroVentiladores_std.csv\"\n                                                                                   beds_vroom_bench <- vroom::vroom(url)\n      min       lq        mean    median        uq       max neval cld\n        0        0      1191.4       100      1800     19400  1000  a \n 77591200 96365150 116003579.9 106687450 118518250 669067900  1000   b\n```\n:::\n:::\n\n\nComo he mencionado, con cada ejecución pueden variar los resultados. El output de este microbench está en nanosegundos. Para efectos de comparación y mejor entendimiento, usaré la mediana y lo pasaré a segundos. Dejó un resultado que obtuve.\n\n*Nota: los datos mostrados en las tablas del benchmark pueden diferir.*\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# data.table\ndatatable_1000 <- 15865750 / 1.0000E+9\nprint(paste(\"Segundos:\", datatable_1000))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"Segundos: 0.01586575\"\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# vroom\nvroom_1000 <- 99581050 / 1.0000E+9\nprint(paste(\"Segundos:\", vroom_1000))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"Segundos: 0.09958105\"\n```\n:::\n:::\n\n\nDiferencia porcentual relativa:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n(abs(vroom_1000) - abs(datatable_1000)) / ((abs(datatable_1000) + abs(vroom_1000)) / 2) * 100\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 145.0284\n```\n:::\n:::\n\n\nVemos que `data.table` es un 145% más rápido. No deja de ser interesante ese dato.\n\n## Finalmente\n\nRevisamos cómo descargar archivos csv desde GitHub y lo hicimos tanto en Python como R.\n\nPractica tus análisis usando estas funciones. Investiga distintos respositorios que contengan datos y úsalos.\n\nAdemás, hicimos algunas comparaciones de cuándo demoran y vimos que hay diferencias importantes. Si bien para este ejemplo no es muy significativo, dado que el archivo es pequeño, puede llegar a ser un gran problema si no te fijas bien. El análisis critico y la decisión de qué función usar en cada momento, es algo que requiere práctica y conocimientos técnicos para optimizar el desempeño del código, en especial, en tareas de mucha carga. La optimización del código permite mejorar el rendimiento, uso de memoria y eficiencia y es un aspecto relevante en soluciones informáticas de mayor escala.\n\nNos vemos pronto!! 😁\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}