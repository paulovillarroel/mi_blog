{
  "hash": "35f75b345edb689fb8d5b5398efb5975",
  "result": {
    "markdown": "---\ntitle: \"¬øQui√©n sobrevivi√≥ al accidente del Titanic?\"\ndescription: \"Entreno 20 modelos de Machine Learning para averiguarlo.\"\nauthor: \"Paulo Villarroel\"\ndate: \"2022-10-08\"\ncategories: [r, tutorial, machine learning]\nimage: \"titanic.jpg\"\n---\n\n\nBienvendo/a!! üòõ\n\nHoy vamos a hacer algo distinto. Vamos a tratar de predecir qui√©n sobrevive de los pasajeros del Titanic, usando inteligencia artificial.\n\nOjo. No voy a ser muy exhaustivo, ni tan profundo en el an√°lisis ni usar√© modelos de machine learning muy complejos, pues quiero que este art√≠culo te sirva como ejemplo de lo que se puede llegar a hacer y sea simple de seguir. En la vida real la cosa es un poco distinta. Generalmente, se realizan m√∫ltiples pruebas con muchos modelos y se van seleccionando en base a su desempe√±o (ya veremos qu√© significa esto). Por otro lado, los datos tienden a estar mucho m√°s sucios y el trabajo de limpieza es una de las actividades a la que m√°s se le dedica tiempo y esfuerzo.\n\nPara efectos de este art√≠culo, usarmos los datos de una competencia de Kaggle, en donde est√° un dataset (un conjunto de datos) sobre los pasajeros del Titanic, en donde se especifican algunas variables y quien sobrevivi√≥ o no. Puedes revisar los datos en [el siguiente enlace](https://www.kaggle.com/competitions/titanic/data).\n\n## Los datos\n\nComo te mencionada, los datos correponden a registros de pasajeros que se subieron en el Titanic. Si, el Titanic. Ese barco brit√°nico que naufrag√≥ en el oc√©ano Atl√°ntico durante la noche del 14 y la madrugada del 15 de abril de 1912, mientras realizaba su viaje inaugural desde Southampton a Nueva York, tras chocar con un iceberg. En el hundimiento la gran mayor√≠a de las personas que iban a bordo, lo que convierte a esta cat√°strofe en uno de los mayores naufragios de la historia.\n\nHay una pel√≠cula de como 4 horas donde sale Di Caprio, tambi√©n. La viste?\n\nOk. Ya sabemos de qu√© van los datos.\n\nLos datos incluyen 3 archivos:\n\n-   gender_submission.csv\n\n-   test.csv\n\n-   train.csv\n\nEl primero es un ejemplo de c√≥mo se deben subir las predicciones a la web de Kaggle para participar de la competencia (Ah! La competencia tiene que ver con qui√©n logra acertar m√°s con las predicciones).\n\nEl archivo test contiene datos de los pasajeros para realizar las pruebas del modelo de machine learning. No contiene datos sobre la sobrevivencia. Son los registros que hay que predecir.\n\nEl archivo train contiene los datos de los pasajeros para realizar el entrenamiento del algoritmo de machine learninig. Este tiene especificado si el pasajero sobrevivi√≥ o no.\n\nVamos con el tema...\n\nCargamos las librer√≠as:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(here)\nlibrary(skimr)\nlibrary(janitor)\n```\n:::\n\n\nCargamos los datos:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nhere::i_am(\"index.qmd\")\ntrain <- read_csv(\"train.csv\")\ntest <- read_csv(\"test.csv\")\n```\n:::\n\n\n## An√°lisis exploratorio\n\nEsta etapa es fundamental. el an√°lisis exploratorio de datos es una de las primeras etapas en cualquier proyectos de ciencia de datos. El objetivo es analizar los datos para entender c√≥mo est√°n compuestos y qu√© representan. Habitualmente para estos fines, se pueden usar estad√≠sticas descriptivas e inferenciales. Adem√°s, el uso de visualizaciones es un elemento super valioso.\n\nPor lo dem√°s, de este an√°lisis exploratorio, se pueden desprender distintas acciones de **Feature Engeneering**. Este es un paso muy importante en el aprendizaje autom√°tico. El feature engineering (o ingenier√≠a de caracter√≠sticas) se refiere al proceso de dise√±o de caracter√≠sticas artificiales en un algoritmo. Estas caracter√≠sticas artificiales son utilizadas por ese algoritmo para mejorar su rendimiento y precisi√≥n.\n\nEste paso es fundamental en los modelos de machine learning. Aunque para efectos de este art√≠culo, posiblemente no har√© todo lo que se podr√≠a hacer, como expliqu√© al inicio, para privilegiar la comprensi√≥n m√°s que los resultados.\n\nYa que tenemos los datos, ahora veamos un poco de qu√© se tratan:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nstr(train)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nspec_tbl_df [891 √ó 12] (S3: spec_tbl_df/tbl_df/tbl/data.frame)\n $ PassengerId: num [1:891] 1 2 3 4 5 6 7 8 9 10 ...\n $ Survived   : num [1:891] 0 1 1 1 0 0 0 0 1 1 ...\n $ Pclass     : num [1:891] 3 1 3 1 3 3 1 3 3 2 ...\n $ Name       : chr [1:891] \"Braund, Mr. Owen Harris\" \"Cumings, Mrs. John Bradley (Florence Briggs Thayer)\" \"Heikkinen, Miss. Laina\" \"Futrelle, Mrs. Jacques Heath (Lily May Peel)\" ...\n $ Sex        : chr [1:891] \"male\" \"female\" \"female\" \"female\" ...\n $ Age        : num [1:891] 22 38 26 35 35 NA 54 2 27 14 ...\n $ SibSp      : num [1:891] 1 1 0 1 0 0 0 3 0 1 ...\n $ Parch      : num [1:891] 0 0 0 0 0 0 0 1 2 0 ...\n $ Ticket     : chr [1:891] \"A/5 21171\" \"PC 17599\" \"STON/O2. 3101282\" \"113803\" ...\n $ Fare       : num [1:891] 7.25 71.28 7.92 53.1 8.05 ...\n $ Cabin      : chr [1:891] NA \"C85\" NA \"C123\" ...\n $ Embarked   : chr [1:891] \"S\" \"C\" \"S\" \"S\" ...\n - attr(*, \"spec\")=\n  .. cols(\n  ..   PassengerId = col_double(),\n  ..   Survived = col_double(),\n  ..   Pclass = col_double(),\n  ..   Name = col_character(),\n  ..   Sex = col_character(),\n  ..   Age = col_double(),\n  ..   SibSp = col_double(),\n  ..   Parch = col_double(),\n  ..   Ticket = col_character(),\n  ..   Fare = col_double(),\n  ..   Cabin = col_character(),\n  ..   Embarked = col_character()\n  .. )\n - attr(*, \"problems\")=<externalptr> \n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nstr(test)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nspec_tbl_df [418 √ó 11] (S3: spec_tbl_df/tbl_df/tbl/data.frame)\n $ PassengerId: num [1:418] 892 893 894 895 896 897 898 899 900 901 ...\n $ Pclass     : num [1:418] 3 3 2 3 3 3 3 2 3 3 ...\n $ Name       : chr [1:418] \"Kelly, Mr. James\" \"Wilkes, Mrs. James (Ellen Needs)\" \"Myles, Mr. Thomas Francis\" \"Wirz, Mr. Albert\" ...\n $ Sex        : chr [1:418] \"male\" \"female\" \"male\" \"male\" ...\n $ Age        : num [1:418] 34.5 47 62 27 22 14 30 26 18 21 ...\n $ SibSp      : num [1:418] 0 1 0 0 1 0 0 1 0 2 ...\n $ Parch      : num [1:418] 0 0 0 0 1 0 0 1 0 0 ...\n $ Ticket     : chr [1:418] \"330911\" \"363272\" \"240276\" \"315154\" ...\n $ Fare       : num [1:418] 7.83 7 9.69 8.66 12.29 ...\n $ Cabin      : chr [1:418] NA NA NA NA ...\n $ Embarked   : chr [1:418] \"Q\" \"S\" \"Q\" \"S\" ...\n - attr(*, \"spec\")=\n  .. cols(\n  ..   PassengerId = col_double(),\n  ..   Pclass = col_double(),\n  ..   Name = col_character(),\n  ..   Sex = col_character(),\n  ..   Age = col_double(),\n  ..   SibSp = col_double(),\n  ..   Parch = col_double(),\n  ..   Ticket = col_character(),\n  ..   Fare = col_double(),\n  ..   Cabin = col_character(),\n  ..   Embarked = col_character()\n  .. )\n - attr(*, \"problems\")=<externalptr> \n```\n:::\n:::\n\n\nPuedes notar que los 2 archivos son similares en su composici√≥n, excepto que `train` tiene la variable `Survived` y `test` no. Adem√°s, `train` contiene casi el doble de registros (filas) que `test`.\n\nEn Kaggle se explican qu√© significan los variables (diccionario) y ponen algunas notas. Estos datos son importantes para comprender el dataset y pensar en c√≥mo ajustarlos si es necesario. En todo proyecto de datos es muy relevante entender qu√© representan los datos y cu√°les son sus posibilidades.\n\n**Diccionario:**\n\n![](dictionary.png){fig-align=\"center\" width=\"800\"}\n\n**Notas:**\n\n![](dictionary2.png){fig-align=\"center\" width=\"800\"}\n\nRevisemos las primeras filas de los datos:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nhead(train)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 6 √ó 12\n  PassengerId Survived Pclass Name    Sex     Age SibSp Parch Ticket  Fare Cabin\n        <dbl>    <dbl>  <dbl> <chr>   <chr> <dbl> <dbl> <dbl> <chr>  <dbl> <chr>\n1           1        0      3 Braund‚Ä¶ male     22     1     0 A/5 2‚Ä¶  7.25 <NA> \n2           2        1      1 Cuming‚Ä¶ fema‚Ä¶    38     1     0 PC 17‚Ä¶ 71.3  C85  \n3           3        1      3 Heikki‚Ä¶ fema‚Ä¶    26     0     0 STON/‚Ä¶  7.92 <NA> \n4           4        1      1 Futrel‚Ä¶ fema‚Ä¶    35     1     0 113803 53.1  C123 \n5           5        0      3 Allen,‚Ä¶ male     35     0     0 373450  8.05 <NA> \n6           6        0      3 Moran,‚Ä¶ male     NA     0     0 330877  8.46 <NA> \n# ‚Ä¶ with 1 more variable: Embarked <chr>\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nhead(test)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 6 √ó 11\n  PassengerId Pclass Name     Sex     Age SibSp Parch Ticket  Fare Cabin Embar‚Ä¶¬π\n        <dbl>  <dbl> <chr>    <chr> <dbl> <dbl> <dbl> <chr>  <dbl> <chr> <chr>  \n1         892      3 Kelly, ‚Ä¶ male   34.5     0     0 330911  7.83 <NA>  Q      \n2         893      3 Wilkes,‚Ä¶ fema‚Ä¶  47       1     0 363272  7    <NA>  S      \n3         894      2 Myles, ‚Ä¶ male   62       0     0 240276  9.69 <NA>  Q      \n4         895      3 Wirz, M‚Ä¶ male   27       0     0 315154  8.66 <NA>  S      \n5         896      3 Hirvone‚Ä¶ fema‚Ä¶  22       1     1 31012‚Ä¶ 12.3  <NA>  S      \n6         897      3 Svensso‚Ä¶ male   14       0     0 7538    9.22 <NA>  S      \n# ‚Ä¶ with abbreviated variable name ¬π‚ÄãEmbarked\n```\n:::\n:::\n\n\nVeamos m√°s cosas de los datos. Usaremos la librer√≠a `skimr` y su funci√≥n `skim` para explorar los datos y generar algunas primeras estad√≠sticas de las variables.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nskim(train)\n```\n\n::: {.cell-output-display}\nTable: Data summary\n\n|                         |      |\n|:------------------------|:-----|\n|Name                     |train |\n|Number of rows           |891   |\n|Number of columns        |12    |\n|_______________________  |      |\n|Column type frequency:   |      |\n|character                |5     |\n|numeric                  |7     |\n|________________________ |      |\n|Group variables          |None  |\n\n\n**Variable type: character**\n\n|skim_variable | n_missing| complete_rate| min| max| empty| n_unique| whitespace|\n|:-------------|---------:|-------------:|---:|---:|-----:|--------:|----------:|\n|Name          |         0|          1.00|  12|  82|     0|      891|          0|\n|Sex           |         0|          1.00|   4|   6|     0|        2|          0|\n|Ticket        |         0|          1.00|   3|  18|     0|      681|          0|\n|Cabin         |       687|          0.23|   1|  15|     0|      147|          0|\n|Embarked      |         2|          1.00|   1|   1|     0|        3|          0|\n\n\n**Variable type: numeric**\n\n|skim_variable | n_missing| complete_rate|   mean|     sd|   p0|    p25|    p50|   p75|   p100|hist  |\n|:-------------|---------:|-------------:|------:|------:|----:|------:|------:|-----:|------:|:-----|\n|PassengerId   |         0|           1.0| 446.00| 257.35| 1.00| 223.50| 446.00| 668.5| 891.00|‚ñá‚ñá‚ñá‚ñá‚ñá |\n|Survived      |         0|           1.0|   0.38|   0.49| 0.00|   0.00|   0.00|   1.0|   1.00|‚ñá‚ñÅ‚ñÅ‚ñÅ‚ñÖ |\n|Pclass        |         0|           1.0|   2.31|   0.84| 1.00|   2.00|   3.00|   3.0|   3.00|‚ñÉ‚ñÅ‚ñÉ‚ñÅ‚ñá |\n|Age           |       177|           0.8|  29.70|  14.53| 0.42|  20.12|  28.00|  38.0|  80.00|‚ñÇ‚ñá‚ñÖ‚ñÇ‚ñÅ |\n|SibSp         |         0|           1.0|   0.52|   1.10| 0.00|   0.00|   0.00|   1.0|   8.00|‚ñá‚ñÅ‚ñÅ‚ñÅ‚ñÅ |\n|Parch         |         0|           1.0|   0.38|   0.81| 0.00|   0.00|   0.00|   0.0|   6.00|‚ñá‚ñÅ‚ñÅ‚ñÅ‚ñÅ |\n|Fare          |         0|           1.0|  32.20|  49.69| 0.00|   7.91|  14.45|  31.0| 512.33|‚ñá‚ñÅ‚ñÅ‚ñÅ‚ñÅ |\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nskim(test)\n```\n\n::: {.cell-output-display}\nTable: Data summary\n\n|                         |     |\n|:------------------------|:----|\n|Name                     |test |\n|Number of rows           |418  |\n|Number of columns        |11   |\n|_______________________  |     |\n|Column type frequency:   |     |\n|character                |5    |\n|numeric                  |6    |\n|________________________ |     |\n|Group variables          |None |\n\n\n**Variable type: character**\n\n|skim_variable | n_missing| complete_rate| min| max| empty| n_unique| whitespace|\n|:-------------|---------:|-------------:|---:|---:|-----:|--------:|----------:|\n|Name          |         0|          1.00|  13|  63|     0|      418|          0|\n|Sex           |         0|          1.00|   4|   6|     0|        2|          0|\n|Ticket        |         0|          1.00|   3|  18|     0|      363|          0|\n|Cabin         |       327|          0.22|   1|  15|     0|       76|          0|\n|Embarked      |         0|          1.00|   1|   1|     0|        3|          0|\n\n\n**Variable type: numeric**\n\n|skim_variable | n_missing| complete_rate|    mean|     sd|     p0|    p25|     p50|     p75|    p100|hist  |\n|:-------------|---------:|-------------:|-------:|------:|------:|------:|-------:|-------:|-------:|:-----|\n|PassengerId   |         0|          1.00| 1100.50| 120.81| 892.00| 996.25| 1100.50| 1204.75| 1309.00|‚ñá‚ñá‚ñá‚ñá‚ñá |\n|Pclass        |         0|          1.00|    2.27|   0.84|   1.00|   1.00|    3.00|    3.00|    3.00|‚ñÉ‚ñÅ‚ñÉ‚ñÅ‚ñá |\n|Age           |        86|          0.79|   30.27|  14.18|   0.17|  21.00|   27.00|   39.00|   76.00|‚ñÇ‚ñá‚ñÉ‚ñÇ‚ñÅ |\n|SibSp         |         0|          1.00|    0.45|   0.90|   0.00|   0.00|    0.00|    1.00|    8.00|‚ñá‚ñÅ‚ñÅ‚ñÅ‚ñÅ |\n|Parch         |         0|          1.00|    0.39|   0.98|   0.00|   0.00|    0.00|    0.00|    9.00|‚ñá‚ñÅ‚ñÅ‚ñÅ‚ñÅ |\n|Fare          |         1|          1.00|   35.63|  55.91|   0.00|   7.90|   14.45|   31.50|  512.33|‚ñá‚ñÅ‚ñÅ‚ñÅ‚ñÅ |\n:::\n:::\n\n\nCon el an√°lisis anterior, nos damos cuenta que hay datos faltantes (missing data) o NA¬¥s. En particular, en la variable `age`, que es la que tiene una gran cantidad de NA¬¥s. Este dato es relevante, pues podr√≠a afectar bastante el algoritmo. Trabajar los datos faltantes es un √°rea muy importante, por lo que no podemos dejar pasar este hecho. Ya abordaremos esto m√°s adelante.\n\nPor ahora, dado que ambos set de datos son muy similares, los voy a unir para dejarlos en solo 1 objeto. Esto me ser√° √∫til para el an√°lisis y feature engineering de todo el conjunto. Luego lo dividir√© nuevamente para efectos del algoritmo.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nall_data <- bind_rows(list(\"train\" = train, \"test\" = test), .id = \"id\")\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nglimpse(all_data)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nRows: 1,309\nColumns: 13\n$ id          <chr> \"train\", \"train\", \"train\", \"train\", \"train\", \"train\", \"tra‚Ä¶\n$ PassengerId <dbl> 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17,‚Ä¶\n$ Survived    <dbl> 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1‚Ä¶\n$ Pclass      <dbl> 3, 1, 3, 1, 3, 3, 1, 3, 3, 2, 3, 1, 3, 3, 3, 2, 3, 2, 3, 3‚Ä¶\n$ Name        <chr> \"Braund, Mr. Owen Harris\", \"Cumings, Mrs. John Bradley (Fl‚Ä¶\n$ Sex         <chr> \"male\", \"female\", \"female\", \"female\", \"male\", \"male\", \"mal‚Ä¶\n$ Age         <dbl> 22, 38, 26, 35, 35, NA, 54, 2, 27, 14, 4, 58, 20, 39, 14, ‚Ä¶\n$ SibSp       <dbl> 1, 1, 0, 1, 0, 0, 0, 3, 0, 1, 1, 0, 0, 1, 0, 0, 4, 0, 1, 0‚Ä¶\n$ Parch       <dbl> 0, 0, 0, 0, 0, 0, 0, 1, 2, 0, 1, 0, 0, 5, 0, 0, 1, 0, 0, 0‚Ä¶\n$ Ticket      <chr> \"A/5 21171\", \"PC 17599\", \"STON/O2. 3101282\", \"113803\", \"37‚Ä¶\n$ Fare        <dbl> 7.2500, 71.2833, 7.9250, 53.1000, 8.0500, 8.4583, 51.8625,‚Ä¶\n$ Cabin       <chr> NA, \"C85\", NA, \"C123\", NA, NA, \"E46\", NA, NA, NA, \"G6\", \"C‚Ä¶\n$ Embarked    <chr> \"S\", \"C\", \"S\", \"S\", \"S\", \"Q\", \"S\", \"S\", \"S\", \"C\", \"S\", \"S\"‚Ä¶\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nskim(all_data)\n```\n\n::: {.cell-output-display}\nTable: Data summary\n\n|                         |         |\n|:------------------------|:--------|\n|Name                     |all_data |\n|Number of rows           |1309     |\n|Number of columns        |13       |\n|_______________________  |         |\n|Column type frequency:   |         |\n|character                |6        |\n|numeric                  |7        |\n|________________________ |         |\n|Group variables          |None     |\n\n\n**Variable type: character**\n\n|skim_variable | n_missing| complete_rate| min| max| empty| n_unique| whitespace|\n|:-------------|---------:|-------------:|---:|---:|-----:|--------:|----------:|\n|id            |         0|          1.00|   4|   5|     0|        2|          0|\n|Name          |         0|          1.00|  12|  82|     0|     1307|          0|\n|Sex           |         0|          1.00|   4|   6|     0|        2|          0|\n|Ticket        |         0|          1.00|   3|  18|     0|      929|          0|\n|Cabin         |      1014|          0.23|   1|  15|     0|      186|          0|\n|Embarked      |         2|          1.00|   1|   1|     0|        3|          0|\n\n\n**Variable type: numeric**\n\n|skim_variable | n_missing| complete_rate|   mean|     sd|   p0|   p25|    p50|    p75|    p100|hist  |\n|:-------------|---------:|-------------:|------:|------:|----:|-----:|------:|------:|-------:|:-----|\n|PassengerId   |         0|          1.00| 655.00| 378.02| 1.00| 328.0| 655.00| 982.00| 1309.00|‚ñá‚ñá‚ñá‚ñá‚ñá |\n|Survived      |       418|          0.68|   0.38|   0.49| 0.00|   0.0|   0.00|   1.00|    1.00|‚ñá‚ñÅ‚ñÅ‚ñÅ‚ñÖ |\n|Pclass        |         0|          1.00|   2.29|   0.84| 1.00|   2.0|   3.00|   3.00|    3.00|‚ñÉ‚ñÅ‚ñÉ‚ñÅ‚ñá |\n|Age           |       263|          0.80|  29.88|  14.41| 0.17|  21.0|  28.00|  39.00|   80.00|‚ñÇ‚ñá‚ñÖ‚ñÇ‚ñÅ |\n|SibSp         |         0|          1.00|   0.50|   1.04| 0.00|   0.0|   0.00|   1.00|    8.00|‚ñá‚ñÅ‚ñÅ‚ñÅ‚ñÅ |\n|Parch         |         0|          1.00|   0.39|   0.87| 0.00|   0.0|   0.00|   0.00|    9.00|‚ñá‚ñÅ‚ñÅ‚ñÅ‚ñÅ |\n|Fare          |         1|          1.00|  33.30|  51.76| 0.00|   7.9|  14.45|  31.27|  512.33|‚ñá‚ñÅ‚ñÅ‚ñÅ‚ñÅ |\n:::\n:::\n\n\nF√≠jate que hice algo extra al unir los datos. Inclu√≠ una nueva variable `id` en donde se se√±ala si los registros son de `test` o `train`.\n\nVeamos cu√°ntas personas sobrevivieron (de los datos, que son una muestra de todos los pasajeros del Titanic, que eran m√°s de 2200 en total).\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntable(all_data$Survived)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n  0   1 \n549 342 \n```\n:::\n:::\n\n\nLos datos clasificados como 0 no sobrevivieron y los 1; si. Podemos ver la misma tabla, pero en proporci√≥n.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nround(prop.table(table(all_data$Survived)), 4) * 100\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n    0     1 \n61.62 38.38 \n```\n:::\n:::\n\n\nVeamos si el sexo nos da alguna informaci√≥n importante.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntable(all_data$Sex)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nfemale   male \n   466    843 \n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ntable(all_data$Sex, all_data$Survived)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n        \n           0   1\n  female  81 233\n  male   468 109\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nall_data |> \n  filter(!is.na(Survived)) |> \n  mutate(Survived = factor(Survived)) |> \n  group_by(Sex, Survived) |> \n  summarise(n = n()) |> \n  ggplot(aes(Survived, n, fill = Survived)) +\n  geom_col(show.legend = FALSE)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-16-1.png){width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nall_data |> \n  filter(!is.na(Survived)) |> \n  mutate(Survived = factor(Survived)) |> \n  group_by(Sex, Survived) |> \n  summarise(n = n()) |> \n  ggplot(aes(Survived, n, fill = Sex)) +\n  geom_col(position = \"dodge\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-17-1.png){width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nall_data |> \n  filter(Survived == 1) |> # Si sobreviven\n  tabyl(Sex)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n    Sex   n   percent\n female 233 0.6812865\n   male 109 0.3187135\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nall_data |> \n  filter(Survived == 0) |> # No sobreviven\n  tabyl(Sex)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n    Sex   n  percent\n female  81 0.147541\n   male 468 0.852459\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nall_data |> \n  filter(!is.na(Age)) |> \n  group_by(Sex) |> \n  summarise(mean = mean(Age))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 2 √ó 2\n  Sex     mean\n  <chr>  <dbl>\n1 female  28.7\n2 male    30.6\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nall_data |> \n  filter(!is.na(Age), !is.na(Survived)) |> \n  group_by(Sex, Survived) |> \n  summarise(mean = mean(Age))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 4 √ó 3\n# Groups:   Sex [2]\n  Sex    Survived  mean\n  <chr>     <dbl> <dbl>\n1 female        0  25.0\n2 female        1  28.8\n3 male          0  31.6\n4 male          1  27.3\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nall_data |> \n  filter(!is.na(Age), !is.na(Survived)) |> \n  ggplot(aes(Age, Sex)) +\n  geom_boxplot()\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-22-1.png){width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nall_data |> \n  filter(!is.na(Age), !is.na(Survived)) |> \n  ggplot(aes(Age, factor(Survived), fill = Sex)) +\n  geom_boxplot()\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-23-1.png){width=672}\n:::\n:::\n\n\nRevisemos algo de los datos las clases:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nall_data |>\n  ggplot(aes(factor(Pclass), fill = factor(Pclass))) +\n  geom_bar() +\n  scale_fill_discrete(name = \"Class\", labels = c(\"1st\", \"2nd\", \"3rd\"))\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-24-1.png){width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nall_data |> \n  filter(!is.na(Survived)) |> \n  ggplot(aes(factor(Pclass), fill = factor(Survived))) +\n  geom_bar(position = \"dodge\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-25-1.png){width=672}\n:::\n:::\n\n\nAc√° aparece algo interesate. Las personas de tercera clase, en su mayor√≠a, no sobrevivieron. A diferencia de los de primera clase, donde hubo m√°s sobrevivientes que fallecidos (proporcionalmente).\n\nRevisemos el costo de los tickets.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nall_data |> \n  ggplot(aes(Fare)) + \n  geom_histogram(bins = 30)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-26-1.png){width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nall_data |> \n  filter(!is.na(Survived)) |> \n  ggplot(aes(Fare, fill = factor(Survived))) + \n  geom_density(alpha = 0.6)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-27-1.png){width=672}\n:::\n:::\n\n\nEstos datos se√±alan en el mismo sentido de que las personas de clases m√°s altas y con tickets m√°s caros, sobrevivieron m√°s.\n\n## Manejo de NA¬¥s\n\nVolvamos un poco a revisar los datos faltantes (NA¬¥s).\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(VIM::aggr(all_data))\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-28-1.png){width=672}\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n\n Missings per variable: \n    Variable Count\n          id     0\n PassengerId     0\n    Survived   418\n      Pclass     0\n        Name     0\n         Sex     0\n         Age   263\n       SibSp     0\n       Parch     0\n      Ticket     0\n        Fare     1\n       Cabin  1014\n    Embarked     2\n\n Missings in combinations of variables: \n              Combinations Count     Percent\n 0:0:0:0:0:0:0:0:0:0:0:0:0   183 13.98013751\n 0:0:0:0:0:0:0:0:0:0:0:0:1     2  0.15278839\n 0:0:0:0:0:0:0:0:0:0:0:1:0   529 40.41252865\n 0:0:0:0:0:0:1:0:0:0:0:0:0    19  1.45148969\n 0:0:0:0:0:0:1:0:0:0:0:1:0   158 12.07028266\n 0:0:1:0:0:0:0:0:0:0:0:0:0    87  6.64629488\n 0:0:1:0:0:0:0:0:0:0:0:1:0   244 18.64018335\n 0:0:1:0:0:0:0:0:0:0:1:1:0     1  0.07639419\n 0:0:1:0:0:0:1:0:0:0:0:0:0     4  0.30557678\n 0:0:1:0:0:0:1:0:0:0:0:1:0    82  6.26432391\n```\n:::\n:::\n\n\nYa lo hab√≠amos visto antes. Los registros tienen bastantes datos faltantes (missing data). Estos datos se presentan especialmente en las variables Age y Cabin. Survived tienen muchos NA¬¥s, pues unimos los regitros al inicio por temas metodol√≥gicos, pero ac√° no tiene relevancia.\n\nAc√° tenemos un tema muy relevante en proyectos de datos y es el manejar los datos faltantes y tomar decisiones al respecto. En este art√≠culo no profundizar√© demasiado en este tema, pues se escapa del objetivo de ser m√°s demostrativo. Pero es un √°rea relevante de estudio que posiblemente aborde en otro art√≠culo m√°s adelante.\n\nEn este caso, tomar√© 2 decisiones. Con respecto a la variable `Cabin` no la considar√© para el desarrollo y entrenamiento del algoritmo, pues tiene un porcentaje muy alto de p√©rdida y adaptarlo a algo √∫til parece ser poco factible, en un primer momento. Con la variable `Age` pasa algo distinto, pues parece tener m√°s relevancia en la probabilidad de sobrevivir.\n\nEntonces, haremos algo para completar esos datos. Una alternativa es imputar datos. Es decir, asumir la edad en base a algunos supuestos. Existen varias metodolog√≠as como calcular la media o mediana de todos los datos de edad y pon√©rselo a los NA¬¥s. Otro m√©todo es aplicar algoritmos de machine learning para predecir ese dato e imputarlo.\n\nVeamos algunos datos de la edad:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(all_data$Age)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n   0.17   21.00   28.00   29.88   39.00   80.00     263 \n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nall_data |> \n  ggplot(aes(Age)) +\n  geom_histogram()\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-30-1.png){width=672}\n:::\n:::\n\n\nDebido a que la distribuci√≥n de la edad se acerca a una distribuci√≥n normal (histograma) y a que la media y mediana se a cercan bastante, podriamos usar el primer enfoque de amputaci√≥n, que es usar la mediana para completar los datos faltantes. Te recuerdo que este es solo un enfoque posible. En proyectos reales, lo que se hace es probar muchos modelos y m√©todos y analizar su desempe√±o. Insisto, para efectos de este art√≠culo, usar√© este acercamiento.\n\nVerifiquemos las normalidad de los datos de edad. Primero, realizaremos un gr√°fico QQ plot, el cual consiste en comparar los cuantiles de la distribuci√≥n observada con los cuantiles te√≥ricos de una distribuci√≥n normal con la misma media y desviaci√≥n est√°ndar que los datos. En la medida que los datos se ajusten a la l√≠nea proyectada, m√°s se aproximan a una distribuci√≥n normal.\n\nAplicar√© la funci√≥n `lillie.test` de la librer√≠a `nortest` para usar el test de *Lilliefors*. Este test nace con la idea de resolver uno de los problemas del test *Kolmogorov-Smirnov* que es al no conocer la media y la varianza poblacional, tiene poca potencia estad√≠stica. El test *Lilliefors* asume que la media y varianza son desconocidas y es especialmente √∫til en muestras grandes.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nqqnorm(all_data$Age, pch = 19, col = \"gray50\")\nqqline(all_data$Age)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-31-1.png){width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nnortest::lillie.test(all_data$Age)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n\tLilliefors (Kolmogorov-Smirnov) normality test\n\ndata:  all_data$Age\nD = 0.078928, p-value < 2.2e-16\n```\n:::\n:::\n\n\nVemos que tanto QQ plot y el test *Lilliefors* apuntan a que los datos tienen una distribuci√≥n normal.\n\nVolvamos a ver los datos, separados por sexo:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nall_data |> \n  ggplot(aes(Age)) +\n  geom_histogram() +\n  facet_grid(~ Sex)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-33-1.png){width=672}\n:::\n:::\n\n\n**Imputaci√≥n de datos.**\n\nPodr√≠amos imputar los datos faltantes usando las siguientes l√≠neas de c√≥digo.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nall_data$Age[is.na(all_data$Age)] <- median(all_data$Age, na.rm = TRUE)\nall_data$Fare[is.na(all_data$Fare)] <- mean(all_data$Fare, na.rm = TRUE)\nall_data$Embarked[is.na(all_data$Embarked)] <- mode(all_data$Embarked)\n```\n:::\n\n\n## Feature Engineering\n\nEl dataset tiene algunas variables que son de dudosa ayuda para predecir y generar el algoritmo, al menos, a priori. Casi por sentido com√∫n. A√∫n cuando tomar decisiones por sentido com√∫n podr√≠a no ser la mejor alternativa, muchas veces nos equivocamos o traspasamos nuestros sesgos a los an√°lisis. La idea siempre es tratar de dejar que \"los datos hablen\". En este caso, como es un art√≠culo m√°s referencial, no profundizar√© en la generaci√≥n y manipulaci√≥n de las variables para no confundir tanto, pero si har√© un par de cosas que me parecen interesantes y que muestra las posibilidades del feature engineering.\n\nLas variables `Ticket`, `Name` y `Cabin` parecen ser datos poco informativos para saber si alguien sobrevivi√≥ o no al accidente. Sin embargo, no ser√≠a tan as√≠. Es posible que la cabina tenga alguna relaci√≥n con la sobrevivencia, quiz√°s por cercan√≠a a un pasillo o algo as√≠. El n¬∞ de ticket tambi√©n quiz√°s se asocia a algo, no lo sabemos, es necesario investigar. Que como ya te mencion√©, no lo haremos en esta ocasi√≥n jaja üòÑ\n\nPero si revisemos un poco m√°s el nombre de los pasajeros. Si revisas, el registro contiene el nombre de la personas y su t√≠tulo (Mr, Miss, Dr, Col, etc). Ese dato podr√≠a ser √∫til para la predicci√≥n. Quiz√°s hab√≠a alg√∫n tipo de jerarqu√≠a que favoreciera que se salvaran. Tambi√©n el apellido de las personas podr√≠a ser √∫til, m√°s que nada por temas de agrupar a familias.\n\nEntonces, crearemos 2 nuevas variables: `Title` y `Surname`. Para ello, usaremos la funci√≥n `mutate` y nos ayudaremos de las REGEX (expresiones regulares) para extraer los datos desde el nombre.\n\nConsejo: aprende REGEX. Es una gran herramienta en el an√°lisis de datos y aplica no solo para programaci√≥n en R, sino que para cualquier otro. Incluso se usa en navegadores web, email y diversas aplicaciones.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nall_data <- all_data |>\n  mutate(\n    Title = str_extract(Name, \"(?<=,[:space:])(.*?)[.]\"),\n    Surname = str_extract(Name, \".*(?=[,])\")\n  )\n```\n:::\n\n\nVeamos como nos quedaron los datos...\n\n\n::: {.cell}\n\n```{.r .cell-code}\nall_data |> \n  head()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 6 √ó 15\n  id    Passen‚Ä¶¬π Survi‚Ä¶¬≤ Pclass Name  Sex     Age SibSp Parch Ticket  Fare Cabin\n  <chr>    <dbl>   <dbl>  <dbl> <chr> <chr> <dbl> <dbl> <dbl> <chr>  <dbl> <chr>\n1 train        1       0      3 Brau‚Ä¶ male     22     1     0 A/5 2‚Ä¶  7.25 <NA> \n2 train        2       1      1 Cumi‚Ä¶ fema‚Ä¶    38     1     0 PC 17‚Ä¶ 71.3  C85  \n3 train        3       1      3 Heik‚Ä¶ fema‚Ä¶    26     0     0 STON/‚Ä¶  7.92 <NA> \n4 train        4       1      1 Futr‚Ä¶ fema‚Ä¶    35     1     0 113803 53.1  C123 \n5 train        5       0      3 Alle‚Ä¶ male     35     0     0 373450  8.05 <NA> \n6 train        6       0      3 Mora‚Ä¶ male     28     0     0 330877  8.46 <NA> \n# ‚Ä¶ with 3 more variables: Embarked <chr>, Title <chr>, Surname <chr>, and\n#   abbreviated variable names ¬π‚ÄãPassengerId, ¬≤‚ÄãSurvived\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nunique(all_data$Title)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n [1] \"Mr.\"           \"Mrs.\"          \"Miss.\"         \"Master.\"      \n [5] \"Don.\"          \"Rev.\"          \"Dr.\"           \"Mme.\"         \n [9] \"Ms.\"           \"Major.\"        \"Lady.\"         \"Sir.\"         \n[13] \"Mlle.\"         \"Col.\"          \"Capt.\"         \"the Countess.\"\n[17] \"Jonkheer.\"     \"Dona.\"        \n```\n:::\n:::\n\n\nPodemos ampliar la variable Sex en base a los t√≠tulos. En especial, agregar algo que identifique a los ni√±os. Si te fijaste, los sobrevivientes son en general m√°s j√≥venes. Adem√°s, existe esta frase t√≠pica de salvar a los ni√±os y las mujeres primero. No tengo claro si eso es tan real, pero podr√≠a serlo. As√≠ que agreguemos la posibilidad.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nMan <- c(\n  \"Mr.\", \"Sir.\", \"Don.\", \"Rev.\", \"Major.\",\n  \"Col.\", \"Capt.\", \"Jonkheer.\",\n  \"Dr.\", \"Nobel.\"\n)\nFemale <- c(\"Mrs.\", \"Miss.\", \"Mme.\", \"Ms.\", \"Lady.\", \"Mlle.\", \"the Countess.\", \"Dona.\")\nBoy <- c(\"Master.\")\n\nall_data <- all_data |>\n  rowwise() |>\n  mutate(\n    Sex =\n      case_when(\n        (Title %in% Man) ~ \"Man\",\n        (Title %in% Female) ~ \"Female\",\n        (Title %in% Boy) ~ \"Boy\",\n        TRUE ~ Title\n      )\n  )\n```\n:::\n\n\nRevisemos c√≥mo quedan los grupos con esta nueva clasificaci√≥n:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nall_data |> \n  filter(!is.na(Survived)) |> \n  tabyl(Sex, Survived)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n    Sex   0   1\n    Boy  17  23\n Female  81 232\n    Man 451  87\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nall_data |> \n  filter(!is.na(Survived)) |> \n  mutate(Survived = factor(Survived)) |> \n  group_by(Sex, Survived) |> \n  summarise(n = n()) |> \n  ggplot(aes(Sex, n, fill = Survived)) +\n  geom_col(position = \"dodge\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-40-1.png){width=672}\n:::\n:::\n\n\nAhora vamos a crear una nueva variable, que es el tama√±o de la familia. Para eso, sumaremos las variables `SibSp` (n¬∞ de hermanos y esposas/os) y `Parch` (n¬∞ de hijos y padres). A la suma le agregamos 1, pues agregamos al mismo pasajero al grupo familiar.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nall_data <- all_data |>\n  mutate(\n    Family_size = as.numeric(SibSp) + as.numeric(Parch) + 1,\n    Family_type = factor(ifelse(Family_size == 1, \"Single\",\n      ifelse(Family_size <= 3, \"Small\", \"Large\")\n    ))\n  ) |> \n  unnest(cols = c())\n```\n:::\n\n\nRevisemos nuevamente los datos faltantes. Esto es importante, pues muchos modelos de machine learning no soportan variables con NA¬¥s o pueden alterar las predicciones.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(VIM::aggr(all_data))\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-42-1.png){width=672}\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n\n Missings per variable: \n    Variable Count\n          id     0\n PassengerId     0\n    Survived   418\n      Pclass     0\n        Name     0\n         Sex     0\n         Age     0\n       SibSp     0\n       Parch     0\n      Ticket     0\n        Fare     0\n       Cabin  1014\n    Embarked     0\n       Title     0\n     Surname     0\n Family_size     0\n Family_type     0\n\n Missings in combinations of variables: \n                      Combinations Count   Percent\n 0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0   204 15.584416\n 0:0:0:0:0:0:0:0:0:0:0:1:0:0:0:0:0   687 52.482811\n 0:0:1:0:0:0:0:0:0:0:0:0:0:0:0:0:0    91  6.951872\n 0:0:1:0:0:0:0:0:0:0:0:1:0:0:0:0:0   327 24.980901\n```\n:::\n:::\n\n\nPor ahora, voy a dejar el an√°lisis exploratorio hasta ac√°. Podr√≠amos estar mucho tiempo m√°s revisando y d√°ndole vueltas al tema, pero creo que con lo que hemos visto ya te has hecho una buena idea de qu√© va √©sto.\n\nHabitualmente, los proyecto de Ciencia de Datos destinan cerca del 80% del tiempo en este tipo de actividades.\n\n## Modelamiento ML\n\nPara el modelamiento del algoritmo de machine learning (ML) usaremos la librer√≠a `H2O`.\n\n[H2O](https://h2o.ai/) funciona bastante distinto que otras librer√≠as, pues lo que hace es levantar un servidor y nos conoctamos a √©l, y es en ese servidor donde se ejecutan los algoritmos. H2O es una plataforma de autoML o de modelos de machine learning pre-entrenados. En palabras simples, la configuraci√≥n de los algoritmos viene ya lista \"de f√°brica\". En palabras m√°s complejas, los modelos de autoML ajustan el tuneo de los hiperpar√°metros de forma autom√°tica o ya cuentan con valores predefinidos.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Separamos el dataset para los datos de entrenamiento\ntitanic_train <- all_data |>\n  filter(id == \"train\") |>\n  select(-c(id, PassengerId, Name, Ticket, Cabin))\n```\n:::\n\n\nAnalicemos los tipos de datos de `titanic_train`:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nstr(titanic_train)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ntibble [891 √ó 12] (S3: tbl_df/tbl/data.frame)\n $ Survived   : num [1:891] 0 1 1 1 0 0 0 0 1 1 ...\n $ Pclass     : num [1:891] 3 1 3 1 3 3 1 3 3 2 ...\n $ Sex        : chr [1:891] \"Man\" \"Female\" \"Female\" \"Female\" ...\n $ Age        : num [1:891] 22 38 26 35 35 28 54 2 27 14 ...\n $ SibSp      : num [1:891] 1 1 0 1 0 0 0 3 0 1 ...\n $ Parch      : num [1:891] 0 0 0 0 0 0 0 1 2 0 ...\n $ Fare       : num [1:891] 7.25 71.28 7.92 53.1 8.05 ...\n $ Embarked   : chr [1:891] \"S\" \"C\" \"S\" \"S\" ...\n $ Title      : chr [1:891] \"Mr.\" \"Mrs.\" \"Miss.\" \"Mrs.\" ...\n $ Surname    : chr [1:891] \"Braund\" \"Cumings\" \"Heikkinen\" \"Futrelle\" ...\n $ Family_size: num [1:891] 2 2 1 2 1 1 1 5 3 2 ...\n $ Family_type: Factor w/ 3 levels \"Small\",\"Single\",..: 1 1 2 1 2 2 2 3 1 1 ...\n```\n:::\n:::\n\n\nCambiaremos algunos tipos de datos por algo m√°s √∫til.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntitanic_train <- titanic_train |>\n  mutate(\n    Survived = factor(Survived),\n    Pclass = factor(Pclass),\n    Sex = factor(Sex),\n    Embarked = factor(Embarked),\n    Title = factor(Title)\n  )\n```\n:::\n\n\nOk. Tenemos nuestros datos de entrenamiento listos o, al menos, m√°s preparados para ser analizados.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Cargamos la librer√≠a de H2O\nlibrary(h2o)\n\n# Creaci√≥n de un cluster local con todos los cores disponibles\nh2o.init(\n  ip = \"localhost\",\n  # -1 indica que se empleen todos los cores disponibles\n  nthreads = -1,\n  # M√°xima memoria disponible para el cluster\n  max_mem_size = \"3g\"\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nH2O is not running yet, starting it now...\n\nNote:  In case of errors look at the following log files:\n    C:\\Users\\pvill\\AppData\\Local\\Temp\\Rtmp6bs63f\\file6a4072b175ec/h2o_pvill_started_from_r.out\n    C:\\Users\\pvill\\AppData\\Local\\Temp\\Rtmp6bs63f\\file6a4064e47f35/h2o_pvill_started_from_r.err\n\n\nStarting H2O JVM and connecting:  Connection successful!\n\nR is connected to the H2O cluster: \n    H2O cluster uptime:         2 seconds 775 milliseconds \n    H2O cluster timezone:       America/Santiago \n    H2O data parsing timezone:  UTC \n    H2O cluster version:        3.36.1.2 \n    H2O cluster version age:    4 months and 11 days !!! \n    H2O cluster name:           H2O_started_from_R_pvill_qxv170 \n    H2O cluster total nodes:    1 \n    H2O cluster total memory:   2.98 GB \n    H2O cluster total cores:    16 \n    H2O cluster allowed cores:  16 \n    H2O cluster healthy:        TRUE \n    H2O Connection ip:          localhost \n    H2O Connection port:        54321 \n    H2O Connection proxy:       NA \n    H2O Internal Security:      FALSE \n    R Version:                  R version 4.2.1 (2022-06-23 ucrt) \n```\n:::\n\n```{.r .cell-code}\n# Se eliminan los datos del cluster por si ya hab√≠a sido iniciado\nh2o.removeAll()\n\n# Para que no se muestre la barra de progreso\nh2o.no_progress()\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ntitanic_h2o = as.h2o(titanic_train)\n```\n:::\n\n\nPara nuestro caso, aplicaremos autoML o machine learning automatizado.\n\nEn la [documentaci√≥n oficial de H2O](https://docs.h2o.ai/h2o/latest-stable/h2o-docs/automl.html) puedes ver m√°s detalles.\n\nVamos al tema y veamos c√≥mo se podr√≠a implementar una soluci√≥n simple de autoML.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Separaci√≥n de las observaciones en conjunto de entrenamiento y test\nsplits <- h2o.splitFrame(data = titanic_h2o, ratios = 0.8, seed = 123)\ntitanic_train_h2o <- splits[[1]]\ntitanic_test_h2o  <- splits[[2]]\n\n# Se define la variable respuesta y los predictores\nresponse <- \"Survived\"\n\n# Para este modelo se emplean todos los predictores disponibles\npredictors  <- setdiff(h2o.colnames(titanic_h2o), response)\n\n# Ejectutamos 20 modelos de autoML\naml <- h2o.automl(\n  x = predictors, y = response,\n  training_frame = titanic_train_h2o,\n  max_models = 20,\n  seed = 123\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n03:43:13.238: AutoML: XGBoost is not available; skipping it.\n03:43:13.264: _train param, Dropping bad and constant columns: [Surname]\n03:43:14.149: _train param, Dropping bad and constant columns: [Surname]\n03:43:14.848: _train param, Dropping bad and constant columns: [Surname]\n03:43:15.724: _train param, Dropping bad and constant columns: [Surname]\n03:43:16.912: _train param, Dropping bad and constant columns: [Surname]\n03:43:19.725: _train param, Dropping bad and constant columns: [Surname]\n03:43:19.986: _train param, Dropping bad and constant columns: [Surname]\n03:43:20.478: _train param, Dropping bad and constant columns: [Surname]\n03:43:20.747: _train param, Dropping bad and constant columns: [Surname]\n03:46:22.718: _train param, Dropping unused columns: [Surname]\n03:46:23.399: _train param, Dropping unused columns: [Surname]\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# Ver autoML Leaderboard\nlb <- aml@leaderboard\nprint(lb, n = nrow(lb)) \n```\n\n::: {.cell-output .cell-output-stdout}\n```\n                                                 model_id       auc   logloss\n1                           XRT_1_AutoML_1_20221008_34313 0.8709619 0.4280390\n2  StackedEnsemble_BestOfFamily_1_AutoML_1_20221008_34313 0.8689309 0.4153067\n3                           GBM_2_AutoML_1_20221008_34313 0.8684124 0.4212379\n4     StackedEnsemble_AllModels_1_AutoML_1_20221008_34313 0.8656339 0.4143545\n5                           GBM_4_AutoML_1_20221008_34313 0.8644931 0.4242718\n6              GBM_grid_1_AutoML_1_20221008_34313_model_3 0.8636030 0.4252027\n7              GBM_grid_1_AutoML_1_20221008_34313_model_1 0.8635338 0.4239069\n8                           GBM_5_AutoML_1_20221008_34313 0.8622764 0.4245518\n9                           GLM_1_AutoML_1_20221008_34313 0.8607726 0.4308457\n10             GBM_grid_1_AutoML_1_20221008_34313_model_4 0.8603405 0.4310881\n11                          GBM_3_AutoML_1_20221008_34313 0.8595238 0.4300964\n12                          DRF_1_AutoML_1_20221008_34313 0.8591176 0.4954942\n13             GBM_grid_1_AutoML_1_20221008_34313_model_5 0.8572120 0.4396899\n14    DeepLearning_grid_2_AutoML_1_20221008_34313_model_1 0.8543600 0.4787892\n15                 DeepLearning_1_AutoML_1_20221008_34313 0.8499568 0.4481447\n16    DeepLearning_grid_2_AutoML_1_20221008_34313_model_2 0.8491660 0.4693684\n17                          GBM_1_AutoML_1_20221008_34313 0.8477487 0.4579601\n18             GBM_grid_1_AutoML_1_20221008_34313_model_2 0.8461974 0.4603829\n19    DeepLearning_grid_1_AutoML_1_20221008_34313_model_2 0.8450739 0.4765796\n20    DeepLearning_grid_3_AutoML_1_20221008_34313_model_1 0.8389508 0.4731186\n21    DeepLearning_grid_3_AutoML_1_20221008_34313_model_2 0.8379570 0.4686067\n22    DeepLearning_grid_1_AutoML_1_20221008_34313_model_1 0.8351266 0.4805536\n       aucpr mean_per_class_error      rmse       mse\n1  0.8346343            0.1859390 0.3661840 0.1340907\n2  0.8409394            0.1840722 0.3593080 0.1291023\n3  0.8327107            0.1849797 0.3617789 0.1308840\n4  0.8398073            0.1816654 0.3586963 0.1286630\n5  0.8276144            0.1848846 0.3614534 0.1306485\n6  0.8311913            0.1859260 0.3635181 0.1321454\n7  0.8353353            0.1885360 0.3626827 0.1315388\n8  0.8292195            0.1808141 0.3626516 0.1315162\n9  0.8366766            0.2003414 0.3684605 0.1357631\n10 0.8290372            0.1826938 0.3667785 0.1345264\n11 0.8246865            0.1873866 0.3651803 0.1333567\n12 0.8311538            0.1900095 0.3704124 0.1372053\n13 0.8172740            0.1861291 0.3691785 0.1362927\n14 0.8073969            0.2068317 0.3838420 0.1473347\n15 0.8138987            0.1995981 0.3746397 0.1403549\n16 0.8200406            0.2067237 0.3806173 0.1448695\n17 0.8225336            0.2232089 0.3811518 0.1452767\n18 0.8238082            0.2195575 0.3818452 0.1458057\n19 0.8142517            0.2068317 0.3872434 0.1499574\n20 0.8012208            0.2147438 0.3887060 0.1510923\n21 0.8188159            0.2043298 0.3762246 0.1415450\n22 0.8110971            0.2083614 0.3829747 0.1466696\n\n[22 rows x 7 columns] \n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# Podemos revisar el modelo ganador\naml@leader\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nModel Details:\n==============\n\nH2OBinomialModel: drf\nModel ID:  XRT_1_AutoML_1_20221008_34313 \nModel Summary: \n  number_of_trees number_of_internal_trees model_size_in_bytes min_depth\n1              33                       33               39124        13\n  max_depth mean_depth min_leaves max_leaves mean_leaves\n1        20   17.72727         42        137    89.30303\n\n\nH2OBinomialMetrics: drf\n** Reported on training data. **\n** Metrics reported on Out-Of-Bag training samples **\n\nMSE:  0.1491174\nRMSE:  0.3861573\nLogLoss:  0.5119568\nMean Per-Class Error:  0.2038026\nAUC:  0.8463313\nAUCPR:  0.8090878\nGini:  0.6926627\nR^2:  0.3667232\n\nConfusion Matrix (vertical: actual; across: predicted) for F1-optimal threshold:\n         0   1    Error      Rate\n0      346  89 0.204598   =89/435\n1       54 212 0.203008   =54/266\nTotals 400 301 0.203994  =143/701\n\nMaximum Metrics: Maximum metrics at their respective thresholds\n                        metric threshold      value idx\n1                       max f1  0.437500   0.747795 179\n2                       max f2  0.378927   0.795292 202\n3                 max f0point5  0.755055   0.782353  73\n4                 max accuracy  0.651129   0.803138 103\n5                max precision  0.879121   0.963855  37\n6                   max recall  0.026050   1.000000 393\n7              max specificity  1.000000   0.997701   0\n8             max absolute_mcc  0.459596   0.585658 169\n9   max min_per_class_accuracy  0.437500   0.795402 179\n10 max mean_per_class_accuracy  0.459596   0.796413 169\n11                     max tns  1.000000 434.000000   0\n12                     max fns  1.000000 250.000000   0\n13                     max fps  0.000000 435.000000 399\n14                     max tps  0.026050 266.000000 393\n15                     max tnr  1.000000   0.997701   0\n16                     max fnr  1.000000   0.939850   0\n17                     max fpr  0.000000   1.000000 399\n18                     max tpr  0.026050   1.000000 393\n\nGains/Lift Table: Extract with `h2o.gainsLift(<model>, <data>)` or `h2o.gainsLift(<model>, valid=<T/F>, xval=<T/F>)`\n\nH2OBinomialMetrics: drf\n** Reported on cross-validation data. **\n** 5-fold cross-validation on training data (Metrics computed for combined holdout predictions) **\n\nMSE:  0.1340907\nRMSE:  0.366184\nLogLoss:  0.428039\nMean Per-Class Error:  0.185939\nAUC:  0.8709619\nAUCPR:  0.8346343\nGini:  0.7419238\nR^2:  0.4305393\n\nConfusion Matrix (vertical: actual; across: predicted) for F1-optimal threshold:\n         0   1    Error      Rate\n0      355  80 0.183908   =80/435\n1       50 216 0.187970   =50/266\nTotals 405 296 0.185449  =130/701\n\nMaximum Metrics: Maximum metrics at their respective thresholds\n                        metric threshold      value idx\n1                       max f1  0.418827   0.768683 202\n2                       max f2  0.274074   0.821104 257\n3                 max f0point5  0.628670   0.789720 135\n4                 max accuracy  0.497812   0.823110 180\n5                max precision  1.000000   1.000000   0\n6                   max recall  0.046593   1.000000 394\n7              max specificity  1.000000   1.000000   0\n8             max absolute_mcc  0.496049   0.624389 181\n9   max min_per_class_accuracy  0.418827   0.812030 202\n10 max mean_per_class_accuracy  0.418827   0.814061 202\n11                     max tns  1.000000 435.000000   0\n12                     max fns  1.000000 263.000000   0\n13                     max fps  0.023070 435.000000 399\n14                     max tps  0.046593 266.000000 394\n15                     max tnr  1.000000   1.000000   0\n16                     max fnr  1.000000   0.988722   0\n17                     max fpr  0.023070   1.000000 399\n18                     max tpr  0.046593   1.000000 394\n\nGains/Lift Table: Extract with `h2o.gainsLift(<model>, <data>)` or `h2o.gainsLift(<model>, valid=<T/F>, xval=<T/F>)`\nCross-Validation Metrics Summary: \n                             mean       sd cv_1_valid cv_2_valid cv_3_valid\naccuracy                 0.830233 0.029251   0.836879   0.871429   0.792857\nauc                      0.874299 0.027893   0.851602   0.907378   0.841389\nerr                      0.169767 0.029251   0.163121   0.128571   0.207143\nerr_count               23.800000 4.086563  23.000000  18.000000  29.000000\nf0point5                 0.763671 0.029804   0.733083   0.810277   0.743034\nf1                       0.789989 0.020935   0.772277   0.820000   0.768000\nf2                       0.818491 0.014345   0.815900   0.829959   0.794702\nlift_top_group           2.664176 0.313517   3.065218   2.857143   2.372881\nlogloss                  0.432077 0.031004   0.432689   0.391192   0.478483\nmax_per_class_error      0.187373 0.027715   0.168421   0.163265   0.222222\nmcc                      0.650161 0.049185   0.653039   0.720411   0.584946\nmean_per_class_accuracy  0.831079 0.025358   0.839703   0.863422   0.795669\nmean_per_class_error     0.168921 0.025358   0.160297   0.136578   0.204331\nmse                      0.135078 0.011345   0.135005   0.120964   0.152672\npr_auc                   0.834787 0.042016   0.773003   0.882346   0.814953\nprecision                0.747210 0.035625   0.709091   0.803922   0.727273\nr2                       0.422426 0.041388   0.385804   0.468290   0.373851\nrecall                   0.838855 0.015019   0.847826   0.836735   0.813559\nrmse                     0.367275 0.015319   0.367430   0.347799   0.390732\nspecificity              0.823302 0.044511   0.831579   0.890110   0.777778\n                        cv_4_valid cv_5_valid\naccuracy                  0.814286   0.835714\nauc                       0.876458   0.894668\nerr                       0.185714   0.164286\nerr_count                26.000000  23.000000\nf0point5                  0.768072   0.763889\nf1                        0.796875   0.792793\nf2                        0.827922   0.823970\nlift_top_group            2.333333   2.692308\nlogloss                   0.429635   0.428385\nmax_per_class_error       0.212500   0.170455\nmcc                       0.631219   0.661193\nmean_per_class_accuracy   0.818750   0.837850\nmean_per_class_error      0.181250   0.162150\nmse                       0.133763   0.132989\npr_auc                    0.849705   0.853930\nprecision                 0.750000   0.745763\nr2                        0.453803   0.430380\nrecall                    0.850000   0.846154\nrmse                      0.365736   0.364676\nspecificity               0.787500   0.829545\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# Importancia de las variables\nh2o.varimp(aml@leader)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nVariable Importances: \n      variable relative_importance scaled_importance percentage\n1        Title          940.399963          1.000000   0.283786\n2          Sex          586.606567          0.623784   0.177021\n3         Fare          567.343628          0.603300   0.171208\n4          Age          460.132050          0.489294   0.138855\n5       Pclass          328.859924          0.349702   0.099241\n6  Family_size          121.330360          0.129020   0.036614\n7        SibSp           91.768562          0.097585   0.027693\n8  Family_type           87.194229          0.092720   0.026313\n9     Embarked           85.729736          0.091163   0.025871\n10       Parch           44.400452          0.047214   0.013399\n```\n:::\n\n```{.r .cell-code}\nh2o.varimp_plot(aml@leader)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-51-1.png){width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# √Årea bajo la curva AUC para los datos de entrenamiento\nh2o.auc(aml@leader, train = TRUE)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.8463313\n```\n:::\n\n```{.r .cell-code}\nh2o.performance(model = aml@leader, train = TRUE)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nH2OBinomialMetrics: drf\n** Reported on training data. **\n** Metrics reported on Out-Of-Bag training samples **\n\nMSE:  0.1491174\nRMSE:  0.3861573\nLogLoss:  0.5119568\nMean Per-Class Error:  0.2038026\nAUC:  0.8463313\nAUCPR:  0.8090878\nGini:  0.6926627\nR^2:  0.3667232\n\nConfusion Matrix (vertical: actual; across: predicted) for F1-optimal threshold:\n         0   1    Error      Rate\n0      346  89 0.204598   =89/435\n1       54 212 0.203008   =54/266\nTotals 400 301 0.203994  =143/701\n\nMaximum Metrics: Maximum metrics at their respective thresholds\n                        metric threshold      value idx\n1                       max f1  0.437500   0.747795 179\n2                       max f2  0.378927   0.795292 202\n3                 max f0point5  0.755055   0.782353  73\n4                 max accuracy  0.651129   0.803138 103\n5                max precision  0.879121   0.963855  37\n6                   max recall  0.026050   1.000000 393\n7              max specificity  1.000000   0.997701   0\n8             max absolute_mcc  0.459596   0.585658 169\n9   max min_per_class_accuracy  0.437500   0.795402 179\n10 max mean_per_class_accuracy  0.459596   0.796413 169\n11                     max tns  1.000000 434.000000   0\n12                     max fns  1.000000 250.000000   0\n13                     max fps  0.000000 435.000000 399\n14                     max tps  0.026050 266.000000 393\n15                     max tnr  1.000000   0.997701   0\n16                     max fnr  1.000000   0.939850   0\n17                     max fpr  0.000000   1.000000 399\n18                     max tpr  0.026050   1.000000 393\n\nGains/Lift Table: Extract with `h2o.gainsLift(<model>, <data>)` or `h2o.gainsLift(<model>, valid=<T/F>, xval=<T/F>)`\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# Performance para datos de validaci√≥n\nh2o.auc(aml@leader, xval = TRUE)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.8709619\n```\n:::\n:::\n\n\nRevisemos la curva entre **Recall vs Precision**.\n\nEstos 2 indicadores son de gran relevancia en la evaluaci√≥n del desempe√±o de machine learninig. La Precisi√≥n (tambi√©n llamada valor predictivo positivo) es la fracci√≥n de instancias relevantes entre las instancias recuperadas, mientras que el Recall (tambi√©n conocida como sensibilidad) es la fracci√≥n de instancias relevantes que se recuperaron.\n\n![](Precisionrecall.png){fig-align=\"center\"}\n\nPuedes profundizar un poco m√°s sobre este tema [en este art√≠culo](https://towardsdatascience.com/precision-and-recall-made-simple-afb5e098970f).\n\n\n::: {.cell}\n\n```{.r .cell-code}\nperf <- h2o.performance(model = aml@leader, xval = TRUE)\n\nmetrics <- as.data.frame(h2o.metric(perf))\n\nmetrics |>\n  ggplot(aes(recall, precision)) +\n  geom_line() +\n  theme_minimal()\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-54-1.png){width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# Predicciones para test\npredictions <- h2o.predict(object = aml@leader, newdata = titanic_test_h2o)\npredictions\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n  predict         p0        p1\n1       1 0.33766234 0.6623377\n2       1 0.07670676 0.9232932\n3       1 0.16081239 0.8391876\n4       0 0.62626263 0.3737374\n5       0 0.76809858 0.2319014\n6       1 0.21647427 0.7835257\n\n[190 rows x 3 columns] \n```\n:::\n:::\n\n\nPara guardar el modelo generado en el directorio actual de tu computador y usarlo despu√©s, puedes usar este c√≥digo:\n\n\n::: {.cell}\n\n```{.yaml .cell-code}\nh2o.saveModel(object = aml@leader, path = getwd(), force = TRUE)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# Separamos dataset en test\ntitanic_test <- all_data |>\n  filter(id == \"test\") |>\n  select(-c(id, Survived, PassengerId, Name, Ticket, Cabin))\n\n# Creamos el objeto H2O\ntitanic_test_h2o <- as.h2o(titanic_test)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# Predicciones en nuevo set de datos.\npredictions_test <- h2o.predict(object = aml@leader, newdata = titanic_test_h2o)\n\n# Separamos solo las predicciones.\nsurvived <- as.data.frame(predictions_test$predict)\n```\n:::\n\n\nYa que tenemos nuestas predicciones realizadas, generaremos el archivo para luego subir a Kaggle.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsubmission <- read_csv(\"gender_submission.csv\") |>\n  select(1)\n\ngender_submission <- as.data.frame(cbind(submission, survived)) |>\n  rename(\"Survived\" = predict)\n\nwrite.csv(gender_submission, \"submission_autoML_leader_h2o.csv\", row.names = FALSE)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# Se apaga el cluster H2O\nh2o.shutdown(prompt = FALSE)\n```\n:::\n\n\n## Subir a Kaggle\n\nSubimos nuestro archivo a [Kaggle](https://www.kaggle.com/competitions/titanic/submit) y btenemos una puntuaci√≥n de 0.76076\n\nNo esta mal, pero podr√≠a ser mejor. Hace unos meses obtuve 0.79 usando un modelo de ML ensamblado GLM + GBM (Gradient Boosting Machine).\n\nEn este caso, para mejorar el desempe√±o del algoritmo GBM (que fue el seleccionado como l√≠der de la comparaci√≥n de autoML), habr√≠a que tocar los hiperpar√°metros y reanalizar el feature engineering. Adem√°s, de ver opciones de otros modelos de machine learning. Pero considerar que el modelo que usamos para este art√≠culo lo realizamos aplicando autoML. Es decir, usamos modelos pre-entrenados y con auto ajustes de hiperpar√°metros \"out of the box\" (predefinidos).\n\n![](kaggle.png){fig-align=\"center\" width=\"800\"}\n\n## Conclusiones\n\nEste art√≠culo tiene fines demostrativos, no lo olvides. En la vida real, todo este proceso es (mucho) m√°s largo y complejo. Existen muchas instancias de an√°lisis estad√≠sticos y matem√©ticos sobre los datos dsponibles. Adem√°s, es un proceso iterativo. O sea, generalmente se vuelve a etapas previas frecuentemente, se revisa lo que hay. Incluso, muchas veces es necesario investigar sobre la generaci√≥n y captura de los datos. Del mismo modo, se generan muchos modelos y se realizan muchas pruebas hasta encontrar los de mejor desempe√±o y que tengan m√°s explicabilidad (seg√∫n sea el caso).\n\nRevisamos, adem√°s, el uso de una tecnolog√≠a emergente que son los modelos de machine learning pre-entrenados o autoML. Este tipo de modelos est√°n cada vez m√°s difundidos, en especial, en ambientes cloud. Por ejemplo, [Google](https://cloud.google.com/automl/) y Microsoft Azure tienen los suyos.\n\nEl desarrollo de modelos de machine learning tradicional consume bastantes recursos, y que requieren un conocimiento del dominio y tiempo significativos para generar y comparar docenas de modelos. El autoML reduce el tiempo necesario para obtener modelos de aprendizaje autom√°tico listos para producci√≥n con gran eficiencia y facilidad. Estos elementos son vitales en la actualidad para muchas startups e industrias. Adem√°s, amplia las opciones en el uso de este tipo de tecnolog√≠as a m√°s personas y organizaciones.\n\nA pesar de que parece magia todo √©sto (o una caja negra que nadie sabe lo que pasa dentro), el uso de autoML ayuda a generar modelos de forma m√°s r√°pida y facilita el desarrollo de modelos m√°s completos en el corto plazo. Es una buena estrategia para screening.\n\nOk. Entonces no necesitamos ingenieros, matem√°ticos o cient√≠ficos de datos?\n\nNo, para nada. Ciertamente estas tecnolog√≠as (autoML) democratizan el acceso a m√°s personas, pero a√∫n son necesarios los conocimientos y reflexiones de los expertos en el √°rea para ajustar los modelos, controlar sesgos o problemas de equidad. Y, por cierto, que conozcan del negocio, pues finalmente el desarrollo de estas tecnolog√≠as buscan resolver problem√°ticas concretas. Sin mencionar todo el trabajo de comunicaci√≥n, explicaci√≥n y disfusi√≥n de los resultados, para los cual esos perfiles profesionales son de gran valor.\n\nFinalmente, este ejercicio que hemos visto ac√° es m√°s demostrativo que otra cosa, pero espero haberte mostrado c√≥mo se pueden entrenar algoritmos y usar modelos de ML para generar predicciones.\n\nNos vemos!! ü§™\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}